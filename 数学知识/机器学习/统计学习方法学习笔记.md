# 序

本文旨在提供系统的《统计学习方法》**第二版**的学习笔记，本学习笔记的优点在于，提供公式中字母的注释，以便于公式的理解，同时借助 Obsidian 的 [[Obsidian阅读指南#双向链接|双向链接]] 功能，便捷知识之间的串联。

本内容依然在制作中，计划一共分为三步完善本内容：

1. 记录《统计学习方法》中全部的数学公式。
2. 补充知识点的内容。
3. 进行双向链接的建立以及内容的补充。

# 符号

在本文档中的大多数符号均具备其固定的对应定义，部分具体代表性的在这里指出：

- $\mathcal{X},\mathcal{Y}$ 分别为输入空间与输出空间。

# 统计学习及监督学习概论

## 分类

### 基本分类

一般来说，统计学习或机器学习一般包括监督学习、无监督学习、强化学习。有时还包括半监督学习、主动学习。

在本书中，主要讨论的是监督学习与无监督学习，简单来区分，监督学习与无监督学习的区别在于是否需要数据标注，这就是其中"监督"的含义。

#### 监督学习

在监督学习中，输入实例 $x$ 的特征向量记作：

$$
x=\left (x^{(1)}, x^{(2)}, \cdots, x^{(i)}, \cdots, x^{(n)}\right)^{\mathrm{T}}
$$

$x^{(i)}$ 表示 $x$ 的第 $i$ 个特征。比如说一个物体的体积、重量以及价钱就可以作为三个特征（也可以理解为参数）。

注意 $x^{(i)}$ 与 $x_{i}$ 不同，统计学习方法这本书中通常用 $x_{i}$ 表示多个输入变量中的第 $i$ 个变量，即：

$$
x_{i}=\left (x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}}
$$

监督学习从训练数据（training data）集合中学习模型，对测试数据（test data）进行预测。训练数据由输入（或特征向量）与输出对组成，训练集通常表示为：

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

测试数据也由输入与输出对组成。输入与输出对又称为样本（sample）或样本点。

监督学习的模型可以是概率模型或非概率模型，由条件概率分布 $P(Y \mid X)$ 或决策函数（decision function）$Y=f(X)$ 表示, 随具体学习方法而定。对具体的输入进行相应的输出预测时, 写作 $P(y \mid x)$ 或 $y=f(x)$。

监督模型其本质上是一个从输入空间到输出空间的映射，称这一类映射的集合为假设空间。

条件概率分布 $P(Y \mid X)$ 表示当输入为 $X$ 时对应的结果为 $Y$ 的概率，不难理解对于可能存在的结果 $Y_1,Y_2,Y_3,\cdots,Y_n$，存在若干条件概率 $P(Y_1 \mid X),P(Y_2 \mid X),P(Y_3 \mid X),\cdots,P(Y_n \mid X)$，其中概率最大的就是算法可能输出的结果。

而对于决策函数，输出结果则直接由决策函数决定。值得一提的是，条件概率分布的方式，不难用决策函数表示（$y=\arg \max\limits _{y} P\left (y \mid x\right)$）。

监督学习分为学习和预测两个过程，由学习系统和预测系统完成：

![[监督学习.png]]

这其中首先给定一个训练数据集：

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

其中 $\left (x_{i}, y_{i}\right), i=1,2, \cdots, N,$ 称为样本或样本点。$x_{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}$ 是输入的观测值，也称为输入或实例，$y_{i} \in \mathcal{Y}$ 是输出的观测值，也称为输出。

在这其中，$\mathcal{X}$ 为输入空间，$\mathcal{Y}$ 为输出空间。

监督学习分为学习和预测两个过程，由学习系统与预测系统完成。在学习过程中，学习系统利用给定的训练数据集，通过学习（或训练）得到一个模型，表示为条件概率分布 $\hat{P}(Y \mid X)$ 或决策函数 $Y=\hat{f}(X)$。条件概率分布 $\hat{P}(Y \mid X)$ 或决策函数 $Y=\hat{f}(X)$ 描述输入与输出随机变量之间的映射关系。在预测过程中，预测系统对于给定的测试样本集中的输入 $x_{N+1}$，由模型 $y_{N+1}=\arg \max\limits _{y} \hat{P}\left (y \mid x_{N+1}\right)$ 或 $y_{N+1}=\hat{f}\left (x_{N+1}\right)$ 给出相应的输出 $y_{N+1}$。

在监督学习中，假设训练数据与测试数据是依联合概率分布 $P (X, Y)$ 独立同分布产生的。

学习系统（也就是学习算法）试图通过训练数据集中的样本 $\left (x_{i}, y_{i}\right)$ 带来的信息学习模型。具体地说，对输入 $x_{i}$，一个具体的模型 $y=f (x)$ 可以产生一个输出 $f\left (x_{i}\right)$，而训练数据集中对应的输出是 $y_{i}$。如果这个模型有很好的预测能力，训练样本输出 $y_{i}$ 和模型输出 $f\left (x_{i}\right)$ 之间的差就应该足够小。学习系统通过不断地尝试，选取最好的模型，以便对训练数据集有足够好的预测，同时对末知的测试数据集的预测也有尽可能好的推广。

#### 无监督学习

假设 $\mathcal{X}$ 是输入空间，$\mathcal{Z}$ 是隐式结构空间。要学习的模型可以表示为函数 $z=g (x)$，条件概率分布 $P (z \mid x)$，或者条件概率分布 $P (x \mid z)$ 的形式，其中 $x \in \mathcal{X}$ 是输入，$z \in \mathcal{Z}$ 是输出。包含所有可能的模型的集合称为假设空间。无监督学习旨在从假设空间中选出在给定评价标准下的最优模型。

在这里，隐式结构空间指模型在学习过程中并没有直接观测到的、隐含在数据背后的结构化信息的空间。这些结构化信息通常用于指导模型学习任务或是提供额外的知识，但并不作为模型的显式输入。

举例说明，我们有一个任务是判断一个长方形是横着的还是竖着的，而为了完成这个任务，我们需要知道长方形的两边之比。然而，机器学习中的模型可能并不知道这个隐式结构空间（长方形的两边之比）是什么，它只接收到了两边长度的输入信息。模型在学习的过程中会尝试发现输入与输出之间的关系，以便在未见过的长方形上作出正确的判断。通过大量的训练样本，模型可以学习到长方形的两边之比与其横竖方向之间的关联，从而在未知情况下也能够准确地进行预测。

无监督学习通常使用大量的无标注数据学习或训练，每一个样本是一个实例。训练数据表示为 $U=\left\{x_{1}, x_{2}, \cdots, x_{N}\right\}$，其中 $x_{i}, i=1,2, \cdots, N,$ 是样本。

无监督学习可以用于对已有数据的分析，也可以用于对末来数据的预测。分析时使用学习得到的模型, 即函数 $z=\hat{g}(x)$，条件概率分布 $\hat{P}(z \mid x)$，或者条件概率分布 $\hat{P}(x \mid z)$。

其中不难发现的是，条件概率分布 $\hat{P}(z \mid x)$ 与 $\hat{P}(x \mid z)$ 的格式上的相似性，其前者代表类似监督学习中的输入与输出的流程，而后者则表示为在已知隐变量的前提下给出输入的概率。

预测时，和监督学习有类似的流程。由学习系统与预测系统完成：

![[无监督学习.png]]

在学习过程中，学习系统从训练数据集学习，得到一个最优模型，表示为函数 $z=\hat{g}(x)$，条件概率分布 $\hat{P}(z \mid x)$ 或者条件概率分布 $\hat{P}(x \mid z)$。在预测过程中，预测系统对于给定的输入 $x_{N+1}$，由模型 $z_{N+1}=\hat{g}\left (x_{N+1}\right)$ 或 $z_{N+1}=\arg \max _{z} \hat{P}\left (z \mid x_{N+1}\right)$ 给出相应的输出 $z_{N+1}$，进行聚类或降维，或者由模型 $\hat{P}(x \mid z)$ 给出输入的概率 $\hat{P}\left (x_{N+1} \mid z_{N+1}\right)$，进行概率估计。

#### 强化学习

*本书仅在此处提及强化学习，并未展开说明。*

强化学习（reinforcement learning）是指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题。假设智能系统与环境的互动基于马尔可夫决策过程（Markov decision process），智能系统能观测到的是与环境互动得到的数据序列。强化学习的本质是学习最优的序贯决策。

智能系统与环境的互动如下：

![[强化学习.png]]

在每一步 $t$，智能系统从环境中观测到一个状态（state）$s_{t}$ 与一个奖励（reward）$r_{t}$，采取一个动作（action）$a_{t}$。环境根据智能系统选择的动作，决定下一步 $t+1$ 的状态 $s_{t+1}$ 与奖励 $r_{t+1}$。要学习的策略表示为给定的状态下采取的动作。智能系统的目标不是短期奖励的最大化，而是长期累积奖励的最大化。强化学习过程中，系统不断地试错（trial and error），以达到学习最优策略的目的。

强化学习的马尔可夫决策过程是状态、奖励、动作序列上的随机过程，由五元组 $\langle S, A, P, r, \gamma\rangle$ 组成。

- $S$ 是有限状态（state）的集合
- $A$ 是有限动作（action）的集合
- $P$ 是状态转移概率（transition probability）函数：$P\left(s^{\prime} \mid s, a\right)=P\left(s_{t+1}=s^{\prime} \mid s_{t}=s, a_{t}=a\right)$ 表示当状态 $s$ 与动作 $a$ 时，下一个状态为 $s^{\prime}$ 的概率。
- $r$ 是奖励函数（reward function）：$r(s, a)=E\left(r_{t+1} \mid s_{t}=s, a_{t}=a\right.  )$ 表示在状态 $s$ 下采取动作 $a$ 所获得的即时奖励的期望值。换句话说，给定智能体处于状态 $s$ 并采取动作 $a$，奖励函数告诉我们在下一个时间步 $t+1$ 中，智能体会获得多少奖励。
- $\gamma$ 是衰减系数（discount factor）：$\gamma \in[0,1]$。在强化学习中，衰减系数用于衡量未来奖励对当前决策的重要性。通常，智能体会以较小的折扣因子考虑未来奖励，以便更多地关注即时奖励，而随着时间的推移，折扣因子会逐渐增大，使得智能体更加重视长期回报。这里的折扣因子可以看作是一种衰减系数。

**马尔可夫决策过程具有马尔可夫性，下一个状态只依赖于前一个状态与动作**，由状态转移概率函数 $P\left(s^{\prime} \mid s, a\right)$ 表示。下一个奖励依赖于前一个状态与动作，由奖励函数 $r(s, a)$ 表示。

策略 $\pi$ 定义为给定状态下动作的函数 $a=f(s)$ 或者条件概率分布 $P(a \mid s)$。给定一个策略 $\pi$，智能系统与环境互动的行为就已确定（或者是确定性的或者是随机性的）。

价值函数（value function）或状态价值函数（state value function）定义为策略 $\pi$ 从某一个状态 $s$ 开始的长期累积奖励的数学期望：

$$
v_{\pi}(s)=E_{\pi}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\cdots \mid s_{t}=s\right]
$$

动作价值函数（action value function）定义为策略 $\pi$ 的从某一个状态 $s$ 和动作 $a$ 开始的长期累积奖励的数学期望：

$$
q_{\pi}(s, a)=E_{\pi}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\cdots \mid s_{t}=s, a_{t}=a\right]
$$

强化学习的目标就是在所有可能的策略中选出价值函数最大的策略 $\pi^{*}$，而在实际学习中往往从具体的策略出发，不断优化已有策略。这里 $\gamma$ 表示末来的奖励会有衰减。

强化学习方法中有基于策略的（policy-based）、基于价值的（value-based），这两者属于无模型的（model-free）方法, 还有有模型的（model-based）方法。

有模型的方法试图直接学习马尔可夫决策过程的模型，包括转移概率函数 $P\left(s^{\prime} \mid s, a\right)$ 和奖励函数 $r(s, a)$。这样可以通过模型对环境的反馈进行预测，求出价值函数最大的策略 $\pi^{*}$。

无模型的、基于策略的方法不直接学习模型，而是试图求解最优策略 $\pi^{*}$，表示为函数 $a=f^{*}(s)$ 或者是条件概率分布 $P^{*}(a \mid s)$，这样也能达到在环境中做出最优决策的目的。学习通常从一个具体策略开始，通过搜索更优的策略进行。

无模型的、基于价值的方法也不直接学习模型，而是试图求解最优价值函数，特别是最优动作价值函数 $q^{*}(s, a)$。这样可以间接地学到最优策略，根据该策略在给定的状态下做出相应的动作。学习通常从一个具体价值函数开始，通过搜索更优的价值函数进行。

### 按模型分类

#### 概率模型与非概率模型

《统计学习方法》中介绍的决策树、朴素贝叶斯、隐马尔可夫模型、条件随机场、概率潜在语义分析、潜在狄利克雷分配、高斯混合模型是概率模型。感知机、支持向量机、k 近邻、AdaBoost、k 均值、潜在语义分析，以及神经网络是非概率模型。逻辑斯谛回归既可看作是概率模型，又可看作是非概率模型。

对于概率模型来说，均满足概率模型中最基础的加法规则与乘法规则：

- 加法规则：$P(x)=\sum\limits_{y} P(x, y)$
- 乘法规则：$P(x, y)=P(x) P(y \mid x)$

其中 $x$ 和 $y$ 是随机变量。

#### 线性模型与非线性模型

《统计学习方法》中介绍的感知机、线性支持向量机、k 近邻、飞均值、潜在语义分析是线性模型。核函数支持向量机、AdaBoost、神经网络是非线性模型。

#### 参数化模型和非参数化模型

本书介绍的感知机、朴素贝叶斯、逻辑斯谛回归、k 均值、高斯混合模型是参数化模型。决策树、支持向量机、AdaBoost、k 近邻、潜在语义分析、概率潜在语义分析、潜在狄利克雷分配是非参数化模型。

参数化模型适合问题简单的情况，现实中问题往往比较复杂，非参数化模型更加
有效。

### 按算法分类

可以分为在线学习和批量学习。在线学习指每次接受一个样本进行预测，之后学习模型的操作；批量学习则指一次性接受所有数据学习模型。

### 按技巧分类

#### 贝叶斯学习

贝叶斯学习又被称为贝叶斯推理，一般指通过贝叶斯定理计算后验概率并且进行估计的模型，可以用以下公式计算后验概率：

$$P(\theta \mid D)=\frac{P(\theta) P(D \mid \theta)}{P(D)}$$

其中 $D$ 表示随机变量，$\theta$ 表示模型参数，不难得到 $P(\theta)$ 表示先验概率，$P(D\mid \theta)$ 表示似然函数。

其中后验概率指在已经观测到数据后，我们对未知量的新估计。也就是在知道结果之后计算的概率。同样的，有对应的概率，先验概率，这指的是知道结果之前的概率，也就是真正的估计，训练模型的目的就在于使得先验概率尽可能的逼近后验概率。

先验概率和后验概率指的是，先于检验与后于检验。

似然函数，同样不难理解。从其表达式 $P(D\mid \theta)$ 不难得知，是在已知模型参数 $\theta$ 的情况下，数据为 $D$ 的可能性，从某种程度上刻画了模型参数的准确性。

模型估计时，估计整个后验概率分布 $P(\theta \mid D)$。如果需要给出一个模型，通常取后验概率最大的模型。

预测时，计算数据对后验概率分布的期望值：

$$P(x \mid D)=\int P(x \mid \theta, D) P(\theta \mid D) \mathrm{d} \theta$$

本质上在这一部分的内容中可以将随机变量解释为观测数据，而 $x$ 则可以理解为预测数据。

可以通过概率 $P(x\mid D)$ 得到对于 $x$ 期望值，而这个期望值则会作为预测值。

$$\hat{x} = \int x \cdot P(x \mid D) \mathrm{d} x$$

而非使用后验概率最大的参数值，即后验概率最大估计（Maximum a Posteriori Estimation，MAP）：

$$\hat{x}_{\text{MAP}} = \arg\max\limits_{x} P(x \mid D)$$ ^MAP

原式中的 $P(x \mid \theta, D) P(\theta \mid D)$ 表示预测数据 $x$ 和参数 $\theta$ 的联合概率分布。它表示在观测数据 $D$ 的条件下，同时考虑预测数据 $x$ 和参数 $\theta$ 的联合概率，也就是 $P(x,\theta\mid D)$，此时进行积分也就是对 $\theta$ 进行了边缘化处理。

#todo 在概率论模块中解释边缘化处理。

#### 核方法

核方法（kernel method）是使用核函数表示和学习非线性模型的一种机器学习方法，可以用于监督学习和无监督学习。有一些线性模型的学习方法基于相似度计算，更具体地，向量内积计算。核方法可以把它们扩展到非线性模型的学习，使其应用范围更广泛。
《统计学习方法》介绍的核函数支持向量机，以及核 PCA、核 k 均值属于核方法。
把线性模型扩展到非线性模型，直接的做法是显式地定义从输入空间（低维空间）到特征空间（高维空间）的映射，在特征空间中进行内积计算。比如，支持向量机，把输入空间的线性不可分问题转化为特征空间的线性可分问题，如下图所示。核方法的技巧在于不显式地定义这个映射，而是直接定义核函数，即映射之后在特征空间的内积。这样可以简化计算，达到同样的效果。

![[核方法的映射.png]]

这一方法的具体解释我们会在后续的内容中提及。

## 三要素

一般来说，我们认为统计学习方法由模型、策略和算法组成，这三者也被称为统计学习方法三要素。

### 模型

统计学习首要考虑的问题是学习什么样的模型。在监督学习过程中，模型就是所
要学习的**条件概率分布**或**决策函数**。模型的假设空间（hypothesis space）包含所有可能的条件概率分布或决策函数。假设空间中的模型一般有无穷多个。

假设空间是决策函数的集合，则表示如下：

$$\mathcal{F}=\{f \mid Y=f(X)\}$$

其中 $X\in\mathcal{X}$，$Y\in\mathcal{Y}$。也就是二者是定义在输入与输出空间上的变量。此时一般来说 $\mathcal{F}$ 一般来说是一个参数向量决定的 [[函数族]]。

$$\mathcal{F}=\left\{f \mid Y=f_{\theta}(X), \theta \in \mathbf{R}^{n}\right\}$$

其中参数向量 $\theta$ 取值于 $n$ 维欧氏空间 $R^n$，称为参数空间（parameter space）。

同样，假设空间是条件概率分布的集合，则表示如下：

$$\mathcal{F}=\{P \mid P(Y \mid X)\}$$

其中，$X$ 和 $Y$ 是定义在输入空间 $\mathcal{X}$ 和输出空间 $\mathcal{Y}$ 上的随机变量。这时 $\mathcal{F}$ 通常是由一个参数向量决定的条件概率分布族（概念与 [[函数族]] 接近）：

$$\mathcal{F}=\left\{P \mid P_{\theta}(Y \mid X), \theta \in \mathbf{R}^{n}\right\}$$

### 策略

有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。统计学习的目标在于从假设空间中选取最优模型。

首先引入损失函数与风险函数的概念。损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。

#### 损失函数

监督学习问题是在假设空间 $\mathcal{F}$ 中选取模型 $f$ 作为决策函数，对于给定的输入 $X$，由 $f(X)$ 给出相应的输出 $Y$，这个输出的预测值 $f(X)$ 与真实值 $Y$ 可能一致也可能不一致，用一个损失函数（loss function）或代价函数（cost function）来度量预测错误的程度。损失函数是 $f(X)$ 和 $Y$ 的非负实值函数，记作 $L(Y, f(X))$。

需要注意的是，损失函数用来衡量函数的不准确性而非准确性，切勿混淆。

以下是几类常见的损失函数：

1. 0-1 损失函数

$$L(Y, f(X))=\left\{\begin{array}{ll}
1, & Y \neq f(X) \\
0, & Y=f(X)
\end{array}\right.$$

0-1 损失函数不难理解，当模型给出的预测值与真实值一致的时候则为 $0$，若不一致则为 $1$，即**有损失**。

2. 平方损失函数

$$L(Y, f(X))=(Y-f(X))^{2}$$ ^MSE

平方损失函数（Mean Squared Error, MSE）使用预测值与真实值的平方差作为损失的衡量。使用平方利于求导且能保证非负，对于后续优化过程中使用梯度下降等方法十分适合。

3. 绝对损失函数

$$L(Y, f(X))=|Y-f(X)|$$

绝对损失函数（Mean Absolute Error, MAE）使用预测值与真实值的差的绝对值作为损失的衡量，这种表示并非利于求导，但是其对于异常值有更强的鲁棒性。

4. 对数似然损失函数

$$L(Y, P(Y \mid X))=-\log P(Y \mid X)$$

对数似然损失函数也用于描述损失，但是正如其公式显示，其主要用于描述概率中的内容，其中 $P(Y\mid X)$ 的含义即为当输入为 $X$ 的时候输出为 $Y$ 的概率，也就是正确的概率，而因为 $正确+错误=1$，加上负号则可以形容错误的情况，也就是损失。

这里需要解释的是使用对数而非直接使用 $P(Y\mid X)$，这主要是因为对数可以将乘法转换为加法（如 $P(Y\mid X)=P(y^{(1)}\mid X)\cdot P(y^{(2)}\mid X)\cdots P(y^{(n)}\mid X)$，此时使用对数可以表示为 $\log{P(Y\mid X)}=\sum\limits_{i=1}^{n}{\log{P(y^{(i)}\mid X)}}$）。

##### 期望损失

根据损失函数以及 $X$ 与 $Y$ 的分布可以计算损失函数的期望（称作风险函数，或者期望损失），如下：

$$\begin{aligned}
R_{\exp }(f) & =E_{P}[L(Y, f(X))] \\
& =\int_{\mathcal{X} \times \mathcal{Y}} L(y, f(x)) P(x, y) \mathrm{d} x \mathrm{~d} y
\end{aligned}$$

本式为加权平均计算方差，计算在输入与输出空间中损失函数的期望值，其中权重为 $P(x, y) \mathrm{d} x \mathrm{~d} y$。

可惜的是，上式仅为理论上的值，但是实际上我们很难按照此定义计算风险函数。

学习的目标就是选择期望风险最小的模型。由于联合分布 $P(X, Y)$ 是末知的，$R_{\exp }(f)$ 不能直接计算。实际上，如果知道联合分布 $P(X, Y)$，可以从联合分布直接求出条件概率分布 $P(Y \mid X)$，也就不需要学习了。正因为不知道联合概率分布，所以才需要进行学习。这样一来，一方面根据期望风险最小学习模型要用到联合分布，另一方面联合分布又是末知的，所以监督学习就成为一个病态问题（ill-formed problem，在此处指一个问题的解决为循环论证，无法真正解决）。

##### 经验风险

于是引入经验风险，假设已经有训练数据集 $T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}$，定义经验风险如下：

$$R_{\mathrm{emp}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)$$

也就是对每一个训练数据计算损失函数并且求平均。根据大数定律，当 $N$ 也就是样本数量趋近于无穷的时候，$R_{emp}(f)$ 将趋近于 $R_{\exp }(f)$。也就是说我们可以使用经验风险估计期望风险。

#todo 在概率论模块解释大数定律。

但是实际上数据集的数量有限，甚至可以说十分的少，于是就引出了两种不同的策略，经验风险最小化以及结构风险最小化。

根据经验风险的公式，不难得到，本质上经验风险最小化的操作就是为了获得一个函数，使得经验风险最小：

$$\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)$$

但是仅使用经验风险，在数据较少的时候可能会产生过拟合的现象，如下：

![[过拟合.png]]

其中 $M$ 表示用于拟合的多项式的次，可以发现当 $M=9$ 的时候拟合的曲线完美通过每一个数据，但是实际上不难得出，这个结果对于新数据的预测能力极差。

这形容的就是过拟合，即对于训练数据的准确度高，但是对于测试数据的准确度低，也可以称为泛化能力差。

![[模型复杂度与误差的关系.png]]

##### 结构风险

此时比较便于理解的解决此问题的方法就是添加一个正则化项，也被称为罚项，此时的函数被称为结构风险：

$$R_{\mathrm{srm}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)$$

其中 $J(f)$ 描述模型的复杂程度，具体例子将在后面给出。此时不难理解的是，对于结构风险最小化来说，要求损失函数以及模型复杂程度同时达到较小，并通过比例系数 $\lambda$ 调整权重。

结构风险最小化如下：

$$\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)$$

### 算法

算法是指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。

这时，统计学习问题归结为最优化问题，统计学习的算法成为**求解最优化问题**的算法。如果最优化问题有显式的解析解，这个最优化问题就比较简单。但通常解析解不存在，这就需要用数值计算的方法求解。如何保证找到全局最优解，并使求解的过程非常高效，就成为一个重要问题。统计学习可以利用已有的最优化算法，有时也需要开发独自的最优化算法。

统计学习方法之间的不同，主要来自其模型、策略、算法的不同。确定了模型、策略、算法，统计学习的方法也就确定了。这就是将其称为统计学习方法三要素的原因。

## 模型评估与模型选择

### 训练误差与测试误差

一般来说，模型使用训练误差与测试误差来评估模型的训练程度以及泛化能力。

假设使用的模型为 $Y=\hat{f}(X)$，则根据之前的 [[统计学习方法学习笔记#经验风险|经验风险]]，得到训练误差，也就是对于训练数据的平均损失为：

$$R_{\mathrm{emp}}(\hat{f})=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right)$$

同样的，测试误差，也就是对于测试数据的平均损失：

$$e_{\mathrm{test}}=\frac{1}{N^{\prime}} \sum_{i=1}^{N^{\prime}} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right)$$

当损失函数是 0-1 损失的时候，测试误差的表示如下：

$$e_{\text {test }}=\frac{1}{N^{\prime}} \sum_{i=1}^{N^{\prime}} I\left(y_{i} \neq \hat{f}\left(x_{i}\right)\right)$$

其中 $I$ 为指示函数，可以理解为其中的表达式为 `true` 的时候为 $1$，否则为 $0$。

同样，不难得到测试数据集上的准确率为：

$$r_{\text {test }}=\frac{1}{N^{\prime}} \sum_{i=1}^{N^{\prime}} I\left(y_{i}=\hat{f}\left(x_{i}\right)\right)$$

显然，

$$r_{\text {test }}+e_{\text {test }}=1$$

### 过拟合与模型选择

此处的内容与上方的 [[统计学习方法学习笔记#结构风险|结构风险]] 类似，主要问题聚焦于过拟合。

此时给出上方内容的具体阐述，依然对于下图中的 10 个数据：

![[过拟合.png]]

设置一个 $M$ 次多项式：

$$f_{M}(x, w)=w_{0}+w_{1} x+w_{2} x^{2}+\cdots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}$$

假如使用平方损失作为损失函数，添加系数 $\frac{1}{2}$ 便于计算（由于梯度下降需要进行求导，会从指数上降下一个系数 $2$）：

$$L(w)=\frac{1}{2} \sum_{i=1}^{N}\left(f\left(x_{i}, w\right)-y_{i}\right)^{2}$$

代入数据集得到损失函数的值为：

$$L(w)=\frac{1}{2} \sum_{i=1}^{N}\left(\sum_{j=0}^{M} w_{j} x_{i}^{j}-y_{i}\right)^{2}$$

此时可以直接进行梯度下降，或者使用其他的策略，但是此时不难看出可能出现上图中出现的过拟合问题。

于是处理以下的结构风险最小化问题，避免过拟合：

$$\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)$$

在此处以这个例子进一步解释正则化项可以使用值，比如参数向量的二范数。

$$L(w)=\frac{1}{N} \sum_{i=1}^{N}\left(f\left(x_{i} ; w\right)-y_{i}\right)^{2}+\frac{\lambda}{2}\|w\|^{2}$$

使用二范数作为正则化项的效果显而易见，可以使得损失函数本身最小化的同时，使得每一项的系数尽可能的小，这样可以使得一些参数较小，从而保证结构不过于复杂（比如说一些项系数接近于 $0$，则约等于少了一项，结构变简单）。

当然，使用 $L_1$ 范数也是同理：

$$L(w)=\frac{1}{N} \sum_{i=1}^{N}\left(f\left(x_{i} ; w\right)-y_{i}\right)^{2}+\lambda\|w\|_{1}$$

正则化符合奥卡姆剃刀（Occam's razor）原理。奥卡姆剃刀原理应用于模型选择时变为以下想法：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。

从贝叶斯估计的角度来看，正则化项对应于模型的先验概率。可以假设复杂的模型有较小的先验概率，简单的模型有较大的先验概率。

> 若定义 $X=\{模型精度\},Y_1=\{选择简单模型\},Y_2=\{选择复杂模型\}$，则根据贝叶斯定理有 $P(Y\mid X)=\frac{P(X\mid Y)P(Y)}{P(X)}$，当模型精度一样的情况下，则有 $P(X\mid Y_1)=P(X\mid Y_2)$，若想要 $P(Y_1\mid X)>P(Y_2\mid X)$，也就是想要选择更简单的模型，则得到 $P(Y_1)>P(Y_2)$，此为先验概率。
> 
> 先验概率是人为确定的，因此假如人为想要选择简单模型，则只需要将简单模型的先验概率设置得大于复杂模型得。

### 交叉验证

交叉验证（cross validation）是一种常用的模型选择方法。

如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别为训练集（training set）、验证集（validation set）和测试集（test set）。训练集用来训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。在学习到的不同复杂度的模型中，选择对验证集有最小预测误差的模型。由于验证集有足够多的数据，用它对模型进行选择也是有效的。

但是, 在许多实际应用中数据是不充足的。为了选择好的模型，可以采用交叉验证方法。交叉验证的基本想法是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。

1. 简单交叉验证

简单交叉验证方法是：首先随机地将已给数据分为两部，一部分作为训练集，另一部分作为测试集（例如，$70 \%$ 的数据为训练集，$30 \%$ 的数据为测试集）；然后用训练集在各种条件下（例如，不同的参数个数）训练模型，从而得到不同的模型；在测试集上评价各个模型的测试误差，选出测试误差最小的模型。

2.  $S$ 折交叉验证

应用最多的是 $S$ 折交叉验证（S-fold cross validation），方法如下：

- 首先随机地将已给数据切分为 $S$ 个互不相交、大小相同的子集；
- 然后利用 $S-1$ 个子集的数据训练模型, 利用余下的子集测试模型；
- 将这一过程对可能的 $S$ 种选择重复进行；
- 最后选出 $S$ 次评测中平均测试误差最小的模型。

3. 留一交叉验证
 
 $S$ 折交叉验证的特殊情形是 $S=N$，称为留一交叉验证（leave-one-out cross validation），往往在数据缺乏的情况下使用。这里，$N$ 是给定数据集的容量。

不难看出，三种不同的交叉验证的方法均为使用数据评估模型的方法，而在数据量较小的时候，数据需要重复使用。

## 泛化能力

### 泛化误差

首先给出泛化误差的定义。如果学到的模型是 $\hat{f}$，那么用这个模型对末知数据预测的误差即为泛化误差（generalization error）：

$$\begin{aligned}
R_{\exp }(\hat{f}) & =E_{P}[L(Y, \hat{f}(X))] \\
& =\int_{\mathcal{X} \times \mathcal{Y}} L(y, \hat{f}(x)) P(x, y) \mathrm{d} x \mathrm{~d} y
\end{aligned}$$

不难看出，泛化误差其实就是 [[统计学习方法学习笔记#期望损失|损失函数的期望]]，也就是泛化误差和损失函数共同描述的都是一个模型预测的误差程度，也就反过来描述了一个模型的预测能力，或者说泛化能力。

### 泛化误差上界

学习方法的泛化能力分析往往是通过研究泛化误差的概率上界进行的，简称为泛化误差上界（generalization error bound）。具体来说, 就是通过比较两种学习方法的泛化误差上界的大小来比较它们的优劣。

泛化误差上界通常具有以下性质：

- 它是样本容量的函数，当样本容量增加时，泛化上界趋于 $0$；
- 它是假设空间容量（capacity）的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大。

下面给出一个简单的泛化误差上界的例子：二类分类问题的泛化误差上界。

考虑二类分类问题。已知训练数据集 $T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}, N$ 是样本容量，$T$ 是从联合概率分布 $P (X, Y)$ 独立同分布产生的，$X \in \mathbf{R}^{n}, Y \in\{-1,+1\}$。

假设空间是函数的有限集合 $\mathcal{F}=\left\{f_{1}, f_{2}, \cdots, f_{d}\right\}$，$d$ 是函数个数。设 $f$ 是从 $\mathcal{F}$ 中选取的函数。损失函数是 0-1 损失。关于 $f$ 的 [[统计学习方法学习笔记#期望损失|期望风险]] 和 [[统计学习方法学习笔记#经验风险|经验风险]] 分别是：

$$
\begin{array}{l}
R(f)=E[L(Y, f(X))]\\
\hat{R}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)
\end{array}
$$

而假如进行 [[统计学习方法学习笔记#经验风险|经验风险最小化]] 则是寻找函数：

$$f_{N}=\arg \min _{f \in \mathcal{F}} \hat{R}(f)$$

毫无疑问的是，$f_N$ 依赖于训练集的样本容量 $N$，但是人们更加关心的是其 [[统计学习方法学习笔记#泛化能力|泛化能力]]，也就是计算：

$$R\left(f_{N}\right)=E\left[L\left(Y, f_{N}(X)\right)\right]$$

下面讨论从有限集合 $\mathcal{F}=\left\{f_{1}, f_{2}, \cdots, f_{d}\right\}$ 中任意选出的函数 $f$ 的泛化误差上界。

#### 泛化误差上界（定理 1.1）

对二类分类问题，当假设空间是有限个函数的集合 $\mathcal{F}=\left\{f_{1}, f_{2}, \cdots, f_{d}\right\}$ 时，对任意一个函数 $f \in \mathcal{F}$，至少以概率 $1-\delta$，$0<\delta<1$，以下不等式成立：

$$R(f) \leqslant \hat{R}(f)+\varepsilon(d, N, \delta)$$

其中：

$$\varepsilon(d, N, \delta)=\sqrt{\frac{1}{2 N}\left(\log d+\log \frac{1}{\delta}\right)}$$

上式中，$d$ 为假设空间中的函数个数，$N$ 为训练样本个数，$\delta$ 为一个概率。

不等式左端 $R(f)$ 是泛化误差，右端即为泛化误差上界。在泛化误差上界中，第 $1$ 项是训练误差，训练误差越小，泛化误差也越小。第 $2$ 项 $\varepsilon(d, N, \delta)$ 是 $N$ 的单调递减函数，当 $N$ 趋于无穷时趋于 $0$；同时它也是 $\sqrt{\log d}$ 阶的函数，假设空间 $\mathcal{F}$ 包含的函数越多，其值越大。

从直观上理解，$N$ 越大，训练越多，模型应当更加精准，因此泛化误差应该减小。

以下给出此结论的证明：

首先引入 [[Hoeffding不等式]]，对于设 $X_{1}, X_{2}, \cdots, X_{N}$ 是独立随机变量，且 $X_{i} \in\left[a_{i}, b_{i}\right], i=1,2, \cdots, N$；$\bar{X}$ 是 $X_{1}, X_{2}, \cdots, X_{N}$ 的经验均值，即 $\bar{X}=\frac{1}{N} \sum\limits_{i=1}^{N} X_{i}$，则对任意 $t>0$，以下不等式成立：

$$
\begin{array}{l}
P[\bar{X}-E(\bar{X}) \geqslant t] \leqslant \exp \left(-\frac{2 N^{2} t^{2}}{\sum\limits_{i=1}^{N}\left(b_{i}-a_{i}\right)^{2}}\right) \\
P[E (\bar{X})-\bar{X} \geqslant t] \leqslant \exp \left (-\frac{2 N^{2} t^{2}}{\sum\limits_{i=1}^{N}\left (b_{i}-a_{i}\right)^{2}}\right)
\end{array}
$$

对任意函数 $f \in \mathcal{F}$，$\hat{R}(f)$ 是 $N$ 个独立的随机变量 $L(Y, f(X))$ 的样本均值，$R(f)$ 是随机变量 $L(Y, f(X))$ 的期望值。如果损失函数取值于区间 $[0,1]$，即对所有 $i$， $\left[a_{i}, b_{i}\right]=[0,1]$，那么由 [[Hoeffding不等式]] 不难得知，对 $\varepsilon>0$，以下不等式成立：

$$P (R (f)-\hat{R}(f) \geqslant \varepsilon) \leqslant \exp \left (-2 N \varepsilon^{2}\right)$$

其中化简步骤为：

- $b_i-a_i = 1$，从而得到 $\sum\limits_{i=1}^{N}\left (b_{i}-a_{i}\right)^{2} = N$。
- $t=\varepsilon$。

之后进行消元，得到上式。

则对于每一个 $f\in\mathcal{F}$ 均有 $P (R (f)-\hat{R}(f) \geqslant \varepsilon) \leqslant \exp \left (-2 N \varepsilon^{2}\right)$，对于假设空间，则有：

$$
\begin{aligned}
P (\exists f \in \mathcal{F}: R (f)-\hat{R}(f) \geqslant \varepsilon) & =P\left (\bigcup_{f \in \mathcal{F}}\{R (f)-\hat{R}(f) \geqslant \varepsilon\}\right) \\
& \leqslant \sum_{f \in \mathcal{F}} P (R (f)-\hat{R}(f) \geqslant \varepsilon) \\
& \leqslant d \exp \left (-2 N \varepsilon^{2}\right)
\end{aligned}
$$

其中，$\exists f \in \mathcal{F}: R (f)-\hat{R}(f) \geqslant \varepsilon$ 意为存在假设空间 $\mathcal{F}$ 中的 $f$，满足 $R (f)-\hat{R}(f) \geqslant \varepsilon$。因为每个 $f$ 之间为互斥事件，所以此概率为每个 $f$ 的合取，等于概率的和。

存在 $f$ 满足上述条件与没有 $f$ 满足上述条件互为补集，因此得到下式：

$$
P (R (f)-\hat{R}(f)<\varepsilon) \geqslant 1-d \exp \left (-2 N \varepsilon^{2}\right)
$$

定义 $\delta$ 如下：

$$
\delta=d \exp \left (-2 N \varepsilon^{2}\right)
$$

通过上式，若 $\delta$、$d$ 与 $N$ 已知，不难求出 $\varepsilon(d, N, \delta)=\sqrt{\frac{1}{2 N}\left(\log d+\log \frac{1}{\delta}\right)}$。

同时则有：

$$
P (R (f)<\hat{R}(f)+\varepsilon) \geqslant 1-\delta
$$

即至少以概率 $1-\delta$ 有 $R(f)<\hat{R}(f)+\varepsilon$。所以得到结论，有 $1-\delta$ 概率满足下式：

$$
R\left (f_{N}\right) \leqslant \hat{R}\left (f_{N}\right)+\varepsilon (d, N, \delta)
$$

得证。

## 生成模型与判别模型

监督学习的任务就是学习一个模型，应用这一模型，对给定的输入预测相应的输出。这个模型的一般形式为决策函数：

$$
Y=f (X)
$$

或者条件概率分布：

$$
P (Y \mid X)
$$

监督学习方法又可以分为生成方法（generative approach）和判别方法（discriminative approach））。所学到的模型分别称为生成模型（generative model）和判别模型（discriminative model）。

生成方法由数据学习联合概率分布 $P (X, Y)$，然后求出条件概率分布 $P (Y \mid X)$ 作为预测的模型，即生成模型：

$$
P (Y \mid X)=\frac{P (X, Y)}{P (X)}
$$

这样的方法之所以称为生成方法，是因为模型表示了给定输入 $X$ 产生输出 $Y$ 的生成关系。

判别方法由数据直接学习决策函数 $f (X)$ 或者条件概率分布 $P (Y \mid X)$ 作为预测的模型，即判别模型。判别方法关心的是对给定的输入 $X$，应该预测什么样的输出 $Y$。

也就是说，一个聚焦于二者的内在联系，一个聚焦于二者的输入输出。

在监督学习中，生成方法和判别方法各有优缺点，适合于不同条件下的学习问题。

### 生成方法的特点

生成方法可以还原出联合概率分布 $P (X, Y)$，而判别方法则不能；生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快地收玫于真实模型；当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。

### 判别方法的特点

判别方法直接学习的是条件概率 $P (Y \mid X)$ 或决策函数 $f (X)$，直接面对预测，往往学习的准确率更高；由于直接学习 $P (Y \mid X)$ 或 $f (X)$，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。

## 监督学习应用

### 分类问题

分类是监督学习的一个核心问题。在监督学习中，当输出变量 $Y$ 取有限个离散值时，预测问题便成为分类问题。这时，输入变量 $X$ 可以是离散的，也可以是连续的。监督学习从数据中学习一个分类模型或分类决策函数，称为分类器（classifier）。分类器对新的输入进行输出的预测，称为分类（classification）。可能的输出称为类别（class）。分类的类别为多个时，称为多类分类问题。本书主要讨论二类分类问题。

也就是说，分类问题是根据输入的数据输出一个标签，或者说类别的问题。

分类问题包括学习和分类两个过程。在学习过程中，根据已知的训练数据集利用有效的学习方法学习一个分类器；在分类过程中，利用学习的分类器对新的输入实例进行分类。分类问题可用下图描述。图中 $\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)$ 是训练数据集，学习系统由训练数据学习一个分类器 $P(Y \mid X)$ 或 $Y=f(X)$；分类系统通过学到的分类器 $P(Y \mid X)$ 或 $Y=f(X)$ 对于新的输入实例 $x_{N+1}$ 进行分类，即预测其输出的类标记 $y_{N+1}$。

![[分类问题.png]]

评价分类器性能的指标一般是分类准确率（accuracy），其定义是：对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。也就是损失函数是 0-1 损失时测试数据集上的 [[统计学习方法学习笔记#训练误差与测试误差|准确率]]。

对于二类分类问题常用的评价指标是精确率（precision）与召回率（recall）。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，$4$ 种情况出现的总数分别记作：

- $\mathrm{TP}$ — 将正类预测为正类数；
- $\mathrm{FN}$ — 将正类预测为负类数；
- $\mathrm{FP}$ — 将负类预测为正类数；
- $\mathrm{TN}$ — 将负类预测为负类数。

于是可以定义准确率为：

$$
P=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}
$$

同理，召回率定义为：

$$
R=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}
$$

召回率可以理解为，将正样本召回的时候召回的比例，也就是在分类的时候，正样本中被认为是正样本的比例。

此外，还有 $F_{1}$ 值，是精确率和召回率的调和均值，即：

$$
\begin{aligned}
\quad\quad&\quad\frac{2}{F_{1}}=\frac{1}{P}+\frac{1}{R}\\
F_{1}&=\frac{2 \mathrm{TP}}{2 \mathrm{TP}+\mathrm{FP}+\mathrm{FN}}
\end{aligned}
$$

### 标注问题

标注（tagging）也是一个监督学习问题。可以认为标注问题是分类问题的一个推广，标注问题又是更复杂的结构预测（structure prediction）问题的简单形式。

标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测。注意，可能的标记个数是有限的，但其组合所成的标记序列的个数是依序列长度呈指数级增长的（应该不难理解，若总共序列长度为 $n$，标记个数为 $m$，则组合所称的标记序列个数可以是 $n^m$ 个）。

标注问题分为学习和标注两个过程。首先给定一个训练数据集

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

这里，$x_{i}=\left (x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}}, i=1,2, \cdots, N$，是输入观测序列；$y_{i}=\left (y_{i}^{(1)}, y_{i}^{(2)}, \cdots\right. ,  \left. y_{i}^{(n)}\right)^{\mathrm{T}}$ 是相应的输出标记序列；$n$ 是序列的长度，对不同样本可以有不同的值。学习系统基于训练数据集构建一个模型，表示为条件概率分布：

$$
P\left (Y^{(1)}, Y^{(2)}, \cdots, Y^{(n)} \mid X^{(1)}, X^{(2)}, \cdots, X^{(n)}\right)
$$

这里，每一个 $X^{(i)}(i=1,2, \cdots, n)$ 取值为所有可能的观测，每一个 $Y^{(i)}(i=   1,2, \cdots, n)$ 取值为所有可能的标记，一般 $n \ll N$。标注系统按照学习得到的条件概率分布模型，对新的输入观测序列找到相应的输出标记序列。具体地，对一个观测序列 $x_{N+1}=\left (x_{N+1}^{(1)}, x_{N+1}^{(2)}, \cdots, x_{N+1}^{(n)}\right)^{\mathrm{T}}$ 找到使条件概率 $P\left (\left (y_{N+1}^{(1)}, y_{N+1}^{(2)}, \cdots, y_{N+1}^{(n)}\right)^{\mathrm{T}} \mid\left (x_{N+1}^{(1)}\right.\right. ,  \left.\left. x_{N+1}^{(2)}, \cdots, x_{N+1}^{(n)}\right)^{\mathrm{T}}\right)$ 最大的标记序列 $y_{N+1}=\left (y_{N+1}^{(1)}, y_{N+1}^{(2)}, \cdots, y_{N+1}^{(n)}\right)^{\mathrm{T}}$（也就是说此处使用了 [[统计学习方法学习笔记#^MAP|后验概率最大估计（MAP）]]）。

![[标注问题.png]]

评价标注模型的指标与评价分类模型的指标一样，常用的有标注准确率、精确率和召回率。其定义与分类模型相同。

标注问题在信息抽取、自然语言处理等领域被广泛应用，是这些领域的基本问题。例如，自然语言处理中的词性标注（part of speech tagging）就是一个典型的标注问题：给定一个由单词组成的句子，对这个句子中的每一个单词进行词性标注，即对一个单词序列预测其对应的词性标记序列。

举一个信息抽取的例子。从英文文章中抽取基本名词短语（base noun phrase）。为此，要对文章进行标注。英文单词是一个观测，英文句子是一个观测序列，标记表示名词短语的“开始”、“结束”或“其他”（分别以 B, E, O 表示），标记序列表示英文句子中基本名词短语的所在位置。信息抽取时，将标记“开始”到标记“结束”的单词作为名词短语。例如，给出以下的观测序列，即英文句子，标注系统产生相应的标记序列，即给出句子中的基本名词短语。

输入：At Microsoft Research, we have an insatiable curiosity and the desire to create new technology that will help define the computing experience.

输出：At/O Microsoft/B Research/E, we/O have/O an/O insatiable/B curiosity/E and/O the/O desire/BE to/O create/O new/B technology/E that/O will/O help/O define/O the/O computing/B experience/E.

在这里就可以解释上面的一些概念，在本例子中，输入观测序列是每一个单词，而观测序列的长度是句子中单词的数量，输出的是与每一个单词匹配的标签，组合起来也就是输出标记序列。

### 回归问题

回归（regression）是监督学习的另一个重要问题。回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系，特别是当输入变量的值发生变化时，输出变量的值随之发生的变化。回归模型正是表示从输入变量到输出变量之间映射的函数。**回归问题的学习等价于函数拟合**：选择一条函数曲线使其很好地拟合已知数据且很好地预测末知数据。

回归问题分为学习和预测两个过程。首先给定一个训练数据集：

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

这里，$x_{i} \in \mathbf{R}^{n}$ 是输入，$y \in \mathbf{R}$ 是对应的输出，$i=1,2, \cdots, N$。学习系统基于训练数据构建一个模型，即函数 $Y=f (X)$；对新的输入 $x_{N+1}$，预测系统根据学习的模型 $Y=f (X)$ 确定相应的输出 $y_{N+1}$。

![[回归问题.png]]

回归问题按照输入变量的个数，分为一元回归和多元回归；按照输入变量和输出变量之间关系的类型即模型的类型，分为线性回归和非线性回归。

回归学习最常用的损失函数是平方损失函数，在此情况下，回归问题可以由著名的[[最小二乘法]]（least squares）求解。

# 感知机

## 概念

首先介绍感知机，感知机是从输入空间到输出空间的以下模型：

$$
f (x)=\operatorname{sign}(w \cdot x+b)
$$

其中 $w\in R^n$ 与 $b\in R$ 均为模型的参数，前者被称为权值或者权值向量，而后者则被称为偏置。

上式中的 $\operatorname{sign}(x)$ 定义如下：

$$
\operatorname{sign}(x)=\left\{\begin{array}{ll}
+1, & x \geqslant 0 \\
-1, & x<0
\end{array}\right.
$$

不难理解，感知机是一种线性分类模型，通过将输入进行运算之后得到一个值，通过值的正负判断这个输入“是”或者“不是”需要的结果。

对于感知机的几何解释是，线性方程：

$$
w \cdot x+b=0
$$

是特征空间 $R^n$ 中的一个超平面（可以理解为是三维空间的平面的推广，可以将空间分为两部分）$S$，其中 $w$ 为超平面的法向量，$b$ 则为超平面的截距。

![[感知机.png]]

## 学习策略

感知机学习，由训练数据集（实例的特征向量及类别）：

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

其中，$x_{i} \in \mathcal{X}=\mathbf{R}^{n},y_{i} \in \mathcal{Y}=\{+1,-1\}, i=1,2, \cdots, N$，求得感知机模型，即求得模型参数 $w, b$。感知机预测，通过学习得到的感知机模型，对于新的输入实例给出其对应的输出类别。

### 数据集的线性可分性

感知机只能处理线性可分的数据集，数学表示来说，数据集中，$x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{+1,-1\}, i=1,2, \cdots, N$，如果存在某个超平面 $S$： 

$$
w \cdot x+b=0
$$

能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有 $y_{i}=+1$ 的实例 $i$，有 $w \cdot x_{i}+b>0$，对所有 $y_{i}=-1$ 的实例 $i$，有 $w \cdot x_{i}+b<0$，则称数据集 $T$ 为线性可分数据集（linearly separable data set）；否则，称数据集 $T$ 线性不可分。

而一种直观的理解是，正实例点与负实例点能否被特征空间中的超平面 $S$ 完整分开，没有任何一个点不在其属于的一侧。

### 损失函数

若训练数据集是线性可分的，则感知机的学习目标则是找到一个能够将正实例点与负实例点完全正确分开的超平面，当然，假如说数据集并不线性可分，则无法用于训练感知机。训练感知机的过程就是寻找感知机的参数 $w,b$ 的过程。

损失函数的一个自然选择是误分类点的总数，也就是使用 0-1 损失函数。但是，这样的损失函数不是参数 $w ,  b$ 的连续可导函数，不易优化。

损失函数的另一个选择是误分类点到超平面 $S$ 的总距离，这是感知机所采用的，同时，距离也能描述当前超平面与所求超平面之间的偏差大小。为此，首先写出输入空间 $\mathbf{R}^{n}$ 中任一点 $x_{0}$ 到超平面 $S$ 的距离：

$$
\frac{1}{\|w\|}\left|w \cdot x_{0}+b\right|
$$

这其中，$\|w\|$ 是超平面法向量的 $L_2$ 范数，此处代入二维与三维情况，不难理解。

同时，误分类点可以通过下式选出：

$$
-y_{i}\left (w \cdot x_{i}+b\right)>0
$$

不难理解的是，误分类则 $y_i\neq f(x_i)$，而 $f (x)=\operatorname{sign}(w \cdot x+b)$，也就是说 $y_i$ 与 $w \cdot x_{i}+b$ 异号，则二者相乘取负，得到的值应当大于 $0$。

因为 $y_i=\pm 1$，因此 $\frac{1}{\|w\|}\left|w \cdot x_{0}+b\right|=-\frac{1}{\|w\|} y_{i}\left (w \cdot x_{i}+b\right)$，故将任意一点到超平面的距离写为：

$$
-\frac{1}{\|w\|} y_{i}\left (w \cdot x_{i}+b\right)
$$

如此消去了绝对值，便于了求导。

对于误分类点的总距离，则得到：

$$
-\frac{1}{\|w\|} \sum_{x_{i} \in M} y_{i}\left (w \cdot x_{i}+b\right)
$$

不考虑 $\frac{1}{\|w\|}$ 作为参数，则得到损失函数：

$$
L (w, b)=-\sum_{x_{i} \in M} y_{i}\left (w \cdot x_{i}+b\right)
$$

## 学习算法

### 原始形式

对于感知机算法，通过上方对于损失函数的定义，可以得到其本质上就是对以下问题的最优化问题：

给定一个数据集 $T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}$，其中 $x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{+1,-1\}, i=1,2, \cdots, N$，求参数 $w,b$，使其为以下最小化问题的解：

$$
\min _{w, b} L (w, b)=-\sum_{x_{i} \in M} y_{i}\left (w \cdot x_{i}+b\right)
$$

若采用 [[最小二乘法#随机梯度下降（Stochastic Gradient Descent）|随机梯度下降法]] 进行优化，则首先任意选择一个平面，其对应参数为 $w_0,b_0$，则存在梯度：

$$
\begin{aligned}
\nabla_{w} L (w, b)&=-\sum\limits_{x_{i} \in M} y_{i} x_{i} \\
\nabla_{b} L (w, b)&=-\sum\limits_{x_{i} \in M} y_{i}
\end{aligned}
$$

值得注意的是，由于使用随机梯度下降法，每一次仅使用一个误分类点使梯度下降。上式中的 $\sum$ 在随机梯度下降法中只取一个值，若为小批量，则取若干值。

当选择步长 $\eta$ 之后，则存在以下的迭代：

$$
\begin{array}{l}
w \leftarrow w+\eta y_{i} x_{i}\\
b \leftarrow b+\eta y_{i}
\end{array}
$$

之所以是 $+$ 而非 $-$，是因为需要沿梯度下降，也就是减去梯度。

$$
y_{i}\left (\hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i}\right)=y_{i}\left (w_{\mathrm{opt}} \cdot x_{i}+b_{\mathrm{opt}}\right) \geqslant \gamma
$$

于是可以给出学习算法的基础形式：

输入：训练数据集 $T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}$，其中 $x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in   \mathcal{Y}=\{-1,+1\}, i=1,2, \cdots, N$；学习率 $\eta (0<\eta \leqslant 1)$；

输出:：$w, b$；感知机模型 $f (x)=\operatorname{sign}(w \cdot x+b)$。

1. 选取初值 $w_{0}, b_{0}$；
2. 在训练集中选取数据 $\left (x_{i}, y_{i}\right)$；
3. 如果 $y_{i}\left (w \cdot x_{i}+b\right) \leqslant 0$，

$$
\begin{array}{l}
w \leftarrow w+\eta y_{i} x_{i} \\
b \leftarrow b+\eta y_{i}
\end{array}
$$

4. 转至（2），直至训练集中没有误分类点。

### 算法的收敛性

为了便于叙述与推导, 将偏置 $b$ 并入权重向量 $w$，记作 $\hat{w}=\left(w^{\mathrm{T}}, b\right)^{\mathrm{T}}$，同样也将输入向量加以扩充，加进常数 $1$，记作 $\hat{x}=\left(x^{\mathrm{T}}, 1\right)^{\mathrm{T}}$。这样，$\hat{x} \in \mathbf{R}^{n+1}, \hat{w} \in \mathbf{R}^{n+1}$。显然，$\hat{w} \cdot \hat{x}=w \cdot x+b$。

#### Novikoff 定理

设训练数据集 $T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}$ 是线性可分的，其中 $x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_{i} \in \mathcal{Y}=\{-1,+1\}, i=1,2, \cdots, N$，则：

1. 存在满足条件 $\left\|\hat{w}_{\mathrm{opt}}\right\|=1$ 的超平面 $\hat{w}_{\mathrm{opt}} \cdot \hat{x}=w_{\mathrm{opt}} \cdot x+b_{\mathrm{opt}}=0$ 将训练数据集完全正确分开；且存在 $\gamma>0$，对所有 $i=1,2, \cdots, N$ 

$$
y_{i}\left (\hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i}\right)=y_{i}\left (w_{\mathrm{opt}} \cdot x_{i}+b_{\mathrm{opt}}\right) \geqslant \gamma
$$

2. 令 $R=\max\limits _{1 \leqslant i \leqslant N}\left\|\hat{x}_{i}\right\|$，则感知机算法在训练数据集上的误分类次数 $k$ 满足不等式：

$$
k \leqslant\left (\frac{R}{\gamma}\right)^{2}
$$

以下给出证明：

首先由于训练数据集是线性可分的，所以存在超平面可以将数据集完全正确分开，取此超平面为 $\hat{w}_{\mathrm{opt}} \cdot \hat{x}=w_{\mathrm{opt}} \cdot x+b_{\mathrm{opt}}=0$，使 $\|\hat{w}_{\mathrm{opt}}\|=1$。由于对于有限的 $i=1,2,\cdots,N$，均有：

$$
y_{i}\left (\hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i}\right)=y_{i}\left (w_{\mathrm{opt}} \cdot x_{i}+b_{\mathrm{opt}}\right)>0
$$

上式不难理解，为对于数据的判别式，由于全部数据均已经正确，所以可以得到 $y_i=\operatorname{sign}(\hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i})$，也就是 $y_i$ 与 $\hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i}$ 同号且不为零，因此成绩大于 $0$。

所以存在：

$$
\gamma=\min _{i}\left\{y_{i}\left (w_{\mathrm{opt}} \cdot x_{i}+b_{\mathrm{opt}}\right)\right\}
$$

使：

$$
y_{i}\left (\hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i}\right)=y_{i}\left (w_{\mathrm{opt}} \cdot x_{i}+b_{\mathrm{opt}}\right) \geqslant \gamma
$$

以上，Novikoff 定理第一条得证。

$$
\hat{w}_{k-1}=\left (w_{k-1}^{\mathrm{T}}, b_{k-1}\right)^{\mathrm{T}}
$$

$$
y_{i}\left (\hat{w}_{k-1} \cdot \hat{x}_{i}\right)=y_{i}\left (w_{k-1} \cdot x_{i}+b_{k-1}\right) \leqslant 0
$$

$$
\begin{array}{l}
w_{k} \leftarrow w_{k-1}+\eta y_{i} x_{i} \\
b_{k} \leftarrow b_{k-1}+\eta y_{i}
\end{array}\\
$$

$$
\hat{w}_{k}=\hat{w}_{k-1}+\eta y_{i} \hat{x}_{i}
$$

$$
\hat{w}_{k} \cdot \hat{w}_{\text {opt }} \geqslant k \eta \gamma
$$

$$
\begin{aligned}
\hat{w}_{k} \cdot \hat{w}_{\mathrm{opt}} & =\hat{w}_{k-1} \cdot \hat{w}_{\mathrm{opt}}+\eta y_{i} \hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i} \\
& \geqslant \hat{w}_{k-1} \cdot \hat{w}_{\mathrm{opt}}+\eta \gamma
\end{aligned}
$$

$$
\begin{array}{c}
\hat{w}_{k} \cdot \hat{w}_{\mathrm{opt}} \geqslant \hat{w}_{k-1} \cdot \hat{w}_{\mathrm{opt}}+\eta \gamma \geqslant \hat{w}_{k-2} \cdot \hat{w}_{\mathrm{opt}}+2 \eta \gamma \geqslant \cdots \geqslant k \eta \gamma \\
\left\|\hat{w}_{k}\right\|^{2} \leqslant k \eta^{2} R^{2}
\end{array}
$$

$$
\begin{aligned}
\left\|\hat{w}_{k}\right\|^{2} & =\left\|\hat{w}_{k-1}\right\|^{2}+2 \eta y_{i} \hat{w}_{k-1} \cdot \hat{x}_{i}+\eta^{2}\left\|\hat{x}_{i}\right\|^{2} \\
& \leqslant\left\|\hat{w}_{k-1}\right\|^{2}+\eta^{2}\left\|\hat{x}_{i}\right\|^{2} \\
& \leqslant\left\|\hat{w}_{k-1}\right\|^{2}+\eta^{2} R^{2} \\
& \leqslant\left\|\hat{w}_{k-2}\right\|^{2}+2 \eta^{2} R^{2} \leqslant \cdots \\
& \leqslant k \eta^{2} R^{2}
\end{aligned}
$$

$$
\begin{array}{l}
k \eta \gamma \leqslant \hat{w}_{k} \cdot \hat{w}_{\mathrm{opt}} \leqslant\left\|\hat{w}_{k}\right\|\left\|\hat{w}_{\mathrm{opt}}\right\| \leqslant \sqrt{k} \eta R \\
k^{2} \gamma^{2} \leqslant k R^{2}
\end{array}
$$

$$
w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}
$$

$$
b=\sum_{i=1}^{N} \alpha_{i} y_{i}
$$

故一下给出感知机模型的训练方法：

输入：线性可分的数据集 $T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}$，其中 $x_{i} \in \mathbf{R}^{n}, y_{i} \in   \{-1,+1\}, i=1,2, \cdots, N$；学习率 $\eta (0<\eta \leqslant 1)$。

输出：$\alpha, b$；感知机模型 $f (x)=\operatorname{sign}\left (\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x+b\right)$，其中 $\alpha=   \left (\alpha_{1}, \alpha_{2}, \cdots, \alpha_{N}\right)^{\mathrm{T}}$。

1.  $\alpha \leftarrow 0$，$b \leftarrow 0$；
2. 在训练集中选取数据 $\left (x_{i}, y_{i}\right)$；
3. 如果 $y_{i}\left (\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x_{i}+b\right) \leqslant 0$，

$$
\begin{array}{c}
\alpha_{i} \leftarrow \alpha_{i}+\eta \\
b \leftarrow b+\eta y_{i}
\end{array}
$$

4. 转至 <u>2</u> 直到没有误分类数据。

$$
\boldsymbol{G}=\left[x_{i} \cdot x_{j}\right]_{N \times N}
$$

$$
\operatorname{conv}(S)=\left\{x=\sum_{i=1}^{k} \lambda_{i} x_{i} \mid \sum_{i=1}^{k} \lambda_{i}=1, \lambda_{i} \geqslant 0, i=1,2, \cdots, k\right\}
$$

# K 近邻算法

$$
y=\arg \max _{c_{j}} \sum_{x_{i} \in N_{k}(x)} I\left (y_{i}=c_{j}\right), \quad i=1,2, \cdots, N ; j=1,2, \cdots, K
$$

$$
L_{p}\left (x_{i}, x_{j}\right)=\left (\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{p}\right)^{\frac{1}{p}}
$$

$$
L_{2}\left (x_{i}, x_{j}\right)=\left (\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{2}\right)^{\frac{1}{2}}
$$

$$
L_{1}\left (x_{i}, x_{j}\right)=\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|
$$

$$
L_{\infty}\left (x_{i}, x_{j}\right)=\max _{l}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|
$$

$$
f: \mathbf{R}^{n} \rightarrow\left\{c_{1}, c_{2}, \cdots, c_{K}\right\}
$$

$$
P (Y \neq f (X))=1-P (Y=f (X))
$$

$$
\frac{1}{k} \sum_{x_{i} \in N_{k}(x)} I\left (y_{i} \neq c_{j}\right)=1-\frac{1}{k} \sum_{x_{i} \in N_{k}(x)} I\left (y_{i}=c_{j}\right)
$$

# 朴素贝叶斯方法

设输入空间 $\mathcal{X}\subseteq R^n$ 为维向量的集合，输出空间为类标记集合 $\mathcal{Y}=\{c_1, c_2,\cdots, c_K\}$。输入为特征向量 $x\in \mathcal{X}$，输出为类标记 $y\in \mathcal{Y}$。$X$ 是定义在输入空间 $\mathcal{X}$ 上的随机向量，$Y$ 是定义在输出空间 $\mathcal{Y}$ 上的随机变量。$P (X, Y)$ 是 X 和 Y 的联合概率分布。训练数据集：

$$T=\{(x_1, y_1), (x_2, y_2),\cdots, (x_N, y_N)\}$$

由 $P (X, Y)$ 独立同分布产生。

朴素贝叶斯法通过训练数据集学习联合概率分布 $P (X, Y)$。具体地，学习以下先
验概率分布及条件概率分布。

先验概率分布：

$$P (Y=C_k), k=1,2,\cdots, K$$

条件概率分布：

$$
P\left (X=x \mid Y=c_{k}\right)=P\left (X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} \mid Y=c_{k}\right), \quad k=1,2, \cdots, K
$$

$$
\begin{aligned}
P\left (X=x \mid Y=c_{k}\right) & =P\left (X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} \mid Y=c_{k}\right) \\
& =\prod_{j=1}^{n} P\left (X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
\end{aligned}
$$

$$
P\left (Y=c_{k} \mid X=x\right)=\frac{P\left (X=x \mid Y=c_{k}\right) P\left (Y=c_{k}\right)}{\sum_{k} P\left (X=x \mid Y=c_{k}\right) P\left (Y=c_{k}\right)}
$$

$$
P\left (Y=c_{k} \mid X=x\right)=\frac{P\left (Y=c_{k}\right) \prod\limits_{j} P\left (X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}{\sum\limits_{k} P\left (Y=c_{k}\right) \prod\limits_{j} P\left (X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}, \quad k=1,2, \cdots, K
$$

$$
y=f (x)=\arg \max _{c_{k}} \frac{P\left (Y=c_{k}\right) \prod\limits_{j} P\left (X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}{\sum\limits_{k} P\left (Y=c_{k}\right) \prod\limits_{j} P\left (X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}
$$

$$
y=\arg \max _{c_{k}} P\left (Y=c_{k}\right) \prod_{j} P\left (X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
$$

0-1 损失函数：

$$
L (Y, f (X))=\left\{\begin{array}{cc}
1, & Y \neq f (X) \\
0, & Y=f (X)
\end{array}\right.
$$

期望风险函数：

$$
R_{\exp }(f)=E[L (Y, f (X))]
$$

$$
R_{\exp }(f)=E[L (Y, f (X))]
$$

$$
R_{\exp }(f)=E_{X} \sum_{k=1}^{K}\left[L\left (c_{k}, f (X)\right)\right] P\left (c_{k} \mid X\right)
$$

$$
\begin{aligned}
f (x) & =\arg \min _{y \in \mathcal{Y}} \sum_{k=1}^{K} L\left (c_{k}, y\right) P\left (c_{k} \mid X=x\right) \\
& =\arg \min _{y \in \mathcal{Y}} \sum_{k=1}^{K} P\left (y \neq c_{k} \mid X=x\right) \\
& =\arg \min _{y \in \mathcal{Y}}\left (1-P\left (y=c_{k} \mid X=x\right)\right) \\
& =\arg \max _{y \in \mathcal{Y}} P\left (y=c_{k} \mid X=x\right)
\end{aligned}
$$

$$
f (x)=\arg \max _{c_{k}} P\left (c_{k} \mid X=x\right)
$$

先验概率的极大似然估计：

$$
P\left (Y=c_{k}\right)=\frac{\sum\limits_{i=1}^{N} I\left (y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K
$$

$$
\begin{array}{l}
P\left (X^{(j)}=a_{j l} \mid Y=c_{k}\right)=\frac{\sum\limits_{i=1}^{N} I\left (x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)}{\sum\limits_{i=1}^{N} I\left (y_{i}=c_{k}\right)}\\
j=1,2, \cdots, n ; \quad l=1,2, \cdots, S_{j} ; \quad k=1,2, \cdots, K
\end{array}
$$

$$
P\left (Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left (y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K
$$

$$
\begin{array}{c}
P\left (X^{(j)}=a_{j l} \mid Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left (x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)}{\sum_{i=1}^{N} I\left (y_{i}=c_{k}\right)} \\
j=1,2, \cdots, n ; \quad l=1,2, \cdots, S_{j} ; \quad k=1,2, \cdots, K
\end{array}
$$

$$
P\left (Y=c_{k}\right) \prod_{j=1}^{n} P\left (X^{(j)}=x^{(j)} \mid Y=c_{k}\right), \quad k=1,2, \cdots, K
$$

$$
y=\arg \max _{c_{k}} P\left (Y=c_{k}\right) \prod_{j=1}^{n} P\left (X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
$$

条件概率的贝叶斯估计：

$$
P_{\lambda}\left (X^{(j)}=a_{j l} \mid Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left (x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)+\lambda}{\sum_{i=1}^{N} I\left (y_{i}=c_{k}\right)+S_{j} \lambda}
$$

$$
\begin{array}{l}
P_{\lambda}\left (X^{(j)}=a_{j l} \mid Y=c_{k}\right)>0 \\
\sum_{l=1}^{S_{j}} P\left (X^{(j)}=a_{j l} \mid Y=c_{k}\right)=1
\end{array}
$$

先验概率的贝叶斯估计：

$$
P_{\lambda}\left (Y=c_{k}\right)=\frac{\sum\limits_{i=1}^{N} I\left (y_{i}=c_{k}\right)+\lambda}{N+K \lambda}
$$

# 决策树

随机变量 X 的熵定义为：

$$
P\left (X=x_{i}\right)=p_{i}, \quad i=1,2, \cdots, n
$$

$$
H (X)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$

$$
H (p)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$

$$
0 \leqslant H (p) \leqslant \log n
$$

$$
P (X=1)=p, \quad P (X=0)=1-p, \quad 0 \leqslant p \leqslant 1
$$

$$
H (p)=-p \log _{2} p-(1-p) \log _{2}(1-p)
$$

$$
P\left (X=x_{i}, Y=y_{j}\right)=p_{i j}, \quad i=1,2, \cdots, n ; \quad j=1,2, \cdots, m
$$

$$ 
H (Y \mid X)=\sum\limits_{i=1}^{n} p_{i} H\left (Y \mid X=x_{i}\right)
$$

这里 $p_{i}=P\left (X=x_{i}\right), i=1,2, \cdots, n$。

信息增益：

$$
g (D, A)=H (D)-H (D \mid A)
$$

$$
H (D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}
$$

$$
H (D \mid A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left (D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}
$$

$$
g (D, A)=H (D)-H (D \mid A)
$$

信息增益比：

$$
g_{R}(D, A)=\frac{g (D, A)}{H_{A}(D)}
$$

$$
H_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log _{2} \frac{\left|D_{i}\right|}{|D|}
$$

$$
C_{\alpha}(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)+\alpha|T|
$$

$$
H_{t}(T)=-\sum_{k} \frac{N_{t k}}{N_{t}} \log \frac{N_{t k}}{N_{t}}
$$

$$
C (T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)=-\sum_{t=1}^{|T|} \sum_{k=1}^{K} N_{t k} \log \frac{N_{t k}}{N_{t}}
$$

$$
C_{\alpha}(T)=C (T)+\alpha|T|
$$

$$
C_{\alpha}\left (T_{A}\right) \leqslant C_{\alpha}\left (T_{B}\right)
$$

$$
D=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

$$
f (x)=\sum_{m=1}^{M} c_{m} I\left (x \in R_{m}\right)
$$

可以使用平方误差 $\sum_{x_{i} \in R_{m}}\left (y_{i}-f\left (x_{i}\right)\right)^{2}$

$$
\hat{c}_{m}=\operatorname{ave}\left (y_{i} \mid x_{i} \in R_{m}\right)
$$

$$
R_{1}(j, s)=\left\{x \mid x^{(j)} \leqslant s\right\} \quad R_{2}(j, s)=\left\{x \mid x^{(j)}>s\right\}
$$

$$
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left (y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left (y_{i}-c_{2}\right)^{2}\right]
$$

$$
\hat{c}_{1}=\operatorname{ave}\left (y_{i} \mid x_{i} \in R_{1}(j, s)\right) \quad \hat{c}_{2}=\operatorname{ave}\left (y_{i} \mid x_{i} \in R_{2}(j, s)\right)
$$

$$
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left (y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left (y_{i}-c_{2}\right)^{2}\right]
$$

$$
\begin{array}{c}
R_{1}(j, s)=\left\{x \mid x^{(j)} \leqslant s\right\}, \quad R_{2}(j, s)=\left\{x \mid x^{(j)}>s\right\} \\
\hat{c}_{m}=\frac{1}{N_{m}} \sum_{x_{i} \in R_{m}(j, s)} y_{i}, \quad x \in R_{m}, \quad m=1,2
\end{array}
$$

$$
f (x)=\sum_{m=1}^{M} \hat{c}_{m} I\left (x \in R_{m}\right)
$$

$$
\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left (1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}
$$

$$
\operatorname{Gini}(p)=2 p (1-p)
$$

$$
\operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left (\frac{\left|C_{k}\right|}{|D|}\right)^{2}
$$

$$
D_{1}=\{(x, y) \in D \mid A (x)=a\}, \quad D_{2}=D-D_{1}
$$

$$
\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left (D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left (D_{2}\right)
$$

$$
C_{\alpha}(T)=C (T)+\alpha|T|
$$

$$
C_{\alpha}(t)=C (t)+\alpha
$$

$$
C_{\alpha}\left (T_{t}\right)=C\left (T_{t}\right)+\alpha\left|T_{t}\right|
$$

$$
C_{\alpha}\left (T_{t}\right)<C_{\alpha}(t)
$$

$$
C_{\alpha}\left (T_{t}\right)=C_{\alpha}(t)
$$

$$
g (t)=\frac{C (t)-C\left (T_{t}\right)}{\left|T_{t}\right|-1}
$$

$$
\begin{aligned}
g (t) & =\frac{C (t)-C\left (T_{t}\right)}{\left|T_{t}\right|-1} \\
\alpha & =\min (\alpha, g (t))
\end{aligned}
$$

$$
\begin{array}{l}
g (D, A)=H (D)-H (D \mid A) \\
H (D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|} \\
H (D \mid A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left (D_{i}\right)
\end{array}
$$

$$
g_{R}(D, A)=\frac{g (D, A)}{H_{A}(D)}
$$

$$
\operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left (\frac{\left|C_{k}\right|}{|D|}\right)^{2}
$$

$$
\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left (D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left (D_{2}\right)
$$

# 逻辑斯谛回归与最大熵模型

$$
F (x)=P (X \leqslant x)=\frac{1}{1+\mathrm{e}^{-(x-\mu) / \gamma}}
$$

$$
f (x)=F^{\prime}(x)=\frac{\mathrm{e}^{-(x-\mu) / \gamma}}{\gamma\left (1+\mathrm{e}^{-(x-\mu) / \gamma}\right)^{2}}
$$

$$
F (-x+\mu)-\frac{1}{2}=-F (x+\mu)+\frac{1}{2}
$$

$$
P (Y=1 \mid x)=\frac{\exp (w \cdot x+b)}{1+\exp (w \cdot x+b)}
$$

$$
P (Y=0 \mid x)=\frac{1}{1+\exp (w \cdot x+b)}
$$

$$
P (Y=1 \mid x)=\frac{\exp (w \cdot x)}{1+\exp (w \cdot x)}
$$

$$
P (Y=0 \mid x)=\frac{1}{1+\exp (w \cdot x)}
$$

$$
\operatorname{logit}(p)=\log \frac{p}{1-p}
$$

$$
\log \frac{P (Y=1 \mid x)}{1-P (Y=1 \mid x)}=w \cdot x
$$

$$
P (Y=1 \mid x)=\frac{\exp (w \cdot x)}{1+\exp (w \cdot x)}
$$

$$
P (Y=1 \mid x)=\pi (x), \quad P (Y=0 \mid x)=1-\pi (x)
$$

$$
\prod_{i=1}^{N}\left[\pi\left (x_{i}\right)\right]^{y_{i}}\left[1-\pi\left (x_{i}\right)\right]^{1-y_{i}}
$$

$$
\begin{aligned}
L (w) & =\sum_{i=1}^{N}\left[y_{i} \log \pi\left (x_{i}\right)+\left (1-y_{i}\right) \log \left (1-\pi\left (x_{i}\right)\right)\right. \\
& =\sum_{i=1}^{N}\left[y_{i} \log \frac{\pi\left (x_{i}\right)}{1-\pi\left (x_{i}\right)}+\log \left (1-\pi\left (x_{i}\right)\right)\right] \\
& =\sum_{i=1}^{N}\left[y_{i}\left (w \cdot x_{i}\right)-\log \left (1+\exp \left (w \cdot x_{i}\right)\right]\right.
\end{aligned}
$$

$$
P (Y=1 \mid x)=\frac{\exp (\hat{w} \cdot x)}{1+\exp (\hat{w} \cdot x)} 
$$

$$
P (Y=0 \mid x)=\frac{1}{1+\exp (\hat{w} \cdot x)}
$$

$$
P (Y=k \mid x)=\frac{\exp \left (w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left (w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1 
$$

$$
P (Y=K \mid x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp \left (w_{k} \cdot x\right)}
$$

$$
H (P)=-\sum_{x} P (x) \log P (x)
$$

$$
0 \leqslant H (P) \leqslant \log |X|
$$

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

$$
\begin{array}{l}
\tilde{P}(X=x, Y=y)=\frac{\nu (X=x, Y=y)}{N} \\
\tilde{P}(X=x)=\frac{\nu (X=x)}{N}
\end{array}
$$

$$
f (x, y)=\left\{\begin{array}{ll}
1, & x \text { 与 } y \text { 满足某一事实 } \\
0, & \text { 否则 }
\end{array}\right.
$$

$$
E_{\tilde{P}}(f)=\sum_{x, y} \tilde{P}(x, y) f (x, y)
$$

$$
E_{P}(f)=\sum_{x, y} \tilde{P}(x) P (y \mid x) f (x, y)
$$

$$
E_{P}(f)=E_{\tilde{P}}(f)
$$

$$
\sum_{x, y} \tilde{P}(x) P (y \mid x) f (x, y)=\sum_{x, y} \tilde{P}(x, y) f (x, y)
$$

$$
\mathcal{C} \equiv\left\{P \in \mathcal{P} \mid E_{P}\left (f_{i}\right)=E_{\tilde{P}}\left (f_{i}\right), \quad i=1,2, \cdots, n\right\}
$$

$$
H (P)=-\sum_{x, y} \tilde{P}(x) P (y \mid x) \log P (y \mid x)
$$

$$
\begin{array}{ll}
\max _{P \in \mathbf{C}} & H (P)=-\sum_{x, y} \tilde{P}(x) P (y \mid x) \log P (y \mid x) \\
\text { s.t. } & E_{P}\left (f_{i}\right)=E_{\tilde{P}}\left (f_{i}\right), \quad i=1,2, \cdots, n \\
& \sum_{y} P (y \mid x)=1
\end{array}
$$

$$
\begin{array}{ll}
\min _{P \in \mathbf{C}} & -H (P)=\sum_{x, y} \tilde{P}(x) P (y \mid x) \log P (y \mid x) \\
\text { s.t. } & E_{P}\left (f_{i}\right)-E_{\tilde{P}}\left (f_{i}\right)=0, \quad i=1,2, \cdots, n \\
& \sum_{y} P (y \mid x)=1
\end{array}
$$

$$
\begin{aligned}
L (P, w) \equiv & -H (P)+w_{0}\left (1-\sum_{y} P (y \mid x)\right)+\sum_{i=1}^{n} w_{i}\left (E_{\tilde{P}}\left (f_{i}\right)-E_{P}\left (f_{i}\right)\right) \\
= & \sum_{x, y} \tilde{P}(x) P (y \mid x) \log P (y \mid x)+w_{0}\left (1-\sum_{y} P (y \mid x)\right)+ \\
& \sum_{i=1}^{n} w_{i}\left (\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P (y \mid x) f_{i}(x, y)\right)
\end{aligned}
$$

$$
\min _{P \in \mathbf{C}} \max _{w} L (P, w)
$$

$$
\max _{w} \min _{P \in \mathbf{C}} L (P, w)
$$

$$
\Psi (w)=\min _{P \in \mathbf{C}} L (P, w)=L\left (P_{w}, w\right)
$$

$$
P_{w}=\arg \min _{P \in \mathbf{C}} L (P, w)=P_{w}(y \mid x)
$$

$$
\begin{aligned}
\frac{\partial L (P, w)}{\partial P (y \mid x)} & =\sum_{x, y} \tilde{P}(x)(\log P (y \mid x)+1)-\sum_{y} w_{0}-\sum_{x, y}\left (\tilde{P}(x) \sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
& =\sum_{x, y} \tilde{P}(x)\left (\log P (y \mid x)+1-w_{0}-\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
\end{aligned}
$$

$$
P (y \mid x)=\exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)+w_{0}-1\right)=\frac{\exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)}{\exp \left (1-w_{0}\right)}
$$

$$
P_{w}(y \mid x)=\frac{1}{Z_{w}(x)} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
Z_{w}(x)=\sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
\max _{w} \Psi (w)
$$

$$
w^{*}=\arg \max _{w} \Psi (w)
$$

$$
L_{\tilde{P}}\left (P_{w}\right)=\log \prod_{x, y} P (y \mid x)^{\tilde{P}(x, y)}=\sum_{x, y} \tilde{P}(x, y) \log P (y \mid x)
$$

$$
\begin{aligned}
L_{\tilde{P}}\left (P_{w}\right) & =\sum_{x, y} \tilde{P}(x, y) \log P (y \mid x) \\
& =\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x, y} \tilde{P}(x, y) \log Z_{w}(x) \\
& =\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)
\end{aligned}
$$

$$
\begin{aligned}
\Psi (w)= & \sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) \log P_{w}(y \mid x)+ \\
& \sum_{i=1}^{n} w_{i}\left (\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) f_{i}(x, y)\right) \\
= & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)+\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x)\left (\log P_{w}(y \mid x)-\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
= & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) \log Z_{w}(x) \\
= & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)
\end{aligned}
$$

$$
\Psi (w)=L_{\tilde{P}}\left (P_{w}\right)
$$

$$
P_{w}(y \mid x)=\frac{1}{Z_{w}(x)} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
Z_{w}(x)=\sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
P_{w}(y \mid x)=\frac{1}{Z_{w}(x)} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
Z_{w}(x)=\sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
L (w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)
$$

$$
\begin{aligned}
L (w+\delta)-L (w) & =\sum_{x, y} \tilde{P}(x, y) \log P_{w+\delta}(y \mid x)-\sum_{x, y} \tilde{P}(x, y) \log P_{w}(y \mid x) \\
& =\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log \frac{Z_{w+\delta}(x)}{Z_{w}(x)}
\end{aligned}
$$

$$
-\log \alpha \geqslant 1-\alpha, \quad \alpha>0
$$

$$
\begin{aligned}
L (w+\delta)-L (w) & \geqslant \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \frac{Z_{w+\delta}(x)}{Z_{w}(x)} \\
& =\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \exp \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)
\end{aligned}
$$

$$
A (\delta \mid w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \exp \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)
$$

$$
L (w+\delta)-L (w) \geqslant A (\delta \mid w)
$$

$$
f^{\#}(x, y)=\sum_{i} f_{i}(x, y)
$$

$$
\begin{aligned}
A (\delta \mid w)= & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \exp \left (f^{\#}(x, y) \sum_{i=1}^{n} \frac{\delta_{i} f_{i}(x, y)}{f^{\#}(x, y)}\right)
\end{aligned}
$$

$$
\frac{f_{i}(x, y)}{f^{\#}(x, y)} \geqslant 0 \text { 且 } \sum_{i=1}^{n} \frac{f_{i}(x, y)}{f^{\#}(x, y)}=1
$$

$$
\exp \left (\sum_{i=1}^{n} \frac{f_{i}(x, y)}{f^{\#}(x, y)} \delta_{i} f^{\#}(x, y)\right) \leqslant \sum_{i=1}^{n} \frac{f_{i}(x, y)}{f^{\#}(x, y)} \exp \left (\delta_{i} f^{\#}(x, y)\right)
$$

$$
\begin{aligned}
A (\delta \mid w) \geqslant & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \sum_{i=1}^{n}\left (\frac{f_{i}(x, y)}{f^{\#}(x, y)}\right) \exp \left (\delta_{i} f^{\#}(x, y)\right)
\end{aligned}
$$

$$
B (\delta \mid w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \sum_{i=1}^{n}\left (\frac{f_{i}(x, y)}{f^{\#}(x, y)}\right) \exp \left (\delta_{i} f^{\#}(x, y)\right)
$$

$$
L (w+\delta)-L (w) \geqslant B (\delta \mid w)
$$

$$
\frac{\partial B (\delta \mid w)}{\partial \delta_{i}}=\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) f_{i}(x, y) \exp \left (\delta_{i} f^{\#}(x, y)\right)
$$

$$
\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) f_{i}(x, y) \exp \left (\delta_{i} f^{\#}(x, y)\right)=E_{\tilde{P}}\left (f_{i}\right)
$$

$$
\sum_{x, y} \tilde{P}(x) P (y \mid x) f_{i}(x, y) \exp \left (\delta_{i} f^{\#}(x, y)\right)=E_{\tilde{P}}\left (f_{i}\right)
$$

$$
f^{\#}(x, y)=\sum_{i=1}^{n} f_{i}(x, y)
$$

$$
w_{i} \leftarrow w_{i}+\delta_{i}
$$

$$
\delta_{i}=\frac{1}{M} \log \frac{E_{\tilde{P}}\left (f_{i}\right)}{E_{P}\left (f_{i}\right)}
$$

$$
\delta_{i}^{(k+1)}=\delta_{i}^{(k)}-\frac{g\left (\delta_{i}^{(k)}\right)}{g^{\prime}\left (\delta_{i}^{(k)}\right)}
$$

$$
P_{w}(y \mid x)=\frac{\exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)}{\sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)}
$$

$$
\min _{w \in \mathbf{R}^{n}} f (w)=\sum_{x} \tilde{P}(x) \log \sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)-\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)
$$

$$
g (w)=\left (\frac{\partial f (w)}{\partial w_{1}}, \frac{\partial f (w)}{\partial w_{2}}, \cdots, \frac{\partial f (w)}{\partial w_{n}}\right)^{\mathrm{T}}
$$

$$
\frac{\partial f (w)}{\partial w_{i}}=\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) f_{i}(x, y)-E_{\tilde{P}}\left (f_{i}\right), \quad i=1,2, \cdots, n
$$

$$
f\left (w^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geqslant 0} f\left (w^{(k)}+\lambda p_{k}\right)
$$

$$
B_{k+1}=B_{k}+\frac{y_{k} y_{k}^{\mathrm{T}}}{y_{k}^{\mathrm{T}} \delta_{k}}-\frac{B_{k} \delta_{k} \delta_{k}^{\mathrm{T}} B_{k}}{\delta_{k}^{\mathrm{T}} B_{k} \delta_{k}}
$$

$$
y_{k}=g_{k+1}-g_{k}, \quad \delta_{k}=w^{(k+1)}-w^{(k)}
$$

$$
\begin{array}{c}
P (Y=k \mid x)=\frac{\exp \left (w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left (w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1 \\
P (Y=K \mid x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp \left (w_{k} \cdot x\right)}
\end{array}
$$

$$
\begin{array}{l}
P_{w}(y \mid x)=\frac{1}{Z_{w}(x)} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
Z_{w}(x)=\sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
\end{array}
$$

$$
\begin{array}{ll}
\min & -H (P)=\sum_{x, y} \tilde{P}(x) P (y \mid x) \log P (y \mid x) \\
\text { s.t. } & P\left (f_{i}\right)-\tilde{P}\left (f_{i}\right)=0, \quad i=1,2, \cdots, n \\
& \sum_{y} P (y \mid x)=1
\end{array}
$$

# 支持向量机

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

$$
w^{*} \cdot x+b^{*}=0
$$

$$
f (x)=\operatorname{sign}\left (w^{*} \cdot x+b^{*}\right)
$$

$$
\hat{\gamma}_{i}=y_{i}\left (w \cdot x_{i}+b\right)
$$

$$
\hat{\gamma}=\min _{i=1, \cdots, N} \hat{\gamma}_{i}
$$

$$
\gamma_{i}=\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}
$$

$$
\gamma_{i}=-\left (\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right)
$$

$$
\gamma_{i}=y_{i}\left (\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right)
$$

$$
\gamma=\min _{i=1, \cdots, N} \gamma_{i}
$$

$$
\begin{aligned}
\gamma_{i} & =\frac{\hat{\gamma}_{i}}{\|w\|} \\
\gamma & =\frac{\hat{\gamma}}{\|w\|}
\end{aligned}
$$

$$
\begin{array}{ll}
\max\limits _{w, b} & \gamma \\
\text { s.t. } & y_{i}\left (\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right) \geqslant \gamma, \quad i=1,2, \cdots, N
\end{array}
$$
$$
\begin{array}{ll}
\max _{w, b} & \frac{\hat{\gamma}}{\|w\|} \\
\text { s.t. } & y_{i}\left (w \cdot x_{i}+b\right) \geqslant \hat{\gamma}, \quad i=1,2, \cdots, N
\end{array}
$$

$$
\begin{array}{ll}
\min _{w, b} & \frac{1}{2}\|w\|^{2} \\
\text { s.t. } & y_{i}\left (w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
\begin{array}{ll}
\min _{w} & f (w) \\
\text { s.t. } & g_{i}(w) \leqslant 0, \quad i=1,2, \cdots, k \\
& h_{i}(w)=0, \quad i=1,2, \cdots, l
\end{array}
$$

$$
\begin{array}{ll}
\min _{w, b} & \frac{1}{2}\|w\|^{2} \\
\text { s.t. } & y_{i}\left (w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
w^{*} \cdot x+b^{*}=0
$$

$$
f (x)=\operatorname{sign}\left (w^{*} \cdot x+b^{*}\right)
$$

$$
c \leqslant\|w\| \leqslant \frac{1}{2}\left\|w_{1}^{*}\right\|+\frac{1}{2}\left\|w_{2}^{*}\right\|=c
$$

$$
w_{1}^{*}=w_{2}^{*}
$$

$$
b_{1}^{*}=-\frac{1}{2}\left (w^{*} \cdot x_{1}^{\prime}+w^{*} \cdot x_{1}^{\prime \prime}\right), b_{2}^{*}=   -\frac{1}{2}\left (w^{*} \cdot x_{2}^{\prime}+w^{*} \cdot x_{2}^{\prime \prime}\right)
$$

$$
b_{1}^{*}-b_{2}^{*}=-\frac{1}{2}\left[w^{*} \cdot\left (x_{1}^{\prime}-x_{2}^{\prime}\right)+w^{*} \cdot\left (x_{1}^{\prime \prime}-x_{2}^{\prime \prime}\right)\right]
$$

$$
\begin{array}{c}
w^{*} \cdot x_{2}^{\prime}+b_{1}^{*} \geqslant 1=w^{*} \cdot x_{1}^{\prime}+b_{1}^{*} \\
w^{*} \cdot x_{1}^{\prime}+b_{2}^{*} \geqslant 1=w^{*} \cdot x_{2}^{\prime}+b_{2}^{*}
\end{array}
$$

$$
w^{*} \cdot\left (x_{1}^{\prime}-x_{2}^{\prime}\right)=0, w^{*} \cdot\left (x_{1}^{\prime \prime}-x_{2}^{\prime \prime}\right)=0
$$

$$
b_{1}^{*}-b_{2}^{*}=0
$$

$$
\begin{array}{l}
H_{1}: w \cdot x+b=1\\
H_{2}: w \cdot x+b=-1  
\end{array}
$$

$$
L (w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left (w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}
$$

$$
\max _{\alpha} \min _{w, b} L (w, b, \alpha)
$$

$\min _{w, b} L (w, b, \alpha)$

$$
\begin{array}{l}
\nabla_{w} L (w, b, \alpha)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0 \\
\nabla_{b} L (w, b, \alpha)=-\sum_{i=1}^{N} \alpha_{i} y_{i}=0
\end{array}
$$

$$
w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}
$$

$$
\sum_{i=1}^{N} \alpha_{i} y_{i}=0
$$

$$
\begin{aligned}
L (w, b, \alpha) & =\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left (x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} y_{i}\left (\left (\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j}\right) \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \\
& =-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left (x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
\end{aligned}
$$

$$
\min _{w, b} L (w, b, \alpha)=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left (x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
$$

$$
\begin{array}{ll}
\max _{\alpha} & -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left (x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } & \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
& \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
\begin{array}{ll}
\min _{\alpha} & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left (x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } & \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
& \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
\begin{array}{c}
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \\
b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left (x_{i} \cdot x_{j}\right)
\end{array}
$$

$$
\begin{array}{l}
\nabla_{w} L\left (w^{*}, b^{*}, \alpha^{*}\right)=w^{*}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}=0 \\
\nabla_{b} L\left (w^{*}, b^{*}, \alpha^{*}\right)=-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}=0 \\
\alpha_{i}^{*}\left (y_{i}\left (w^{*} \cdot x_{i}+b^{*}\right)-1\right)=0, \quad i=1,2, \cdots, N \\
y_{i}\left (w^{*} \cdot x_{i}+b^{*}\right)-1 \geqslant 0, \quad i=1,2, \cdots, N \\
\alpha_{i}^{*} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
y_{j}\left (w^{*} \cdot x_{j}+b^{*}\right)-1=0
$$

$$
b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left (x_{i} \cdot x_{j}\right)
$$

$$
\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left (x \cdot x_{i}\right)+b^{*}=0
$$

$$
f (x)=\operatorname{sign}\left (\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left (x \cdot x_{i}\right)+b^{*}\right)
$$

$$
\begin{array}{ll}
\min _{\alpha} & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left (x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } & \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
& \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

求得最优解 $\alpha^{*}=\left (\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{N}^{*}\right)^{\mathrm{T}}$。

$$
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}
$$

$$
b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left (x_{i} \cdot x_{j}\right)
$$

$$
w^{*} \cdot x+b^{*}=0
$$

$$
f (x)=\operatorname{sign}\left (w^{*} \cdot x+b^{*}\right)
$$

$$
\alpha_{i}^{*}\left (y_{i}\left (w^{*} \cdot x_{i}+b^{*}\right)-1\right)=0, \quad i=1,2, \cdots, N
$$

$$
y_{i}\left (w^{*} \cdot x_{i}+b^{*}\right)-1=0
$$

$$
w^{*} \cdot x_{i}+b^{*}= \pm 1
$$

$$
y_{i}\left (w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}
$$

$$
\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}
$$

$$
\begin{array}{ll}
\min _{w, b, \xi} & \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i} \\
\text { s.t. } & y_{i}\left (w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \\
& \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
w^{*} \cdot x+b^{*}=0
$$

$$
f (x)=\operatorname{sign}\left (w^{*} \cdot x+b^{*}\right)
$$

$$
\begin{array}{ll}
\min _{\alpha} & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left (x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } & \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
& 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{array}
$$

$$

L (w, b, \xi, \alpha, \mu) \equiv \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}-\sum_{i=1}^{N} \alpha_{i}\left (y_{i}\left (w \cdot x_{i}+b\right)-1+\xi_{i}\right)-\sum_{i=1}^{N} \mu_{i} \xi_{i}
$$

其中，$\alpha_{i} \geqslant 0$， $\mu_{i} \geqslant 0$。

$$
\begin{array}{c}
\nabla_{w} L (w, b, \xi, \alpha, \mu)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0 \\
\nabla_{b} L (w, b, \xi, \alpha, \mu)=-\sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
\nabla_{\xi_{i}} L (w, b, \xi, \alpha, \mu)=C-\alpha_{i}-\mu_{i}=0 \\
\end{array}
$$

$$
\begin{array}{c}
w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i} \\
\sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
C-\alpha_{i}-\mu_{i}=0
\end{array}
$$

$$
\min _{w, b, \xi} L (w, b, \xi, \alpha, \mu)=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left (x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}
$$

$$
\begin{array}{ll}
\max _{\alpha} & -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left (x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } & \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
& C-\alpha_{i}-\mu_{i}=0 \\
& \alpha_{i} \geqslant 0 \\
& \mu_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
0 \leqslant \alpha_{i} \leqslant C
$$

$$
\begin{array}{c}
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i} \\
b^{*}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}\left (x_{i} \cdot x_{j}\right)
\end{array}
$$

$$
\begin{array}{c}
\nabla_{w} L\left (w^{*}, b^{*}, \xi^{*}, \alpha^{*}, \mu^{*}\right)=w^{*}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}=0 \\
\nabla_{b} L\left (w^{*}, b^{*}, \xi^{*}, \alpha^{*}, \mu^{*}\right)=-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}=0 \\
\nabla_{\xi} L\left (w^{*}, b^{*}, \xi^{*}, \alpha^{*}, \mu^{*}\right)=C-\alpha^{*}-\mu^{*}=0
\end{array}
$$

$$
\begin{array}{c}
\alpha_{i}^{*}\left (y_{i}\left (w^{*} \cdot x_{i}+b^{*}\right)-1+\xi_{i}^{*}\right)=0 \\
\mu_{i}^{*} \xi_{i}^{*}=0
\end{array}
$$

$$
\begin{array}{c}
y_{i}\left (w^{*} \cdot x_{i}+b^{*}\right)-1+\xi_{i}^{*} \geqslant 0 \\
\xi_{i}^{*} \geqslant 0 \\
\alpha_{i}^{*} \geqslant 0 \\
\mu_{i}^{*} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left (x \cdot x_{i}\right)+b^{*}=0
$$

$$
f (x)=\operatorname{sign}\left (\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left (x \cdot x_{i}\right)+b^{*}\right)
$$

$$
\begin{array}{l} 
\min\limits _{\alpha} &\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left (x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
&\text { s.t. } \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
&0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{array}
$$

求得最优解 $\alpha^{*}=\left (\alpha_{1}^{*}, \alpha_{2}^{*}, \cdots, \alpha_{N}^{*}\right)^{\mathrm{T}}$。

$$
w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}
$$

$$
b^{*}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}\left (x_{i} \cdot x_{j}\right)
$$

$$
w^{*} \cdot x+b^{*}=0
$$

$$
f (x)=\operatorname{sign}\left (w^{*} \cdot x+b^{*}\right)
$$

$$
\sum_{i=1}^{N}\left[1-y_{i}\left (w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2}
$$

$$
L (y (w \cdot x+b))=[1-y (w \cdot x+b)]_{+}
$$

$$
[z]_{+}=\left\{\begin{array}{ll}
z, & z>0 \\
0, & z \leqslant 0
\end{array}\right.
$$

$$
\begin{array}{ll}
\min _{w, b, \xi} & \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i} \\
\text { s.t. } & y_{i}\left (w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \\
& \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
\min _{w, b} \sum_{i=1}^{N}\left[1-y_{i}\left (w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2}
$$

$$
\left[1-y_{i}\left (w \cdot x_{i}+b\right)\right]_{+}=\xi_{i}
$$

$$
\min _{w, b} \sum_{i=1}^{N} \xi_{i}+\lambda\|w\|^{2}
$$

若取 $\lambda=\frac{1}{2 C}$，则：

$$
\min _{w, b} \frac{1}{C}\left (\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}\right)
$$

$$
z=\phi (x)=\left (\left (x^{(1)}\right)^{2},\left (x^{(2)}\right)^{2}\right)^{\mathrm{T}}
$$

$$
w_{1} z^{(1)}+w_{2} z^{(2)}+b=0
$$

$$
\phi (x): \mathcal{X} \rightarrow \mathcal{H}
$$

$$
K (x, z)=\phi (x) \cdot \phi (z)
$$

$$
W (\alpha)=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left (x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}
$$

$$
\begin{aligned}
f (x) & =\operatorname{sign}\left (\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} \phi\left (x_{i}\right) \cdot \phi (x)+b^{*}\right) \\
& =\operatorname{sign}\left (\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} K\left (x_{i}, x\right)+b^{*}\right)
\end{aligned}
$$

$$
\phi: x \rightarrow K (\cdot, x)
$$

$$
f (\cdot)=\sum_{i=1}^{m} \alpha_{i} K\left (\cdot, x_{i}\right)
$$

$$
\begin{array}{l}
f (\cdot)=\sum_{i=1}^{m} \alpha_{i} K\left (\cdot, x_{i}\right) \\
g (\cdot)=\sum_{j=1}^{l} \beta_{j} K\left (\cdot, z_{j}\right)
\end{array}
$$

$$
f * g=\sum_{i=1}^{m} \sum_{j=1}^{l} \alpha_{i} \beta_{j} K\left (x_{i}, z_{j}\right)
$$
$$
\begin{array}{l}
(c f) * g=c (f * g), c \in \mathbf{R} \\
(f+g) * h=f * h+g * h, h \in \mathcal{S} \\
f * g=g * f \\
f * f \geqslant 0
\end{array}
$$

$$
f * f=0 \Leftrightarrow f=0
$$

$$
f * f=\sum_{i, j=1}^{m} \alpha_{i} \alpha_{j} K\left (x_{i}, x_{j}\right)
$$

$$
|f * g|^{2} \leqslant (f * f)(g * g)
$$

$$
\begin{array}{c}
(f+\lambda g) *(f+\lambda g) \geqslant 0 \\
f * f+2 \lambda (f * g)+\lambda^{2}(g * g) \geqslant 0
\end{array}
$$

$$
(f * g)^{2}-(f * f)(g * g) \leqslant 0
$$

$$
f (\cdot)=\sum_{i=1}^{m} \alpha_{i} K\left (\cdot, x_{i}\right)
$$

$$
K (\cdot, x) * f=\sum_{i=1}^{m} \alpha_{i} K\left (x, x_{i}\right)=f (x)
$$

$$
|f (x)|^{2}=|K (\cdot, x) * f|^{2}
$$

$$
\begin{aligned}
|K (\cdot, x) * f|^{2} & \leqslant (K (\cdot, x) * K (\cdot, x))(f * f) \\
& =K (x, x)(f * f)
\end{aligned}
$$

$$
|f (x)|^{2} \leqslant K (x, x)(f * f)
$$

$$
f (\cdot)=\sum_{i=1}^{m} \alpha_{i} K\left (\cdot, x_{i}\right), \quad g (\cdot)=\sum_{i=1}^{l} \beta_{j} K\left (\cdot, z_{j}\right)
$$

$$
f \cdot g=\sum_{i=1}^{m} \sum_{j=1}^{l} \alpha_{i} \beta_{j} K\left (x_{i}, z_{j}\right)
$$

$$
\|f\|=\sqrt{f \cdot f}
$$

$$
K (\cdot, x) \cdot f=f (x)
$$

$$
K (\cdot , x) \cdot K (\cdot , z)=K (x, z)
$$

$$
K=\left[K\left (x_{i}, x_{j}\right)\right]_{m \times m}
$$

$$
K (x, z)=\phi (x) \cdot \phi (z)
$$

$$
\left[K_{i j}\right]_{m \times m}=\left[K\left (x_{i}, x_{j}\right)\right]_{m \times m}
$$

$$
\begin{aligned}
\sum_{i, j=1}^{m} c_{i} c_{j} K\left (x_{i}, x_{j}\right) & =\sum_{i, j=1}^{m} c_{i} c_{j}\left (\phi\left (x_{i}\right) \cdot \phi\left (x_{j}\right)\right) \\
& =\left (\sum_{i} c_{i} \phi\left (x_{i}\right)\right) \cdot\left (\sum_{j} c_{j} \phi\left (x_{j}\right)\right) \\
& =\left\|\sum_{i} c_{i} \phi\left (x_{i}\right)\right\|^{2} \geqslant 0
\end{aligned}
$$

$$
\phi: x \rightarrow K (\cdot , x)
$$

$$
\begin{array}{c}
K (\cdot , x) \cdot f=f (x) \\
K (\cdot , x) \cdot K (\cdot , z)=K (x, z) \\
K (x, z)=\phi (x) \cdot \phi (z)
\end{array}
$$

$$
K=\left[K\left (x_{i}, x_{j}\right)\right]_{m \times m}
$$

$$
K (x, z)=(x \cdot z+1)^{p}
$$

$$
f (x)=\operatorname{sign}\left (\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i}\left (x_{i} \cdot x+1\right)^{p}+b^{*}\right)
$$

$$
K (x, z)=\exp \left (-\frac{\|x-z\|^{2}}{2 \sigma^{2}}\right)
$$

$$
f (x)=\operatorname{sign}\left (\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} \exp \left (-\frac{\left\|x-x_{i}\right\|^{2}}{2 \sigma^{2}}\right)+b^{*}\right)
$$

所有字符串的集合记作 $\Sigma^{*}=\bigcup_{n=0}^{\infty} \Sigma^{n}$。

$$
\left[\phi_{n}(s)\right]_{u}=\sum_{i: s (i)=u} \lambda^{l (i)}
$$

$$
\begin{aligned}
k_{n}(s, t) & =\sum_{u \in \Sigma^{n}}\left[\phi_{n}(s)\right]_{u}\left[\phi_{n}(t)\right]_{u} \\
& =\sum_{u \in \Sigma^{n}} \sum_{(i, j): s (i)=t (j)=u} \lambda^{l (i)} \lambda^{l (j)}
\end{aligned}
$$

$$
f (x)=\operatorname{sign}\left (\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K\left (x, x_{i}\right)+b^{*}\right)
$$

$$
\begin{array}{ll}
\min _{\alpha} & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left (x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } & \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
& 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{array}
$$

$$
b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K\left (x_{i}, x_{j}\right)
$$

$$
f (x)=\operatorname{sign}\left (\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K\left (x, x_{i}\right)+b^{*}\right)
$$

$$
\begin{array}{ll}
\min _{\alpha} & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left (x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } & \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
& 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{array}
$$

$$
\alpha_{1}=-y_{1} \sum_{i=2}^{N} \alpha_{i} y_{i}
$$

$$
\begin{array}{cl}
&\begin{array}{cl}
\min\limits _{\alpha_{1}, \alpha_{2}} \quad & W\left (\alpha_{1}, \alpha_{2}\right)= &\frac{1}{2} K_{11} \alpha_{1}^{2}+\frac{1}{2} K_{22} \alpha_{2}^{2}+y_{1} y_{2} K_{12} \alpha_{1} \alpha_{2}-\left (\alpha_{1}+\alpha_{2}\right)\\
&&+y_{1} \alpha_{1} \sum_{i=3}^{N} y_{i} \alpha_{i} K_{i 1}+y_{2} \alpha_{2} \sum_{i=3}^{N} y_{i} \alpha_{i} K_{i 2} 
\end{array}\\
&\begin{array}{cl}
\text { s.t. } \quad & \alpha_{1} y_{1}+\alpha_{2} y_{2}=-\sum_{i=3}^{N} y_{i} \alpha_{i}=\varsigma \\
& 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2
\end{array}
\end{array}
$$

$$
L \leqslant \alpha_{2}^{\text {new }} \leqslant H
$$

$$
L=\max \left (0, \alpha_{2}^{\text {old }}-\alpha_{1}^{\text {old }}\right), \quad H=\min \left (C, C+\alpha_{2}^{\text {old }}-\alpha_{1}^{\text {old }}\right)
$$

$$
g (x)=\sum_{i=1}^{N} \alpha_{i} y_{i} K\left (x_{i}, x\right)+b
$$

$$
E_{i}=g\left (x_{i}\right)-y_{i}=\left (\sum_{j=1}^{N} \alpha_{j} y_{j} K\left (x_{j}, x_{i}\right)+b\right)-y_{i}, \quad i=1,2
$$

$$
\alpha_{2}^{\text {new, unc }}=\alpha_{2}^{\text {old }}+\frac{y_{2}\left (E_{1}-E_{2}\right)}{\eta}
$$

$$
\eta=K_{11}+K_{22}-2 K_{12}=\left\|\Phi\left (x_{1}\right)-\Phi\left (x_{2}\right)\right\|^{2}
$$

$$
\alpha_{2}^{\text {new }}=\left\{\begin{array}{ll}
H, & \alpha_{2}^{\text {new }, \text { unc }}>H \\
\alpha_{2}^{\text {new, unc }}, & L \leqslant \alpha_{2}^{\text {new, unc }} \leqslant H \\
L, & \alpha_{2}^{\text {new, unc }}<L
\end{array}\right. \\
$$

$$
\alpha_{1}^{\text {new }}=\alpha_{1}^{\text {old }}+y_{1} y_{2}\left (\alpha_{2}^{\text {old }}-\alpha_{2}^{\text {new }}\right)
$$

$$
v_{i}=\sum_{j=3}^{N} \alpha_{j} y_{j} K\left (x_{i}, x_{j}\right)=g\left (x_{i}\right)-\sum_{j=1}^{2} \alpha_{j} y_{j} K\left (x_{i}, x_{j}\right)-b, \quad i=1,2
$$

$$
\begin{aligned}
W\left (\alpha_{1}, \alpha_{2}\right)= & \frac{1}{2} K_{11} \alpha_{1}^{2}+\frac{1}{2} K_{22} \alpha_{2}^{2}+y_{1} y_{2} K_{12} \alpha_{1} \alpha_{2}- \\
& \left (\alpha_{1}+\alpha_{2}\right)+y_{1} v_{1} \alpha_{1}+y_{2} v_{2} \alpha_{2}
\end{aligned}
$$

$$
\alpha_{1}=\left (\varsigma-y_{2} \alpha_{2}\right) y_{1}
$$

$$
\begin{aligned}
W\left (\alpha_{2}\right)= & \frac{1}{2} K_{11}\left (\varsigma-\alpha_{2} y_{2}\right)^{2}+\frac{1}{2} K_{22} \alpha_{2}^{2}+y_{2} K_{12}\left (\varsigma-\alpha_{2} y_{2}\right) \alpha_{2}- \\
& \left (\varsigma-\alpha_{2} y_{2}\right) y_{1}-\alpha_{2}+v_{1}\left (\varsigma-\alpha_{2} y_{2}\right)+y_{2} v_{2} \alpha_{2}
\end{aligned}
$$

对 $\alpha_{2}$ 求导数：

$$
\begin{aligned}
\frac{\partial W}{\partial \alpha_{2}}= & K_{11} \alpha_{2}+K_{22} \alpha_{2}-2 K_{12} \alpha_{2}- \\
& K_{11} \varsigma y_{2}+K_{12} \varsigma y_{2}+y_{1} y_{2}-1-v_{1} y_{2}+y_{2} v_{2}
\end{aligned}
$$

令其为 $0$，得到：

$$
\begin{aligned}
\left (K_{11}+K_{22}-2 K_{12}\right) \alpha_{2}= & y_{2}\left (y_{2}-y_{1}+\varsigma K_{11}-\varsigma K_{12}+v_{1}-v_{2}\right) \\
= & y_{2}\left[y_{2}-y_{1}+\varsigma K_{11}-\varsigma K_{12}+\left (g\left (x_{1}\right)-\sum_{j=1}^{2} y_{j} \alpha_{j} K_{1 j}-b\right)-\right. \\
& \left.\left (g\left (x_{2}\right)-\sum_{j=1}^{2} y_{j} \alpha_{j} K_{2 j}-b\right)\right]
\end{aligned}
$$

将 $\varsigma=\alpha_{1}^{\text {old }} y_{1}+\alpha_{2}^{\text {old }} y_{2}$ 代入，得到：

$$
\begin{aligned}
\left (K_{11}+K_{22}-2 K_{12}\right) \alpha_{2}^{\text {new }, \text { unc }} & =y_{2}\left (\left (K_{11}+K_{22}-2 K_{12}\right) \alpha_{2}^{\text {old }} y_{2}+y_{2}-y_{1}+g\left (x_{1}\right)-g\left (x_{2}\right)\right) \\
& =\left (K_{11}+K_{22}-2 K_{12}\right) \alpha_{2}^{\text {old }}+y_{2}\left (E_{1}-E_{2}\right)
\end{aligned}
$$

将 $\eta=K_{11}+K_{22}-2 K_{12}$ 代入，于是得到：

$$
\alpha_{2}^{\text {new, unc }}=\alpha_{2}^{\text {old }}+\frac{y_{2}\left (E_{1}-E_{2}\right)}{\eta}
$$

$$
\begin{aligned}
\alpha_{i} & =0 \Leftrightarrow y_{i} g\left (x_{i}\right) \geqslant 1 \\
0<\alpha_{i} & <C \Leftrightarrow y_{i} g\left (x_{i}\right)=1 \\
\alpha_{i} & =C \Leftrightarrow y_{i} g\left (x_{i}\right) \leqslant 1
\end{aligned}
$$

其中，$g\left (x_{i}\right)=\sum_{j=1}^{N} \alpha_{j} y_{j} K\left (x_{i}, x_{j}\right)+b$。

$$
\sum_{i=1}^{N} \alpha_{i} y_{i} K_{i 1}+b=y_{1}
$$

于是，

$$
b_{1}^{\text {new }}=y_{1}-\sum_{i=3}^{N} \alpha_{i} y_{i} K_{i 1}-\alpha_{1}^{\text {new }} y_{1} K_{11}-\alpha_{2}^{\text {new }} y_{2} K_{21}
$$

由 $E_{1}$ 的定义式有：

$$
E_{1}=\sum_{i=3}^{N} \alpha_{i} y_{i} K_{i 1}+\alpha_{1}^{\text {old }} y_{1} K_{11}+\alpha_{2}^{\text {old }} y_{2} K_{21}+b^{\text {old }}-y_{1}
$$

前两项可写成：

$$
y_{1}-\sum_{i=3}^{N} \alpha_{i} y_{i} K_{i 1}=-E_{1}+\alpha_{1}^{\text {old }} y_{1} K_{11}+\alpha_{2}^{\text {old }} y_{2} K_{21}+b^{\text {old }}
$$

代入可得：

$$
b_{1}^{\text {new }}=-E_{1}-y_{1} K_{11}\left (\alpha_{1}^{\text {new }}-\alpha_{1}^{\text {old }}\right)-y_{2} K_{21}\left (\alpha_{2}^{\text {new }}-\alpha_{2}^{\text {old }}\right)+b^{\text {old }}
$$

同样，如果 $0<\alpha_{2}^{\text {new }}<C$，那么，

$$
b_{2}^{\text {new }}=-E_{2}-y_{1} K_{12}\left (\alpha_{1}^{\text {new }}-\alpha_{1}^{\text {old }}\right)-y_{2} K_{22}\left (\alpha_{2}^{\text {new }}-\alpha_{2}^{\text {old }}\right)+b^{\text {old }}
$$

$\alpha_{1}^{\text {new }}, \alpha_{2}^{\text {new }}$ 是 $0$ 或者 $C$，那么 $b_{1}^{\text {new }}$ 和 $b_{2}^{\text {new }}$ 以及它们之间的数都是符合 KKT 条件的阈值, 这时选择它们的中点作为 $b^{\text {new }}$。

在每次完成两个变量的优化之后，还必须更新对应的 $E_{i}$ 值，并将它们保存在列表中。$E_{i}$ 值的更新要用到 $b^{\text {new }}$ 值，以及所有支持向量对应的 $\alpha_{j}$：

$$
E_{i}^{\text {new }}=\sum_{S} y_{j} \alpha_{j} K\left (x_{i}, x_{j}\right)+b^{\text {new }}-y_{i}
$$

$$
\begin{array}{l}
\sum_{i=1}^{N} \alpha_{i} y_{i}=0, \quad 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N \\
y_{i} \cdot g\left (x_{i}\right)\left\{\begin{array}{ll}
\geqslant 1, & \left\{x_{i} \mid \alpha_{i}=0\right\} \\
=1, & \left\{x_{i} \mid 0<\alpha_{i}<C\right\} \\
\leqslant 1, & \left\{x_{i} \mid \alpha_{i}=C\right\}
\end{array}\right.
\end{array}
$$

$$
g\left (x_{i}\right)=\sum_{j=1}^{N} \alpha_{j} y_{j} K\left (x_{j}, x_{i}\right)+b
$$

$$
\begin{array}{ll}
\min _{w, b} & \frac{1}{2}\|w\|^{2} \\
\text { s.t. } & y_{i}\left (w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
w^{*} \cdot x+b^{*}=0
$$

$$
f (x)=\operatorname{sign}\left (w^{*} \cdot x+b^{*}\right)
$$

$$
\begin{array}{ll}
\min _{\alpha} & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left (x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } & \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
& \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
\begin{array}{ll}
\min _{w, b, \xi} & \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i} \\
\text { s.t. } & y_{i}\left (w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N \\
& \xi_{i} \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
w^{*} \cdot x+b^{*}=0
$$

$$
f (x)=\operatorname{sign}\left (w^{*} \cdot x+b^{*}\right)
$$

$$
\begin{array}{ll}
\min\limits _{\alpha} & \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left (x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} \\
\text { s.t. } & \sum_{i=1}^{N} \alpha_{i} y_{i}=0 \\
& 0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N
\end{array}
$$

$$
\sum_{i=1}^{N}\left[1-y_{i}\left (w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2}
$$

# 提升方法

现在叙述 AdaBoost 算法。假设给定一个二类分类的训练数据集

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

其中，每个样本点由实例与标记组成。实例 $x_{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}$，标记  $y_{i} \in \mathcal{Y}=\{-1,+1\}$，$\mathcal{X}$  是实例空间 $\mathcal{Y}$ 是标记集合。AdaBoost 利用以下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合成为一个强分类器。

输入：训练数据集 $T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}$，其中 $x_{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}$，$y_{i} \in   \mathcal{Y}=\{-1,+1\}$；弱学习算法；

输出: 最终分类器 $G (x)$。

1. 初始化训练数据的权值分布

$$
D_{1}=\left (w_{11}, \cdots, w_{1 i}, \cdots, w_{1 N}\right), \quad w_{1 i}=\frac{1}{N}, \quad i=1,2, \cdots, N
$$

2. 对 $m=1,2, \cdots, M$ 
	1. 使用具有权值分布 $D_{m}$ 的训练数据集学习, 得到基本分类器 $G_{m}(x): \mathcal{X} \rightarrow\{-1,+1\}$
	2. 计算 $G_{m}(x)$  在训练数据集上的分类误差率

$$
e_{m}=\sum_{i=1}^{N} P\left (G_{m}\left (x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left (G_{m}\left (x_{i}\right) \neq y_{i}\right)
$$

$$
\alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}
$$

$$
\begin{array}{c}
D_{m+1}=\left (w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N}\right) \\
w_{m+1, i}=\frac{w_{m i}}{Z_{m}} \exp \left (-\alpha_{m} y_{i} G_{m}\left (x_{i}\right)\right), \quad i=1,2, \cdots, N
\end{array}
$$

$$
Z_{m}=\sum_{i=1}^{N} w_{m i} \exp \left (-\alpha_{m} y_{i} G_{m}\left (x_{i}\right)\right)
$$

$$
f (x)=\sum_{m=1}^{M} \alpha_{m} G_{m}(x)
$$

$$
\begin{aligned}
G (x) & =\operatorname{sign}(f (x)) \\
& =\operatorname{sign}\left (\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)
\end{aligned}
$$

$$\begin{aligned}
e_{m} & =\sum_{i=1}^{N} P\left (G_{m}\left (x_{i}\right) \neq y_{i}\right) \\
& =\sum_{G_{m}\left (x_{i}\right) \neq y_{i}} w_{m i}
\end{aligned}$$

$$w_{m+1, i}=\left\{\begin{array}{ll}
\frac{w_{m i}}{Z_{m}} \mathrm{e}^{-\alpha_{m}}, & G_{m}\left (x_{i}\right)=y_{i} \\
\frac{w_{m i}}{Z_{m}} \mathrm{e}^{\alpha_{m}}, & G_{m}\left (x_{i}\right) \neq y_{i}
\end{array}\right.$$

$$\frac{1}{N} \sum_{i=1}^{N} I\left (G\left (x_{i}\right) \neq y_{i}\right) \leqslant \frac{1}{N} \sum_{i} \exp \left (-y_{i} f\left (x_{i}\right)\right)=\prod_{m} Z_{m}$$

$$w_{m i} \exp \left (-\alpha_{m} y_{i} G_{m}\left (x_{i}\right)\right)=Z_{m} w_{m+1, i}
$$

$$\begin{aligned}
\frac{1}{N} \sum_{i} \exp \left (-y_{i} f\left (x_{i}\right)\right) & =\frac{1}{N} \sum_{i} \exp \left (-\sum_{m=1}^{M} \alpha_{m} y_{i} G_{m}\left (x_{i}\right)\right) \\
& =\sum_{i} w_{1 i} \prod_{m=1}^{M} \exp \left (-\alpha_{m} y_{i} G_{m}\left (x_{i}\right)\right) \\
& =Z_{1} \sum_{i} w_{2 i} \prod_{m=2}^{M} \exp \left (-\alpha_{m} y_{i} G_{m}\left (x_{i}\right)\right) \\
& =Z_{1} Z_{2} \sum_{i} w_{3 i} \prod_{m=3}^{M} \exp \left (-\alpha_{m} y_{i} G_{m}\left (x_{i}\right)\right) \\
& =\cdots \\
& =Z_{1} Z_{2} \cdots Z_{M-1} \sum_{i} w_{M i} \exp \left (-\alpha_{M} y_{i} G_{M}\left (x_{i}\right)\right) \\
& =\prod_{m=1}^{M} Z_{m}
\end{aligned}$$

$$\begin{aligned}
\prod_{m=1}^{M} Z_{m} & =\prod_{m=1}^{M}\left[2 \sqrt{e_{m}\left (1-e_{m}\right)}\right] \\
& =\prod_{m=1}^{M} \sqrt{\left (1-4 \gamma_{m}^{2}\right)} \\
& \leqslant \exp \left (-2 \sum_{m=1}^{M} \gamma_{m}^{2}\right)
\end{aligned}$$

$$\begin{aligned}
Z_{m} & =\sum_{i=1}^{N} w_{m i} \exp \left (-\alpha_{m} y_{i} G_{m}\left (x_{i}\right)\right) \\
& =\sum_{y_{i}=G_{m}\left (x_{i}\right)} w_{m i} \mathrm{e}^{-\alpha_{m}}+\sum_{y_{i} \neq G_{m}\left (x_{i}\right)} w_{m i} \mathrm{e}^{\alpha_{m}}\\
&=\left (1-e_{m}\right) \mathrm{e}^{-\alpha_{m}}+e_{m} \mathrm{e}^{\alpha_{m}} \\
&=2 \sqrt{e_{m}\left (1-e_{m}\right)} \\
&=\sqrt{1-4 \gamma_{m}^{2}}
\end{aligned}$$

$$\prod_{m=1}^{M} \sqrt{\left (1-4 \gamma_{m}^{2}\right)} \leqslant \exp \left (-2 \sum_{m=1}^{M} \gamma_{m}^{2}\right)$$

$$\frac{1}{N} \sum_{i=1}^{N} I\left (G\left (x_{i}\right) \neq y_{i}\right) \leqslant \exp \left (-2 M \gamma^{2}\right)$$

$$f (x)=\sum_{m=1}^{M} \beta_{m} b\left (x ; \gamma_{m}\right)$$

$$\min _{\beta_{m}, \gamma_{m}} \sum_{i=1}^{N} L\left (y_{i}, \sum_{m=1}^{M} \beta_{m} b\left (x_{i} ; \gamma_{m}\right)\right)$$

$$\min _{\beta, \gamma} \sum_{i=1}^{N} L\left (y_{i}, \beta b\left (x_{i} ; \gamma\right)\right)$$

$$\left (\beta_{m}, \gamma_{m}\right)=\arg \min _{\beta, \gamma} \sum_{i=1}^{N} L\left (y_{i}, f_{m-1}\left (x_{i}\right)+\beta b\left (x_{i} ; \gamma\right)\right)$$

$$f_{m}(x)=f_{m-1}(x)+\beta_{m} b\left (x ; \gamma_{m}\right)$$

$$f (x)=f_{M}(x)=\sum_{m=1}^{M} \beta_{m} b\left (x ; \gamma_{m}\right)$$

$$f (x)=\sum_{m=1}^{M} \alpha_{m} G_{m}(x)$$

$$L (y, f (x))=\exp [-y f (x)]$$

$$\begin{aligned}
f_{m-1}(x) & =f_{m-2}(x)+\alpha_{m-1} G_{m-1}(x) \\
& =\alpha_{1} G_{1}(x)+\cdots+\alpha_{m-1} G_{m-1}(x)
\end{aligned}$$

$$f_{m}(x)=f_{m-1}(x)+\alpha_{m} G_{m}(x)$$

$$\left (\alpha_{m}, G_{m}(x)\right)=\arg \min _{\alpha, G} \sum_{i=1}^{N} \exp \left[-y_{i}\left (f_{m-1}\left (x_{i}\right)+\alpha G\left (x_{i}\right)\right)\right]$$

$$\left (\alpha_{m}, G_{m}(x)\right)=\arg \min _{\alpha, G} \sum_{i=1}^{N} \bar{w}_{m i} \exp \left[-y_{i} \alpha G\left (x_{i}\right)\right]$$

$$G_{m}^{*}(x)=\arg \min _{G} \sum_{i=1}^{N} \bar{w}_{m i} I\left (y_{i} \neq G\left (x_{i}\right)\right)$$

$$\begin{aligned}
\sum_{i=1}^{N} \bar{w}_{m i} \exp \left[-y_{i} \alpha G\left (x_{i}\right)\right] & =\sum_{y_{i}=G_{m}\left (x_{i}\right)} \bar{w}_{m i} \mathrm{e}^{-\alpha}+\sum_{y_{i} \neq G_{m}\left (x_{i}\right)} \bar{w}_{m i} \mathrm{e}^{\alpha} \\
& =\left (\mathrm{e}^{\alpha}-\mathrm{e}^{-\alpha}\right) \sum_{i=1}^{N} \bar{w}_{m i} I\left (y_{i} \neq G\left (x_{i}\right)\right)+\mathrm{e}^{-\alpha} \sum_{i=1}^{N} \bar{w}_{m i}
\end{aligned}$$

$$\alpha_{m}^{*}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}$$

$$\begin{aligned}
e_{m} & =\frac{\sum_{i=1}^{N} \bar{w}_{m i} I\left (y_{i} \neq G_{m}\left (x_{i}\right)\right)}{\sum_{i=1}^{N} \bar{w}_{m i}} \\
& =\sum_{i=1}^{N} w_{m i} I\left (y_{i} \neq G_{m}\left (x_{i}\right)\right)
\end{aligned}$$

$$f_{m}(x)=f_{m-1}(x)+\alpha_{m} G_{m}(x)$$

$\bar{w}_{m i}=\exp \left[-y_{i} f_{m-1}\left (x_{i}\right)\right]$

$$\bar{w}_{m+1, i}=\bar{w}_{m, i} \exp \left[-y_{i} \alpha_{m} G_{m}(x)\right]$$

$$f_{M}(x)=\sum_{m=1}^{M} T\left (x ; \Theta_{m}\right)$$

$$f_{m}(x)=f_{m-1}(x)+T\left (x ; \Theta_{m}\right)$$

$$\hat{\Theta}_{m}=\arg \min _{\Theta_{m}} \sum_{i=1}^{N} L\left (y_{i}, f_{m-1}\left (x_{i}\right)+T\left (x_{i} ; \Theta_{m}\right)\right)$$

$$T (x ; \Theta)=\sum_{j=1}^{J} c_{j} I\left (x \in R_{j}\right)
$$

$$\begin{array}{l}
f_{0}(x) =0 \\
f_{m}(x) =f_{m-1}(x)+T\left (x ; \Theta_{m}\right), \quad m=1,2, \cdots, M \\
f_{M}(x) =\sum_{m=1}^{M} T\left (x ; \Theta_{m}\right)
\end{array}$$

$$\hat{\Theta}_{m}=\arg \min _{\Theta_{m}} \sum_{i=1}^{N} L\left (y_{i}, f_{m-1}\left (x_{i}\right)+T\left (x_{i} ; \Theta_{m}\right)\right)$$

$$L (y, f (x))=(y-f (x))^{2}$$

$$\begin{aligned}
L\left (y, f_{m-1}(x)+T\left (x ; \Theta_{m}\right)\right) & =\left[y-f_{m-1}(x)-T\left (x ; \Theta_{m}\right)\right]^{2} \\
& =\left[r-T\left (x ; \Theta_{m}\right)\right]^{2}
\end{aligned}$$

$$r=y-f_{m-1}(x)$$

输入：训练数据集 $T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}$，$x_{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}$，$y_{i} \in \mathcal{Y} \subseteq \mathbf{R}$；
输出：提升树 $f_{M}(x)$。

1. 初始化 $f_{0}(x)=0$。
2. 对 $m=1,2, \cdots, M$。
	1. 计算残差：$r_{m i}=y_{i}-f_{m-1}\left (x_{i}\right), \quad i=1,2, \cdots, N$。
	2. 拟合残差 $r_{m i}$ 学习一个回归树，得到 $T\left (x ; \Theta_{m}\right)$。
	3. 更新 $f_{m}(x)=f_{m-1}(x)+T\left (x ; \Theta_{m}\right)$。
3. 得到回归问题提升树。

$$
f_{M}(x)=\sum_{m=1}^{M} T\left (x ; \Theta_{m}\right)
$$

$$-\left[\frac{\partial L\left (y, f\left (x_{i}\right)\right)}{\partial f\left (x_{i}\right)}\right]_{f (x)=f_{m-1}(x)}
$$

输入：训练数据集 $T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}$，$x_{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}$，$y_{i} \in \mathcal{Y} \subseteq \mathbf{R}$；损失函数 $L (y, f (x))$；

输出：回归树 $\hat{f}(x)$。

1. 初始化

$$
f_{0}(x)=\arg \min _{c} \sum_{i=1}^{N} L\left (y_{i}, c\right)
$$

2. 对 $m=1,2, \cdots, M$
	1. 对  i=1,2, \cdots, N , 计算 $r_{m i}=-\left[\frac{\partial L\left (y_{i}, f\left (x_{i}\right)\right)}{\partial f\left (x_{i}\right)}\right]_{f (x)=f_{m-1}(x)}$
	2. 对  r_{m i}  拟合一个回归树, 得到第 $m$ 棵树的叶结点区域 $R_{m j}, j=1,2, \cdots, J$。
	3. 对 $j=1,2, \cdots, J$，计算 $c_{m j}=\arg \min _{c} \sum_{x_{i} \in R_{m j}} L\left (y_{i}, f_{m-1}\left (x_{i}\right)+c\right)$
	4. 更新 $f_{m}(x)=f_{m-1}(x)+\sum_{j=1}^{J} c_{m j} I\left (x \in R_{m j}\right)$
3. 得到回归树

$$
\hat{f}(x)=f_{M}(x)=\sum_{m=1}^{M} \sum_{j=1}^{J} c_{m j} I\left (x \in R_{m j}\right)
$$

$$
f (x)=\sum_{m=1}^{M} \alpha_{m} G_{m}(x)
$$

$$\left (\beta_{m}, \gamma_{m}\right)=\arg \min _{\beta, \gamma} \sum_{i=1}^{N} L\left (y_{i}, f_{m-1}\left (x_{i}\right)+\beta b\left (x_{i} ; \gamma\right)\right)$$

# EM 算法及其推广

## 三硬币模型

$$\begin{aligned}
P (y \mid \theta) & =\sum_{z} P (y, z \mid \theta)=\sum_{z} P (z \mid \theta) P (y \mid z, \theta) \\
& =\pi p^{y}(1-p)^{1-y}+(1-\pi) q^{y}(1-q)^{1-y}
\end{aligned}$$

$$\begin{array}{c}
P (Y \mid \theta)=\sum_{Z} P (Z \mid \theta) P (Y \mid Z, \theta) \\
P (Y \mid \theta)=\prod_{j=1}^{n}\left[\pi p^{y_{j}}(1-p)^{1-y_{j}}+(1-\pi) q^{y_{j}}(1-q)^{1-y_{j}}\right]
\end{array}$$

$$\hat{\theta}=\arg \max _{\theta} \log P (Y \mid \theta)$$

$$\mu_{j}^{(i+1)}=\frac{\pi^{(i)}\left (p^{(i)}\right)^{y_{j}}\left (1-p^{(i)}\right)^{1-y_{j}}}{\pi^{(i)}\left (p^{(i)}\right)^{y_{j}}\left (1-p^{(i)}\right)^{1-y_{j}}+\left (1-\pi^{(i)}\right)\left (q^{(i)}\right)^{y_{j}}\left (1-q^{(i)}\right)^{1-y_{j}}}
$$

$$\pi^{(i+1)}=\frac{1}{n} \sum_{j=1}^{n} \mu_{j}^{(i+1)}$$

$$p^{(i+1)}=\frac{\sum_{j=1}^{n} \mu_{j}^{(i+1)} y_{j}}{\sum_{j=1}^{n} \mu_{j}^{(i+1)}}$$

$$q^{(i+1)}=\frac{\sum_{j=1}^{n}\left (1-\mu_{j}^{(i+1)}\right) y_{j}}{\sum_{j=1}^{n}\left (1-\mu_{j}^{(i+1)}\right)}$$

$$\begin{aligned}
Q\left (\theta, \theta^{(i)}\right) & =E_{Z}\left[\log P (Y, Z \mid \theta) \mid Y, \theta^{(i)}\right] \\
& =\sum_{Z} \log P (Y, Z \mid \theta) P\left (Z \mid Y, \theta^{(i)}\right)
\end{aligned}$$

$$\theta^{(i+1)}=\arg \max _{\theta} Q\left (\theta, \theta^{(i)}\right)$$

$$Q\left (\theta, \theta^{(i)}\right)=E_{Z}\left[\log P (Y, Z \mid \theta) \mid Y, \theta^{(i)}\right]$$

$\left\|\theta^{(i+1)}-\theta^{(i)}\right\|<\varepsilon_{1}$ 或 $\left\|Q\left (\theta^{(i+1)}, \theta^{(i)}\right)-Q\left (\theta^{(i)}, \theta^{(i)}\right)\right\|<\varepsilon_{2}$

$$\begin{aligned}
L (\theta) & =\log P (Y \mid \theta)=\log \sum_{Z} P (Y, Z \mid \theta) \\
& =\log \left (\sum_{Z} P (Y \mid Z, \theta) P (Z \mid \theta)\right)
\end{aligned}$$

$$L (\theta)-L\left (\theta^{(i)}\right)=\log \left (\sum_{Z} P (Y \mid Z, \theta) P (Z \mid \theta)\right)-\log P\left (Y \mid \theta^{(i)}\right)$$

$$\begin{aligned}
L (\theta)-L\left (\theta^{(i)}\right) & =\log \left (\sum_{Z} P\left (Z \mid Y, \theta^{(i)}\right) \frac{P (Y \mid Z, \theta) P (Z \mid \theta)}{P\left (Z \mid Y, \theta^{(i)}\right)}\right)-\log P\left (Y \mid \theta^{(i)}\right) \\
& \geqslant \sum_{Z} P\left (Z \mid Y, \theta^{(i)}\right) \log \frac{P (Y \mid Z, \theta) P (Z \mid \theta)}{P\left (Z \mid Y, \theta^{(i)}\right)}-\log P\left (Y \mid \theta^{(i)}\right) \\
& =\sum_{Z} P\left (Z \mid Y, \theta^{(i)}\right) \log \frac{P (Y \mid Z, \theta) P (Z \mid \theta)}{P\left (Z \mid Y, \theta^{(i)}\right) P\left (Y \mid \theta^{(i)}\right)}
\end{aligned}$$

$$B\left (\theta, \theta^{(i)}\right) \hat{=} L\left (\theta^{(i)}\right)+\sum_{Z} P\left (Z \mid Y, \theta^{(i)}\right) \log \frac{P (Y \mid Z, \theta) P (Z \mid \theta)}{P\left (Z \mid Y, \theta^{(i)}\right) P\left (Y \mid \theta^{(i)}\right)}$$

$$L (\theta) \geqslant B\left (\theta, \theta^{(i)}\right)$$

$$L\left (\theta^{(i)}\right)=B\left (\theta^{(i)}, \theta^{(i)}\right)$$

$$\theta^{(i+1)}=\arg \max _{\theta} B\left (\theta, \theta^{(i)}\right)$$

$$\begin{aligned}
\theta^{(i+1)} & =\arg \max _{\theta}\left (L\left (\theta^{(i)}\right)+\sum_{Z} P\left (Z \mid Y, \theta^{(i)}\right) \log \frac{P (Y \mid Z, \theta) P (Z \mid \theta)}{P\left (Z \mid Y, \theta^{(i)}\right) P\left (Y \mid \theta^{(i)}\right)}\right) \\
& =\arg \max _{\theta}\left (\sum_{Z} P\left (Z \mid Y, \theta^{(i)}\right) \log (P (Y \mid Z, \theta) P (Z \mid \theta))\right) \\
& =\arg \max _{\theta}\left (\sum_{Z} P\left (Z \mid Y, \theta^{(i)}\right) \log P (Y, Z \mid \theta)\right) \\
& =\arg \max _{\theta} Q\left (\theta, \theta^{(i)}\right)
\end{aligned}$$

$$P\left (Y \mid \theta^{(i+1)}\right) \geqslant P\left (Y \mid \theta^{(i)}\right)$$

$$
P (Y \mid \theta)=\frac{P (Y, Z \mid \theta)}{P (Z \mid Y, \theta)}$$

$$
\log P (Y \mid \theta)=\log P (Y, Z \mid \theta)-\log P (Z \mid Y, \theta)$$

$$
Q\left (\theta, \theta^{(i)}\right)=\sum_{Z} \log P (Y, Z \mid \theta) P\left (Z \mid Y, \theta^{(i)}\right)
$$

$$H\left (\theta, \theta^{(i)}\right)=\sum_{Z} \log P (Z \mid Y, \theta) P\left (Z \mid Y, \theta^{(i)}\right)$$

$$\log P (Y \mid \theta)=Q\left (\theta, \theta^{(i)}\right)-H\left (\theta, \theta^{(i)}\right)$$

$$\begin{array}{l}
\log P\left (Y \mid \theta^{(i+1)}\right)-\log P\left (Y \mid \theta^{(i)}\right) \\
\quad=\left[Q\left (\theta^{(i+1)}, \theta^{(i)}\right)-Q\left (\theta^{(i)}, \theta^{(i)}\right)\right]-\left[H\left (\theta^{(i+1)}, \theta^{(i)}\right)-H\left (\theta^{(i)}, \theta^{(i)}\right)\right]
\end{array}$$

$$Q\left (\theta^{(i+1)}, \theta^{(i)}\right)-Q\left (\theta^{(i)}, \theta^{(i)}\right) \geqslant 0$$

$$\begin{aligned}
H\left (\theta^{(i+1)}, \theta^{(i)}\right)-H\left (\theta^{(i)}, \theta^{(i)}\right) & =\sum_{Z}\left (\log \frac{P\left (Z \mid Y, \theta^{(i+1)}\right)}{P\left (Z \mid Y, \theta^{(i)}\right)}\right) P\left (Z \mid Y, \theta^{(i)}\right) \\
& \leqslant \log \left (\sum_{Z} \frac{P\left (Z \mid Y, \theta^{(i+1)}\right)}{P\left (Z \mid Y, \theta^{(i)}\right)} P\left (Z \mid Y, \theta^{(i)}\right)\right) \\
& =\log \left (\sum_{Z} P\left (Z \mid Y, \theta^{(i+1)}\right)\right)=0
\end{aligned}$$

$$P (y \mid \theta)=\sum_{k=1}^{K} \alpha_{k} \phi\left (y \mid \theta_{k}\right)$$

$$\phi\left (y \mid \theta_{k}\right)=\frac{1}{\sqrt{2 \pi} \sigma_{k}} \exp \left (-\frac{\left (y-\mu_{k}\right)^{2}}{2 \sigma_{k}^{2}}\right)$$

$$P (y \mid \theta)=\sum_{k=1}^{K} \alpha_{k} \phi\left (y \mid \theta_{k}\right)$$

$$\begin{array}{l}
\gamma_{j k}=\left\{\begin{array}{ll}
1, & \text { 第 } j \text { 个观测来自第 } k \text { 个分模型 } \\
0, & \text { 否则 }
\end{array}\right. \\
j=1,2, \cdots, N ; \quad k=1,2, \cdots, K
\end{array}
$$

$$\left (y_{j}, \gamma_{j 1}, \gamma_{j 2}, \cdots, \gamma_{j K}\right), \quad j=1,2, \cdots, N$$

$$\begin{aligned}
P (y, \gamma \mid \theta) & =\prod_{j=1}^{N} P\left (y_{j}, \gamma_{j 1}, \gamma_{j 2}, \cdots, \gamma_{j K} \mid \theta\right) \\
& =\prod_{k=1}^{K} \prod_{j=1}^{N}\left[\alpha_{k} \phi\left (y_{j} \mid \theta_{k}\right)\right]^{\gamma_{j k}} \\
& =\prod_{k=1}^{K} \alpha_{k}^{n_{k}} \prod_{j=1}^{N}\left[\phi\left (y_{j} \mid \theta_{k}\right)\right]^{\gamma_{j k}} \\
& =\prod_{k=1}^{K} \alpha_{k}^{n_{k}} \prod_{j=1}^{N}\left[\frac{1}{\sqrt{2 \pi} \sigma_{k}} \exp \left (-\frac{\left (y_{j}-\mu_{k}\right)^{2}}{2 \sigma_{k}^{2}}\right)\right]^{\gamma_{j k}}
\end{aligned}$$

$$\log P (y, \gamma \mid \theta)=\sum_{k=1}^{K}\left\{n_{k} \log \alpha_{k}+\sum_{j=1}^{N} \gamma_{j k}\left[\log \left (\frac{1}{\sqrt{2 \pi}}\right)-\log \sigma_{k}-\frac{1}{2 \sigma_{k}^{2}}\left (y_{j}-\mu_{k}\right)^{2}\right]\right\}$$

$$\begin{aligned}
Q\left (\theta, \theta^{(i)}\right) & =E\left[\log P (y, \gamma \mid \theta) \mid y, \theta^{(i)}\right] \\
& =E\left\{\sum_{k=1}^{K}\left\{n_{k} \log \alpha_{k}+\sum_{j=1}^{N} \gamma_{j k}\left[\log \left (\frac{1}{\sqrt{2 \pi}}\right)-\log \sigma_{k}-\frac{1}{2 \sigma_{k}^{2}}\left (y_{j}-\mu_{k}\right)^{2}\right]\right\}\right\} \\
& =\sum_{k=1}^{K}\left\{\sum_{j=1}^{N}\left (E \gamma_{j k}\right) \log \alpha_{k}+\sum_{j=1}^{N}\left (E \gamma_{j k}\right)\left[\log \left (\frac{1}{\sqrt{2 \pi}}\right)-\log \sigma_{k}-\frac{1}{2 \sigma_{k}^{2}}\left (y_{j}-\mu_{k}\right)^{2}\right]\right\}
\end{aligned}$$

$$\begin{aligned}
\hat{\gamma}_{j k}= & E\left (\gamma_{j k} \mid y, \theta\right)=P\left (\gamma_{j k}=1 \mid y, \theta\right) \\
& =\frac{P\left (\gamma_{j k}=1, y_{j} \mid \theta\right)}{\sum_{k=1}^{K} P\left (\gamma_{j k}=1, y_{j} \mid \theta\right)} \\
& =\frac{P\left (y_{j} \mid \gamma_{j k}=1, \theta\right) P\left (\gamma_{j k}=1 \mid \theta\right)}{\sum_{k=1}^{K} P\left (y_{j} \mid \gamma_{j k}=1, \theta\right) P\left (\gamma_{j k}=1 \mid \theta\right)} \\
& =\frac{\alpha_{k} \phi\left (y_{j} \mid \theta_{k}\right)}{\sum_{k=1}^{K} \alpha_{k} \phi\left (y_{j} \mid \theta_{k}\right)}, \quad j=1,2, \cdots, N ; \quad k=1,2, \cdots, K
\end{aligned}$$

$$Q\left (\theta, \theta^{(i)}\right)=\sum_{k=1}^{K}\left\{n_{k} \log \alpha_{k}+\sum_{j=1}^{N} \hat{\gamma}_{j k}\left[\log \left (\frac{1}{\sqrt{2 \pi}}\right)-\log \sigma_{k}-\frac{1}{2 \sigma_{k}^{2}}\left (y_{j}-\mu_{k}\right)^{2}\right]\right\}$$

$$\theta^{(i+1)}=\arg \max _{\theta} Q\left (\theta, \theta^{(i)}\right)$$

$$\hat{\mu}_{k}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k} y_{j}}{\sum_{j=1}^{N} \hat{\gamma}_{j k}}, \quad k=1,2, \cdots, K$$

$$\hat{\sigma}_{k}^{2}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}\left (y_{j}-\mu_{k}\right)^{2}}{\sum_{j=1}^{N} \hat{\gamma}_{j k}}, \quad k=1,2, \cdots, K$$

$$\hat{\alpha}_{k}=\frac{n_{k}}{N}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}}{N}, \quad k=1,2, \cdots, K$$

$$\hat{\gamma}_{j k}=\frac{\alpha_{k} \phi\left (y_{j} \mid \theta_{k}\right)}{\sum_{k=1}^{K} \alpha_{k} \phi\left (y_{j} \mid \theta_{k}\right)}, \quad j=1,2, \cdots, N ; \quad k=1,2, \cdots, K$$

$$\hat{\mu}_{k}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k} y_{j}}{\sum_{j=1}^{N} \hat{\gamma}_{j k}}, \quad k=1,2, \cdots, K$$

$$\hat{\sigma}_{k}^{2}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}\left (y_{j}-\mu_{k}\right)^{2}}{\sum_{j=1}^{N} \hat{\gamma}_{j k}}, \quad k=1,2, \cdots, K 
$$

$$\hat{\alpha}_{k}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}}{N}, \quad k=1,2, \cdots, K
$$

$$F (\tilde{P}, \theta)=E_{\tilde{P}}[\log P (Y, Z \mid \theta)]+H (\tilde{P})$$

$$\tilde{P}_{\theta}(Z)=P (Z \mid Y, \theta)$$

$$L=E_{\tilde{P}} \log P (Y, Z \mid \theta)-E_{\tilde{P}} \log \tilde{P}(Z)+\lambda\left (1-\sum_{Z} \tilde{P}(Z)\right)$$

$$\frac{\partial L}{\partial \tilde{P}(Z)}=\log P (Y, Z \mid \theta)-\log \tilde{P}(Z)-1-\lambda$$

$$\lambda=\log P (Y, Z \mid \theta)-\log \tilde{P}_{\theta}(Z)-1$$

$$\frac{P (Y, Z \mid \theta)}{\tilde{P}_{\theta}(Z)}=\mathrm{e}^{1+\lambda}$$

$$F (\tilde{P}, \theta)=\log P (Y \mid \theta)$$

$$L\left (\theta^{*}\right)=F\left (\tilde{P}_{\theta^{*}}, \theta^{*}\right)=F\left (\tilde{P}^{*}, \theta^{*}\right)$$

$$\tilde{P}^{(i+1)}(Z)=\tilde{P}_{\theta^{(i)}}(Z)=P\left (Z \mid Y, \theta^{(i)}\right)$$

$$\begin{aligned}
F\left (\tilde{P}^{(i+1)}, \theta\right) & =E_{\tilde{P}^{(i+1)}}[\log P (Y, Z \mid \theta)]+H\left (\tilde{P}^{(i+1)}\right) \\
& =\sum_{Z} \log P (Y, Z \mid \theta) P\left (Z \mid Y, \theta^{(i)}\right)+H\left (\tilde{P}^{(i+1)}\right)
\end{aligned}$$

$$F\left (\tilde{P}^{(i+1)}, \theta\right)=Q\left (\theta, \theta^{(i)}\right)+H\left (\tilde{P}^{(i+1)}\right)$$

$$\theta^{(i+1)}=\arg \max _{\theta} F\left (\tilde{P}^{(i+1)}, \theta\right)=\arg \max _{\theta} Q\left (\theta, \theta^{(i)}\right)$$

$$\begin{aligned}
Q\left (\theta, \theta^{(i)}\right) & =E_{Z}\left[\log P (Y, Z \mid \theta) \mid Y, \theta^{(i)}\right] \\
& =\sum_{Z} P\left (Z \mid Y, \theta^{(i)}\right) \log P (Y, Z \mid \theta)
\end{aligned}$$

$$Q\left (\theta^{(i+1)}, \theta^{(i)}\right)>Q\left (\theta^{(i)}, \theta^{(i)}\right)$$

$$\begin{aligned}
Q\left (\theta, \theta^{(i)}\right) & =E_{Z}\left[\log P (Y, Z \mid \theta) \mid Y, \theta^{(i)}\right] \\
& =\sum_{Z} P\left (Z \mid y, \theta^{(i)}\right) \log P (Y, Z \mid \theta)
\end{aligned}$$

$$Q\left (\theta^{(i+1)}, \theta^{(i)}\right)>Q\left (\theta^{(i)}, \theta^{(i)}\right)$$

$$Q\left (\theta, \theta^{(i)}\right)=\sum_{Z} \log P (Y, Z \mid \theta) P\left (Z \mid Y, \theta^{(i)}\right)$$

$$\theta^{(i+1)}=\arg \max _{\theta} Q\left (\theta, \theta^{(i)}\right)$$

$$P\left (Y \mid \theta^{(i+1)}\right) \geqslant P\left (Y \mid \theta^{(i)}\right)$$

# 隐马尔可夫模型

$$Q=\left\{q_{1}, q_{2}, \cdots, q_{N}\right\}, \quad V=\left\{v_{1}, v_{2}, \cdots, v_{M}\right\}$$

$$I=\left (i_{1}, i_{2}, \cdots, i_{T}\right), \quad O=\left (o_{1}, o_{2}, \cdots, o_{T}\right)$$

$$A=\left[a_{i j}\right]_{N \times N}$$

$$a_{i j}=P\left (i_{t+1}=q_{j} \mid i_{t}=q_{i}\right), \quad i=1,2, \cdots, N ; \quad j=1,2, \cdots, N$$

$$B=\left[b_{j}(k)\right]_{N \times M}$$

$$b_{j}(k)=P\left (o_{t}=v_{k} \mid i_{t}=q_{j}\right), \quad k=1,2, \cdots, M ; \quad j=1,2, \cdots, N$$

$$\pi=\left (\pi_{i}\right)$$

$$\pi_{i}=P\left (i_{1}=q_{i}\right), \quad i=1,2, \cdots, N$$

$$\lambda=(A, B, \pi)$$

$$P\left (i_{t} \mid i_{t-1}, o_{t-1}, \cdots, i_{1}, o_{1}\right)=P\left (i_{t} \mid i_{t-1}\right), \quad t=1,2, \cdots, T$$

$$P\left (o_{t} \mid i_{T}, o_{T}, i_{T-1}, o_{T-1}, \cdots, i_{t+1}, o_{t+1}, i_{t}, i_{t-1}, o_{t-1}, \cdots, i_{1}, o_{1}\right)=P\left (o_{t} \mid i_{t}\right)$$

$$P (I \mid \lambda)=\pi_{i_{1}} a_{i_{1} i_{2}} a_{i_{2} i_{3}} \cdots a_{i_{T-1} i_{T}}$$

$$P (O \mid I, \lambda)=b_{i_{1}}\left (o_{1}\right) b_{i_{2}}\left (o_{2}\right) \cdots b_{i_{T}}\left (o_{T}\right)$$

$$\begin{aligned}
P (O, I \mid \lambda) & =P (O \mid I, \lambda) P (I \mid \lambda) \\
& =\pi_{i_{1}} b_{i_{1}}\left (o_{1}\right) a_{i_{1} i_{2}} b_{i_{2}}\left (o_{2}\right) \cdots a_{i_{T-1} i_{T}} b_{i_{T}}\left (o_{T}\right)
\end{aligned}$$

$$\begin{aligned}
P (O \mid \lambda) & =\sum_{I} P (O \mid I, \lambda) P (I \mid \lambda) \\
& =\sum_{i_{1}, i_{2}, \cdots, i_{T}} \pi_{i_{1}} b_{i_{1}}\left (o_{1}\right) a_{i_{1} i_{2}} b_{i_{2}}\left (o_{2}\right) \cdots a_{i_{T-1} i_{T}} b_{i_{T}}\left (o_{T}\right)
\end{aligned}$$

$$\alpha_{t}(i)=P\left (o_{1}, o_{2}, \cdots, o_{t}, i_{t}=q_{i} \mid \lambda\right)$$

$$\alpha_{1}(i)=\pi_{i} b_{i}\left (o_{1}\right), \quad i=1,2, \cdots, N$$

$$\alpha_{t+1}(i)=\left[\sum_{j=1}^{N} \alpha_{t}(j) a_{j i}\right] b_{i}\left (o_{t+1}\right), \quad i=1,2, \cdots, N$$

$$P (O \mid \lambda)=\sum_{i=1}^{N} \alpha_{T}(i)$$

$$\alpha_{T}(i)=P\left (o_{1}, o_{2}, \cdots, o_{T}, i_{T}=q_{i} \mid \lambda\right)$$

$$P (O \mid \lambda)=\sum_{i=1}^{N} \alpha_{T}(i)$$

$$\beta_{t}(i)=P\left (o_{t+1}, o_{t+2}, \cdots, o_{T} \mid i_{t}=q_{i}, \lambda\right)$$

$$\beta_{T}(i)=1, \quad i=1,2, \cdots, N$$

$$\beta_{t}(i)=\sum_{j=1}^{N} a_{i j} b_{j}\left (o_{t+1}\right) \beta_{t+1}(j), \quad i=1,2, \cdots, N$$

$$P (O \mid \lambda)=\sum_{i=1}^{N} \pi_{i} b_{i}\left (o_{1}\right) \beta_{1}(i)$$

$$P (O \mid \lambda)=\sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{t}(i) a_{i j} b_{j}\left (o_{t+1}\right) \beta_{t+1}(j), \quad t=1,2, \cdots, T-1$$

$$\gamma_{t}(i)=P\left (i_{t}=q_{i} \mid O, \lambda\right)$$

$$\gamma_{t}(i)=P\left (i_{t}=q_{i} \mid O, \lambda\right)=\frac{P\left (i_{t}=q_{i}, O \mid \lambda\right)}{P (O \mid \lambda)}$$

$$\alpha_{t}(i) \beta_{t}(i)=P\left (i_{t}=q_{i}, O \mid \lambda\right)$$

$$\gamma_{t}(i)=\frac{\alpha_{t}(i) \beta_{t}(i)}{P (O \mid \lambda)}=\frac{\alpha_{t}(i) \beta_{t}(i)}{\sum_{j=1}^{N} \alpha_{t}(j) \beta_{t}(j)}$$

$$\xi_{t}(i, j)=P\left (i_{t}=q_{i}, i_{t+1}=q_{j} \mid O, \lambda\right)$$

$$\xi_{t}(i, j)=\frac{P\left (i_{t}=q_{i}, i_{t+1}=q_{j}, O \mid \lambda\right)}{P (O \mid \lambda)}=\frac{P\left (i_{t}=q_{i}, i_{t+1}=q_{j}, O \mid \lambda\right)}{\sum_{i=1}^{N} \sum_{j=1}^{N} P\left (i_{t}=q_{i}, i_{t+1}=q_{j}, O \mid \lambda\right)}$$

$$P\left (i_{t}=q_{i}, i_{t+1}=q_{j}, O \mid \lambda\right)=\alpha_{t}(i) a_{i j} b_{j}\left (o_{t+1}\right) \beta_{t+1}(j)$$

$$\xi_{t}(i, j)=\frac{\alpha_{t}(i) a_{i j} b_{j}\left (o_{t+1}\right) \beta_{t+1}(j)}{\sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{t}(i) a_{i j} b_{j}\left (o_{t+1}\right) \beta_{t+1}(j)}$$

$$\sum_{t=1}^{T} \gamma_{t}(i)$$

$$\sum_{t=1}^{T-1} \gamma_{t}(i)$$

$$\sum_{t=1}^{T-1} \xi_{t}(i, j)$$

$$\hat{a}_{i j}=\frac{A_{i j}}{\sum_{j=1}^{N} A_{i j}}, \quad i=1,2, \cdots, N ; \quad j=1,2, \cdots, N$$

$$\hat{b}_{j}(k)=\frac{B_{j k}}{\sum_{k=1}^{M} B_{j k}}, \quad j=1,2, \cdots, N ; \quad k=1,2, \cdots, M$$

$$P (O \mid \lambda)=\sum_{I} P (O \mid I, \lambda) P (I \mid \lambda)$$

$$Q (\lambda, \bar{\lambda})=\sum_{I} \log P (O, I \mid \lambda) P (O, I \mid \bar{\lambda})$$

$$Q (\lambda, \bar{\lambda})=E_{I}[\log P (O, I \mid \lambda) \mid O, \bar{\lambda}]$$

$$P (O, I \mid \lambda)=\pi_{i_{1}} b_{i_{1}}\left (o_{1}\right) a_{i_{1} i_{2}} b_{i_{2}}\left (o_{2}\right) \cdots a_{i_{T-1} i_{T}} b_{i_{T}}\left (o_{T}\right)$$

$$\begin{aligned}
Q (\lambda, \bar{\lambda})= & \sum_{I} \log \pi_{i_{1}} P (O, I \mid \bar{\lambda})+\sum_{I}\left (\sum_{t=1}^{T-1} \log a_{i_{t} i_{t+1}}\right) P (O, I \mid \bar{\lambda})+ \\
& \sum_{I}\left (\sum_{t=1}^{T} \log b_{i_{t}}\left (o_{t}\right)\right) P (O, I \mid \bar{\lambda})
\end{aligned}$$

$$\sum_{I} \log \pi_{i_{1}} P (O, I \mid \bar{\lambda})=\sum_{i=1}^{N} \log \pi_{i} P\left (O, i_{1}=i \mid \bar{\lambda}\right)$$

$$\sum_{i=1}^{N} \log \pi_{i} P\left (O, i_{1}=i \mid \bar{\lambda}\right)+\gamma\left (\sum_{i=1}^{N} \pi_{i}-1\right)$$

$$\frac{\partial}{\partial \pi_{i}}\left[\sum_{i=1}^{N} \log \pi_{i} P\left (O, i_{1}=i \mid \bar{\lambda}\right)+\gamma\left (\sum_{i=1}^{N} \pi_{i}-1\right)\right]=0$$

$$P\left (O, i_{1}=i \mid \bar{\lambda}\right)+\gamma \pi_{i}=0$$

$$\gamma=-P (O \mid \bar{\lambda})$$

$$\pi_{i}=\frac{P\left (O, i_{1}=i \mid \bar{\lambda}\right)}{P (O \mid \bar{\lambda})}$$

$$\sum_{I}\left (\sum_{t=1}^{T-1} \log a_{i_{t} i_{t+1}}\right) P (O, I \mid \bar{\lambda})=\sum_{i=1}^{N} \sum_{j=1}^{N} \sum_{t=1}^{T-1} \log a_{i j} P\left (O, i_{t}=i, i_{t+1}=j \mid \bar{\lambda}\right)$$

$$a_{i j}=\frac{\sum_{t=1}^{T-1} P\left (O, i_{t}=i, i_{t+1}=j \mid \bar{\lambda}\right)}{\sum_{t=1}^{T-1} P\left (O, i_{t}=i \mid \bar{\lambda}\right)}$$

$$\sum_{I}\left (\sum_{t=1}^{T} \log b_{i_{t}}\left (o_{t}\right)\right) P (O, I \mid \bar{\lambda})=\sum_{j=1}^{N} \sum_{t=1}^{T} \log b_{j}\left (o_{t}\right) P\left (O, i_{t}=j \mid \bar{\lambda}\right)$$

$$b_{j}(k)=\frac{\sum_{t=1}^{T} P\left (O, i_{t}=j \mid \bar{\lambda}\right) I\left (o_{t}=v_{k}\right)}{\sum_{t=1}^{T} P\left (O, i_{t}=j \mid \bar{\lambda}\right)}$$

$$a_{i j}=\frac{\sum_{t=1}^{T-1} \xi_{t}(i, j)}{\sum_{t=1}^{T-1} \gamma_{t}(i)}$$

$$b_{j}(k) =\frac{\sum_{t=1, o_{t}=v_{k}}^{T} \gamma_{t}(j)}{\sum_{t=1}^{T} \gamma_{t}(j)}$$

$$\pi_{i} =\gamma_{1}(i)$$

$$a_{i j}^{(n+1)} =\frac{\sum_{t=1}^{T-1} \xi_{t}(i, j)}{\sum_{t=1}^{T-1} \gamma_{t}(i)}$$

$$b_{j}(k)^{(n+1)} =\frac{\sum_{t=1, o_{t}=v_{k}}^{T} \gamma_{t}(j)}{\sum_{t=1}^{T} \gamma_{t}(j)}$$

$$\pi_{i}^{(n+1)} =\gamma_{1}(i)$$

$$\gamma_{t}(i)=\frac{\alpha_{t}(i) \beta_{t}(i)}{P (O \mid \lambda)}=\frac{\alpha_{t}(i) \beta_{t}(i)}{\sum_{j=1}^{N} \alpha_{t}(j) \beta_{t}(j)}$$

$$i_{t}^{*}=\arg \max _{1 \leqslant i \leqslant N}\left[\gamma_{t}(i)\right], \quad t=1,2, \cdots, T$$

$$\delta_{t}(i)=\max _{i_{1}, i_{2}, \cdots, i_{t-1}} P\left (i_{t}=i, i_{t-1}, \cdots, i_{1}, o_{t}, \cdots, o_{1} \mid \lambda\right), \quad i=1,2, \cdots, N$$

$$\begin{aligned}
\delta_{t+1}(i) & =\max _{i_{1}, i_{2}, \cdots, i_{t}} P\left (i_{t+1}=i, i_{t}, \cdots, i_{1}, o_{t+1}, \cdots, o_{1} \mid \lambda\right) \\
& =\max _{1 \leqslant j \leqslant N}\left[\delta_{t}(j) a_{j i}\right] b_{i}\left (o_{t+1}\right), \quad i=1,2, \cdots, N ; t=1,2, \cdots, T-1
\end{aligned}$$

$$\Psi_{t}(i)=\arg \max _{1 \leqslant j \leqslant N}\left[\delta_{t-1}(j) a_{j i}\right], \quad i=1,2, \cdots, N$$

$$\delta_{1}(i)=\pi_{i} b_{i}\left (o_{1}\right), \quad i=1,2, \cdots, N$$

$$\Psi_{1}(i)=0, \quad i=1,2, \cdots, N$$

$$\delta_{t}(i)=\max _{1 \leqslant j \leqslant N}\left[\delta_{t-1}(j) a_{j i}\right] b_{i}\left (o_{t}\right), \quad i=1,2, \cdots, N$$

$$\Psi_{t}(i)=\arg \max _{1 \leqslant j \leqslant N}\left[\delta_{t-1}(j) a_{j i}\right], \quad i=1,2, \cdots, N$$

$$P^{*}=\max _{1 \leqslant i \leqslant N} \delta_{T}(i)$$

$$i_{T}^{*}=\arg \max _{1 \leqslant i \leqslant N}\left[\delta_{T}(i)\right]$$

$$i_{t}^{*}=\Psi_{t+1}\left (i_{t+1}^{*}\right)$$

# 条件随机场

$$P\left (Y_{u}, Y_{v} \mid Y_{O}\right)=P\left (Y_{u} \mid Y_{O}\right) P\left (Y_{v} \mid Y_{O}\right)$$

$$P\left (Y_{v}, Y_{O} \mid Y_{W}\right)=P\left (Y_{v} \mid Y_{W}\right) P\left (Y_{O} \mid Y_{W}\right)$$

$$P\left (Y_{v} \mid Y_{W}\right)=P\left (Y_{v} \mid Y_{W}, Y_{O}\right)$$

$$P\left (Y_{A}, Y_{B} \mid Y_{C}\right)=P\left (Y_{A} \mid Y_{C}\right) P\left (Y_{B} \mid Y_{C}\right)$$

$$P (Y)=\frac{1}{Z} \prod_{C} \Psi_{C}\left (Y_{C}\right)$$

$$Z=\sum_{Y} \prod_{C} \Psi_{C}\left (Y_{C}\right)$$

$$\Psi_{C}\left (Y_{C}\right)=\exp \left\{-E\left (Y_{C}\right)\right\}$$

$$P (Y) =\frac{1}{Z} \prod_{C} \Psi_{C}\left (Y_{C}\right)$$

$$Z =\sum_{Y} \prod_{C} \Psi_{C}\left (Y_{C}\right)$$

$$P\left (Y_{v} \mid X, Y_{w}, w \neq v\right)=P\left (Y_{v} \mid X, Y_{w}, w \sim v\right)$$

$$G=(V=\{1,2, \cdots, n\}, E=\{(i, i+1)\}), \quad i=1,2, \cdots, n-1$$

$$\begin{array}{c}
P\left (Y_{i} \mid X, Y_{1}, \cdots, Y_{i-1}, Y_{i+1}, \cdots, Y_{n}\right)=P\left (Y_{i} \mid X, Y_{i-1}, Y_{i+1}\right) \\
i=1,2, \cdots, n (\text { 在 } i=1 \text { 和 } n \text { 时只考虑单边 })
\end{array}$$

$$P (y \mid x) =\frac{1}{Z (x)} \exp \left (\sum_{i, k} \lambda_{k} t_{k}\left (y_{i-1}, y_{i}, x, i\right)+\sum_{i, l} \mu_{l} s_{l}\left (y_{i}, x, i\right)\right)$$

$$Z (x) =\sum_{y} \exp \left (\sum_{i, k} \lambda_{k} t_{k}\left (y_{i-1}, y_{i}, x, i\right)+\sum_{i, l} \mu_{l} s_{l}\left (y_{i}, x, i\right)\right)$$

$$f_{k}\left (y_{i-1}, y_{i}, x, i\right)=\left\{\begin{array}{ll}
t_{k}\left (y_{i-1}, y_{i}, x, i\right), & k=1,2, \cdots, K_{1} \\
s_{l}\left (y_{i}, x, i\right), & k=K_{1}+l ; l=1,2, \cdots, K_{2}
\end{array}\right.$$

$$f_{k}(y, x)=\sum_{i=1}^{n} f_{k}\left (y_{i-1}, y_{i}, x, i\right), \quad k=1,2, \cdots, K$$

$$w_{k}=\left\{\begin{array}{ll}
\lambda_{k}, & k=1,2, \cdots, K_{1} \\
\mu_{l}, & k=K_{1}+l ; l=1,2, \cdots, K_{2}
\end{array}\right.$$

$$P (y \mid x) =\frac{1}{Z (x)} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x)$$

$$Z (x) =\sum_{y} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x)$$

$$w=\left (w_{1}, w_{2}, \cdots, w_{K}\right)^{\mathrm{T}}$$

$$F (y, x)=\left (f_{1}(y, x), f_{2}(y, x), \cdots, f_{K}(y, x)\right)^{\mathrm{T}}$$

$$P_{w}(y \mid x)=\frac{\exp (w \cdot F (y, x))}{Z_{w}(x)}$$

$$Z_{w}(x)=\sum_{y} \exp (w \cdot F (y, x))$$

$$M_{i}(x)=\left[M_{i}\left (y_{i-1}, y_{i} \mid x\right)\right]$$

$$M_{i}\left (y_{i-1}, y_{i} \mid x\right)=\exp \left (W_{i}\left (y_{i-1}, y_{i} \mid x\right)\right)$$

$$W_{i}\left (y_{i-1}, y_{i} \mid x\right)=\sum_{k=1}^{K} w_{k} f_{k}\left (y_{i-1}, y_{i}, x, i\right)$$

$$P_{w}(y \mid x)=\frac{1}{Z_{w}(x)} \prod_{i=1}^{n+1} M_{i}\left (y_{i-1}, y_{i} \mid x\right)$$

$$Z_{w}(x)=\left[M_{1}(x) M_{2}(x) \cdots M_{n+1}(x)\right]_{\text {start, stop }}$$

$$\alpha_{0}(y \mid x)=\left\{\begin{array}{ll}
1, & y=\text { start } \\
0, & \text { 否则 }
\end{array}\right.$$

$$\alpha_{i}^{\mathrm{T}}\left (y_{i} \mid x\right)=\alpha_{i-1}^{\mathrm{T}}\left (y_{i-1} \mid x\right)\left[M_{i}\left (y_{i-1}, y_{i} \mid x\right)\right], \quad i=1,2, \cdots, n+1$$

$$\alpha_{i}^{\mathrm{T}}(x)=\alpha_{i-1}^{\mathrm{T}}(x) M_{i}(x)$$

$$\beta_{n+1}\left (y_{n+1} \mid x\right)  =\left\{\begin{array}{ll}
1, & y_{n+1}=\text { stop } \\
0, & \text { 否则 }
\end{array}\right.$$

$$\beta_{i}\left (y_{i} \mid x\right) =\left[M_{i+1}\left (y_{i}, y_{i+1} \mid x\right)\right] \beta_{i+1}\left (y_{i+1} \mid x\right)$$

$$\beta_{i}(x)=M_{i+1}(x) \beta_{i+1}(x)$$

$$P\left (Y_{i}=y_{i} \mid x\right)=\frac{\alpha_{i}^{\mathrm{T}}\left (y_{i} \mid x\right) \beta_{i}\left (y_{i} \mid x\right)}{Z (x)}$$

$$P\left (Y_{i-1}=y_{i-1}, Y_{i}=y_{i} \mid x\right)=\frac{\alpha_{i-1}^{\mathrm{T}}\left (y_{i-1} \mid x\right) M_{i}\left (y_{i-1}, y_{i} \mid x\right) \beta_{i}\left (y_{i} \mid x\right)}{Z (x)}$$

$$Z (x)=\alpha_{n}^{\mathrm{T}}(x) \mathbf{1}=\mathbf{1} \beta_{1}(x)$$

$$\begin{array}{l} 
E_{P (Y \mid X)}\left[f_{k}\right]&=\sum_{y} P (y \mid x) f_{k}(y, x) \\
&=\sum_{i=1}^{n+1} \sum_{y_{i-1} y_{i}} f_{k}\left (y_{i-1}, y_{i}, x, i\right) \frac{\alpha_{i-1}^{\mathrm{T}}\left (y_{i-1} \mid x\right) M_{i}\left (y_{i-1}, y_{i} \mid x\right) \beta_{i}\left (y_{i} \mid x\right)}{Z (x)} \\
&\qquad k=1,2, \cdots, K
\end{array}$$

$$Z (x)=\alpha_{n}^{\mathrm{T}}(x) \mathbf{1}$$

$$\begin{array}{l} 
E_{P (X, Y)}\left[f_{k}\right]&=\sum_{x, y} P (x, y) \sum_{i=1}^{n+1} f_{k}\left (y_{i-1}, y_{i}, x, i\right) \\
&=\sum_{x} \tilde{P}(x) \sum_{y} P (y \mid x) \sum_{i=1}^{n+1} f_{k}\left (y_{i-1}, y_{i}, x, i\right) \\
&=\sum_{x} \tilde{P}(x) \sum_{i=1}^{n+1} \sum_{y_{i-1} y_{i}} f_{k}\left (y_{i-1}, y_{i}, x, i\right) \frac{\alpha_{i-1}^{\mathrm{T}}\left (y_{i-1} \mid x\right) M_{i}\left (y_{i-1}, y_{i} \mid x\right) \beta_{i}\left (y_{i} \mid x\right)}{Z (x)} \\
&\qquad k=1,2, \cdots, K
\end{array}$$

$$Z (x)=\alpha_{n}^{\mathrm{T}}(x) \mathbf{1}$$

$$L (w)=L_{\tilde{P}}\left (P_{w}\right)=\log \prod_{x, y} P_{w}(y \mid x)^{\tilde{P}(x, y)}=\sum_{x, y} \tilde{P}(x, y) \log P_{w}(y \mid x)$$

$$\begin{aligned}
L (w) & =\sum_{x, y} \tilde{P}(x, y) \log P_{w}(y \mid x) \\
& =\sum_{x, y}\left[\tilde{P}(x, y) \sum_{k=1}^{K} w_{k} f_{k}(y, x)-\tilde{P}(x, y) \log Z_{w}(x)\right] \\
& =\sum_{j=1}^{N} \sum_{k=1}^{K} w_{k} f_{k}\left (y_{j}, x_{j}\right)-\sum_{j=1}^{N} \log Z_{w}\left (x_{j}\right)
\end{aligned}$$

$$\begin{array}{l} 
E_{\tilde{P}}\left[t_{k}\right]&= \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n+1} t_{k}\left (y_{i-1}, y_{i}, x, i\right) \\
&= \sum_{x, y} \tilde{P}(x) P (y \mid x) \sum_{i=1}^{n+1} t_{k}\left (y_{i-1}, y_{i}, x, i\right) \exp \left (\delta_{k} T (x, y)\right) \\
&\qquad k=1,2, \cdots, K_{1}
\end{array}$$

$$\begin{array}{l} 
E_{\tilde{P}}\left[s_{l}\right]&=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n+1} s_{l}\left (y_{i}, x, i\right) \\
&=\sum_{x, y} \tilde{P}(x) P (y \mid x) \sum_{i=1}^{n} s_{l}\left (y_{i}, x, i\right) \exp \left (\delta_{K_{1}+l} T (x, y)\right) \\
&\qquad l=1,2, \cdots, K_{2}
\end{array}$$

$$T (x, y)=\sum_{k} f_{k}(y, x)=\sum_{k=1}^{K} \sum_{i=1}^{n+1} f_{k}\left (y_{i-1}, y_{i}, x, i\right)$$

$$\sum_{x, y} \tilde{P}(x) P (y \mid x) \sum_{i=1}^{n+1} t_{k}\left (y_{i-1}, y_{i}, x, i\right) \exp \left (\delta_{k} T (x, y)\right)=E_{\tilde{P}}\left[t_{k}\right]$$

$$\sum_{x, y} \tilde{P}(x) P (y \mid x) \sum_{i=1}^{n} s_{l}\left (y_{i}, x, i\right) \exp \left (\delta_{K_{1}+l} T (x, y)\right)=E_{\tilde{P}}\left[s_{l}\right]$$

$$s (x, y)=S-\sum_{i=1}^{n+1} \sum_{k=1}^{K} f_{k}\left (y_{i-1}, y_{i}, x, i\right)$$

$$\sum_{x, y} \tilde{P}(x) P (y \mid x) \sum_{i=1}^{n+1} t_{k}\left (y_{i-1}, y_{i}, x, i\right) \exp \left (\delta_{k} S\right)=E_{\tilde{P}}\left[t_{k}\right]$$

$$
\delta_{k}=\frac{1}{S} \log \frac{E_{\tilde{P}}\left[t_{k}\right]}{E_{P}\left[t_{k}\right]}
$$

$$
E_{P}\left (t_{k}\right)=\sum_{x} \tilde{P}(x) \sum_{i=1}^{n+1} \sum_{y_{i-1}, y_{i}} t_{k}\left (y_{i-1}, y_{i}, x, i\right) \frac{\alpha_{i-1}^{\mathrm{T}}\left (y_{i-1} \mid x\right) M_{i}\left (y_{i-1}, y_{i} \mid x\right) \beta_{i}\left (y_{i} \mid x\right)}{Z (x)}$$

$$\sum_{x, y} \tilde{P}(x) P (y \mid x) \sum_{i=1}^{n} s_{l}\left (y_{i}, x, i\right) \exp \left (\delta_{K_{1}+l} S\right)=E_{\tilde{P}}\left[s_{l}\right]$$

$$\delta_{K_{1}+l}=\frac{1}{S} \log \frac{E_{\tilde{P}}\left[s_{l}\right]}{E_{P}\left[s_{l}\right]}$$

$$E_{P}\left (s_{l}\right)=\sum_{x} \tilde{P}(x) \sum_{i=1}^{n} \sum_{y_{i}} s_{l}\left (y_{i}, x, i\right) \frac{\alpha_{i}^{\mathrm{T}}\left (y_{i} \mid x\right) \beta_{i}\left (y_{i} \mid x\right)}{Z (x)}$$

$$T (x)=\max _{y} T (x, y)$$

$$\begin{aligned}
E_{\tilde{P}}\left[t_{k}\right] & =\sum_{x, y} \tilde{P}(x) P (y \mid x) \sum_{i=1}^{n+1} t_{k}\left (y_{i-1}, y_{i}, x, i\right) \exp \left (\delta_{k} T (x)\right) \\
& =\sum_{x} \tilde{P}(x) \sum_{y} P (y \mid x) \sum_{i=1}^{n+1} t_{k}\left (y_{i-1}, y_{i}, x, i\right) \exp \left (\delta_{k} T (x)\right) \\
& =\sum_{x} \tilde{P}(x) a_{k, t} \exp \left (\delta_{k} t\right) \\
& =\sum_{t=0}^{T_{\max }} a_{k, t} \beta_{k}^{t}
\end{aligned}$$

$$\begin{aligned}
E_{\tilde{P}}\left[s_{l}\right] & =\sum_{x, y} \tilde{P}(x) P (y \mid x) \sum_{i=1}^{n} s_{l}\left (y_{i}, x, i\right) \exp \left (\delta_{K_{1}+l} T (x)\right) \\
& =\sum_{x} \tilde{P}(x) \sum_{y} P (y \mid x) \sum_{i=1}^{n} s_{l}\left (y_{i}, x, i\right) \exp \left (\delta_{K_{1}+l} T (x)\right) \\
& =\sum_{x} \tilde{P}(x) b_{l, t} \exp \left (\delta_{k} t\right) \\
& =\sum_{t=0}^{T_{\max }} b_{l, t} \gamma_{l}^{t}
\end{aligned}$$

$$P_{w}(y \mid x)=\frac{\exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)}{\sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)}$$

$$\min _{w \in \mathbf{R}^{n}} f (w)=\sum_{x} \tilde{P}(x) \log \sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)-\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)$$

$$g (w)=\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) f (x, y)-E_{\tilde{P}}(f)$$

$$f\left (w^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geqslant 0} f\left (w^{(k)}+\lambda p_{k}\right)$$

$$B_{k+1}=B_{k}+\frac{y_{k} y_{k}^{\mathrm{T}}}{y_{k}^{\mathrm{T}} \delta_{k}}-\frac{B_{k} \delta_{k} \delta_{k}^{\mathrm{T}} B_{k}}{\delta_{k}^{\mathrm{T}} B_{k} \delta_{k}}$$

$$y_{k}=g_{k+1}-g_{k}, \quad \delta_{k}=w^{(k+1)}-w^{(k)}$$

$$\begin{aligned}
y^{*} & =\arg \max _{y} P_{w}(y \mid x) \\
& =\arg \max _{y} \frac{\exp (w \cdot F (y, x))}{Z_{w}(x)} \\
& =\arg \max _{y} \exp (w \cdot F (y, x)) \\
& =\arg \max _{y}(w \cdot F (y, x))
\end{aligned}$$

$$\max _{y}(w \cdot F (y, x))$$

$$\begin{aligned}
w & =\left (w_{1}, w_{2}, \cdots, w_{K}\right)^{\mathrm{T}} \\
F (y, x) & =\left (f_{1}(y, x), f_{2}(y, x), \cdots, f_{K}(y, x)\right)^{\mathrm{T}} \\
f_{k}(y, x) & =\sum_{i=1}^{n} f_{k}\left (y_{i-1}, y_{i}, x, i\right), \quad k=1,2, \cdots, K
\end{aligned}$$

$$\max _{y} \sum_{i=1}^{n} w \cdot F_{i}\left (y_{i-1}, y_{i}, x\right)$$

$$F_{i}\left (y_{i-1}, y_{i}, x\right)=\left (f_{1}\left (y_{i-1}, y_{i}, x, i\right), f_{2}\left (y_{i-1}, y_{i}, x, i\right), \cdots, f_{K}\left (y_{i-1}, y_{i}, x, i\right.\right.$$

$$\delta_{1}(j)=w \cdot F_{1}\left (y_{0}=\text { start }, y_{1}=j, x\right), \quad j=1,2, \cdots, m$$

$$\delta_{i}(l)=\max _{1 \leqslant j \leqslant m}\left\{\delta_{i-1}(j)+w \cdot F_{i}\left (y_{i-1}=j, y_{i}=l, x\right)\right\}, \quad l=1,2, \cdots, m$$

$$\Psi_{i}(l)=\arg \max _{1 \leqslant j \leqslant m}\left\{\delta_{i-1}(j)+w \cdot F_{i}\left (y_{i-1}=j, y_{i}=l, x\right)\right\}, \quad l=1,2, \cdots, m$$

$$\max _{y}(w \cdot F (y, x))=\max _{1 \leqslant j \leqslant m} \delta_{n}(j)$$

$$y_{n}^{*}=\arg \max _{1 \leqslant j \leqslant m} \delta_{n}(j)$$

$$y_{i}^{*}=\Psi_{i+1}\left (y_{i+1}^{*}\right), \quad i=n-1, n-2, \cdots, 1$$

求得最优路径 $y^{*}=\left (y_{1}^{*}, y_{2}^{*}, \cdots, y_{n}^{*}\right)^{\mathrm{T}}$。

$$\delta_{1}(j)=w \cdot F_{1}\left (y_{0}=\text { start }, y_{1}=j, x\right), \quad j=1,2, \cdots, m$$

$$\delta_{i}(l)=\max _{1 \leqslant j \leqslant m}\left\{\delta_{i-1}(j)+w \cdot F_{i}\left (y_{i-1}=j, y_{i}=l, x\right)\right\}, \quad l=1,2, \cdots, m$$

$$\Psi_{i}(l)=\arg \max _{1 \leqslant j \leqslant m}\left\{\delta_{i-1}(j)+w \cdot F_{i}\left (y_{i-1}=j, y_{i}=l, x\right)\right\}, \quad l=1,2, \cdots, m$$

$$\max _{y}(w \cdot F (y, x))=\max _{1 \leqslant j \leqslant m} \delta_{n}(j)$$

$$y_{n}^{*}=\arg \max _{1 \leqslant j \leqslant m} \delta_{n}(j)$$

$$y_{i}^{*}=\Psi_{i+1}\left (y_{i+1}^{*}\right), \quad i=n-1, n-2, \cdots, 1$$

$$P (y \mid x) =\frac{1}{Z (x)} \exp \left (\sum_{i, k} \lambda_{k} t_{k}\left (y_{i-1}, y_{i}, x, i\right)+\sum_{i, l} \mu_{l} s_{l}\left (y_{i}, x, i\right)\right)$$

$$Z (x) =\sum_{y} \exp \left (\sum_{i, k} \lambda_{k} t_{k}\left (y_{i-1}, y_{i}, x, i\right)+\sum_{i, l} \mu_{l} s_{l}\left (y_{i}, x, i\right)\right)$$

# 监督学习方法总结

$$\begin{array}{c}
{[1-y f (x)]_{+}} \\
\log [1+\exp (-y f (x))] \\
\exp (-y f (x))
\end{array}$$

$$\min _{f \in H} \frac{1}{N} \sum_{i=1}^{N} L\left (y_{i}, f\left (x_{i}\right)\right)+\lambda J (f)$$

$$-\log P (y \mid x)$$

# 无监督学习概论

$$X=\left[\begin{array}{ccc}
x_{11} & \cdots & x_{1 N} \\
\vdots & & \vdots \\
x_{M 1} & \cdots & x_{M N}
\end{array}\right]$$

$$P (z \mid x)=\frac{P (z) P (x \mid z)}{P (x)} \propto P (z) P (x \mid z)$$

# 聚类方法

$$X=\left[x_{i j}\right]_{m \times n}=\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1 n} \\
x_{21} & x_{22} & \cdots & x_{2 n} \\
\vdots & \vdots & & \vdots \\
x_{m 1} & x_{m 2} & \cdots & x_{m n}
\end{array}\right]$$

$$d_{i j}=\left (\sum_{k=1}^{m}\left|x_{k i}-x_{k j}\right|^{p}\right)^{\frac{1}{p}}$$

$$d_{i j}=\left (\sum_{k=1}^{m}\left|x_{k i}-x_{k j}\right|^{2}\right)^{\frac{1}{2}}$$

$$d_{i j}=\sum_{k=1}^{m}\left|x_{k i}-x_{k j}\right|$$

$$d_{i j}=\max _{k}\left|x_{k i}-x_{k j}\right|$$

$$d_{i j}=\left[\left (x_{i}-x_{j}\right)^{\mathrm{T}} S^{-1}\left (x_{i}-x_{j}\right)\right]^{\frac{1}{2}}$$

$$x_{i}=\left (x_{1 i}, x_{2 i}, \cdots, x_{m i}\right)^{\mathrm{T}}, \quad x_{j}=\left (x_{1 j}, x_{2 j}, \cdots, x_{m j}\right)^{\mathrm{T}}$$

$$r_{i j}=\frac{\sum_{k=1}^{m}\left (x_{k i}-\bar{x}_{i}\right)\left (x_{k j}-\bar{x}_{j}\right)}{\left[\sum_{k=1}^{m}\left (x_{k i}-\bar{x}_{i}\right)^{2} \sum_{k=1}^{m}\left (x_{k j}-\bar{x}_{j}\right)^{2}\right]^{\frac{1}{2}}}$$

$$\bar{x}_{i}=\frac{1}{m} \sum_{k=1}^{m} x_{k i}, \quad \bar{x}_{j}=\frac{1}{m} \sum_{k=1}^{m} x_{k j}$$

$$s_{i j}=\frac{\sum_{k=1}^{m} x_{k i} x_{k j}}{\left[\sum_{k=1}^{m} x_{k i}^{2} \sum_{k=1}^{m} x_{k j}^{2}\right]^{\frac{1}{2}}}$$

$$d_{i j} \leqslant T$$

$$d_{i j} \leqslant T$$

$$\frac{1}{n_{G}-1} \sum_{x_{j} \in G} d_{i j} \leqslant T$$

$$\begin{array}{l}
\frac{1}{n_{G}\left (n_{G}-1\right)} \sum_{x_{i} \in G} \sum_{x_{j} \in G} d_{i j} \leqslant T \\
d_{i j} \leqslant V
\end{array}$$

$$\bar{x}_{G}=\frac{1}{n_{G}} \sum_{i=1}^{n_{G}} x_{i}$$

$$D_{G}=\max _{x_{i}, x_{j} \in G} d_{i j}$$

$$A_{G}=\sum_{i=1}^{n_{G}}\left (x_{i}-\bar{x}_{G}\right)\left (x_{i}-\bar{x}_{G}\right)^{\mathrm{T}}$$

$$\begin{aligned}
S_{G} & =\frac{1}{m-1} A_{G} \\
& =\frac{1}{m-1} \sum_{i=1}^{n_{G}}\left (x_{i}-\bar{x}_{G}\right)\left (x_{i}-\bar{x}_{G}\right)^{\mathrm{T}}
\end{aligned}$$

$$D_{p q}=\min \left\{d_{i j} \mid x_{i} \in G_{p}, x_{j} \in G_{q}\right\}$$

$$D_{p q}=\max \left\{d_{i j} \mid x_{i} \in G_{p}, x_{j} \in G_{q}\right\}$$

$$D_{p q}=d_{\bar{x}_{p} \bar{x}_{q}}$$

$$D_{p q}=\frac{1}{n_{p} n_{q}} \sum_{x_{i} \in G_{p}} \sum_{x_{j} \in G_{q}} d_{i j}$$

$$\begin{aligned}
d\left (x_{i}, x_{j}\right) & =\sum_{k=1}^{m}\left (x_{k i}-x_{k j}\right)^{2} \\
& =\left\|x_{i}-x_{j}\right\|^{2}
\end{aligned}$$

$$W (C)=\sum_{l=1}^{k} \sum_{C (i)=l}\left\|x_{i}-\bar{x}_{l}\right\|^{2}$$

$$\begin{aligned}
C^{*} & =\arg \min _{C} W (C) \\
& =\arg \min _{C} \sum_{l=1}^{k} \sum_{C (i)=l}\left\|x_{i}-\bar{x}_{l}\right\|^{2}
\end{aligned}$$

$$S (n, k)=\frac{1}{k !} \sum_{l=1}^{k}(-1)^{k-l}\left (\begin{array}{c}
k \\
l
\end{array}\right) k^{n}$$

$$\min _{C} \sum_{l=1}^{k} \sum_{C (i)=l}\left\|x_{i}-m_{l}\right\|^{2}$$

$$\min _{m_{1}, \cdots, m_{k}} \sum_{l=1}^{k} \sum_{C (i)=l}\left\|x_{i}-m_{l}\right\|^{2}$$

$$m_{l}=\frac{1}{n_{l}} \sum_{C (i)=l} x_{i}, \quad l=1, \cdots, k$$

$$d_{i j} \leqslant T$$

$$A=U \Sigma V^{\mathrm{T}}$$

$$\begin{array}{l}
U U^{\mathrm{T}}=I \\
V V^{\mathrm{T}}=I \\
\Sigma=\operatorname{diag}\left (\sigma_{1}, \sigma_{2}, \cdots, \sigma_{p}\right) \\
\sigma_{1} \geqslant \sigma_{2} \geqslant \cdots \geqslant \sigma_{p} \geqslant 0 \\
p=\min (m, n)
\end{array}$$

$$A=U \Sigma V^{\mathrm{T}}$$

$$\|A x\|^{2}=x^{\mathrm{T}} A^{\mathrm{T}} A x=\lambda x^{\mathrm{T}} x=\lambda\|x\|^{2}$$

$$\lambda=\frac{\|A x\|^{2}}{\|x\|^{2}} \geqslant 0$$

$$\lambda_{1} \geqslant \lambda_{2} \geqslant \cdots \geqslant \lambda_{n} \geqslant 0$$

$$\sigma_{j}=\sqrt{\lambda_{j}}, \quad j=1,2, \cdots, n$$

$$\lambda_{1} \geqslant \lambda_{2} \geqslant \cdots \geqslant \lambda_{r}>0, \quad \lambda_{r+1}=\lambda_{r+2}=\cdots=\lambda_{n}=0$$

$$\sigma_{1} \geqslant \sigma_{2} \geqslant \cdots \geqslant \sigma_{r}>0, \quad \sigma_{r+1}=\sigma_{r+2}=\cdots=\sigma_{n}=0$$

$$V_{1}=\left[\begin{array}{llllll}
\nu_{1} & \nu_{2} & \cdots & \nu_{r}
\end{array}\right], \quad V_{2}=\left[\begin{array}{llll}
\nu_{r+1} & \nu_{r+2} & \cdots & \nu_{n}
\end{array}\right]$$

$$V=\left[\begin{array}{ll}
V_{1} & V_{2}
\end{array}\right]$$

$$\Sigma_{1}=\left[\begin{array}{llll}
\sigma_{1} & & & \\
& \sigma_{2} & & \\
& & \ddots & \\
& & & \sigma_{r}
\end{array}\right]$$

$$\Sigma=\left[\begin{array}{cc}
\Sigma_{1} & 0 \\
0 & 0
\end{array}\right]$$

$$A^{\mathrm{T}} A v_{j}=0, \quad j=r+1, \cdots, n$$

$$I=V V^{\mathrm{T}}=V_{1} V_{1}^{\mathrm{T}}+V_{2} V_{2}^{\mathrm{T}}$$

$$
A=A I=A V_{1} V_{1}^{\mathrm{T}}+A V_{2} V_{2}^{\mathrm{T}}=A V_{1} V_{1}^{\mathrm{T}}
$$

$$u_{j}=\frac{1}{\sigma_{j}} A v_{j}, \quad j=1,2, \cdots, r$$

$$U_{1}=\left[\begin{array}{llll}
u_{1} & u_{2} & \cdots & u_{r}
\end{array}\right]$$

$$A V_{1}=U_{1} \Sigma_{1}$$

$$\begin{aligned}
u_{i}^{\mathrm{T}} u_{j} & =\left (\frac{1}{\sigma_{i}} v_{i}^{\mathrm{T}} A^{\mathrm{T}}\right)\left (\frac{1}{\sigma_{j}} A v_{j}\right) \\
& =\frac{1}{\sigma_{i} \sigma_{j}} v_{i}^{\mathrm{T}}\left (A^{\mathrm{T}} A v_{j}\right) \\
& =\frac{\sigma_{j}}{\sigma_{i}} v_{i}^{\mathrm{T}} v_{j} \\
& =\delta_{i j}, \quad i=1,2, \cdots, r ; \quad j=1,2, \cdots, r
\end{aligned}$$

$$U_{2}=\left[\begin{array}{llll}
u_{r+1} & u_{r+2} & \cdots & u_{m}
\end{array}\right]$$

$$U=\left[\begin{array}{ll}
U_{1} & U_{2}
\end{array}\right]$$

$$\begin{aligned}
U \Sigma V^{\mathrm{T}} & =\left[\begin{array}{ll}
U_{1} & U_{2}
\end{array}\right]\left[\begin{array}{cc}
\Sigma_{1} & 0 \\
0 & 0
\end{array}\right]\left[\begin{array}{l}
V_{1}^{\mathrm{T}} \\
V_{2}^{\mathrm{T}}
\end{array}\right] \\
& =U_{1} \Sigma_{1} V_{1}^{\mathrm{T}} \\
& =A V_{1} V_{1}^{\mathrm{T}} \\
& =A
\end{aligned}$$

$$A=U \Sigma V^{\mathrm{T}}$$

$$A=U_{r} \Sigma_{r} V_{r}^{\mathrm{T}}$$

$$A \approx U_{k} \Sigma_{k} V_{k}^{\mathrm{T}}$$

$$T: x \rightarrow A x$$

$$A^{\mathrm{T}} A=\left (U \Sigma V^{\mathrm{T}}\right)^{\mathrm{T}}\left (U \Sigma V^{\mathrm{T}}\right)=V\left (\Sigma^{\mathrm{T}} \Sigma\right) V^{\mathrm{T}}$$

$$A A^{\mathrm{T}}=\left (U \Sigma V^{\mathrm{T}}\right)\left (U \Sigma V^{\mathrm{T}}\right)^{\mathrm{T}}=U\left (\Sigma \Sigma^{\mathrm{T}}\right) U^{\mathrm{T}}$$

$$A V=U \Sigma$$

$$A v_{j}=\sigma_{j} u_{j}, \quad j=1,2, \cdots, n$$

$$A^{\mathrm{T}} U=V \Sigma^{\mathrm{T}}$$

$$\begin{array}{c}
A^{\mathrm{T}} u_{j}=\sigma_{j} v_{j}, \quad j=1,2, \cdots, n \\
A^{\mathrm{T}} u_{j}=0, \quad j=n+1, n+2, \cdots, m
\end{array}$$

$$\sigma_{j}=\sqrt{\lambda_{j}}, \quad j=1,2, \cdots, n$$

$$(W-\lambda I) x=0$$

$$\lambda_{1} \geqslant \lambda_{2} \geqslant \cdots \geqslant \lambda_{n} \geqslant 0$$

$$V=\left[\begin{array}{llll}
v_{1} & v_{2} & \cdots & v_{n}
\end{array}\right]$$

$$\sigma_{i}=\sqrt{\lambda_{i}}, \quad i=1,2, \cdots, n$$

$$\Sigma=\operatorname{diag}\left (\sigma_{1}, \sigma_{2}, \cdots, \sigma_{n}\right)$$

$$u_{j}=\frac{1}{\sigma_{j}} A v_{j}, \quad j=1,2, \cdots, r$$

$$U_{1}=\left[\begin{array}{llll}
u_{1} & u_{2} & \cdots & u_{r}
\end{array}\right]$$

$$U_{2}=\left[\begin{array}{llll}
u_{r+1} & u_{r+2} & \cdots & u_{m}
\end{array}\right]$$

$$U=\left[\begin{array}{ll}
U_{1} & U_{2}
\end{array}\right]$$

$$A=U \Sigma V^{\mathrm{T}}$$

$$\|A\|_{F}=\left (\sum_{i=1}^{m} \sum_{j=1}^{n}\left (a_{i j}\right)^{2}\right)^{\frac{1}{2}}$$

$$\|A\|_{F}=\left (\sigma_{1}^{2}+\sigma_{2}^{2}+\cdots+\sigma_{n}^{2}\right)^{\frac{1}{2}}$$

$$\|Q A\|_{F}=\|A\|_{F}$$

$$\begin{aligned}
\|Q A\|_{F}^{2} & =\left\|\left (Q a_{1}, Q a_{2}, \cdots, Q a_{n}\right)\right\|_{F}^{2} \\
& =\sum_{i=1}^{n}\left\|Q a_{i}\right\|_{2}^{2}=\sum_{i=1}^{n}\left\|a_{i}\right\|_{2}^{2}=\|A\|_{F}^{2}
\end{aligned}$$

$$\left\|A P^{\mathrm{T}}\right\|_{F}=\|A\|_{F}$$

$$\|A\|_{F}=\left\|U \Sigma V^{\mathrm{T}}\right\|_{F}=\|\Sigma\|_{F}$$

$$\|A\|_{F}=\left (\sigma_{1}^{2}+\sigma_{2}^{2}+\cdots+\sigma_{n}^{2}\right)^{\frac{1}{2}}$$

$$\|A-X\|_{F}=\min _{S \in \mathcal{M}}\|A-S\|_{F}$$

$$\|A-X\|_{F}=\min _{S \in \mathcal{M}}\|A-S\|_{F}$$

$$\|A-X\|_{F}=\left (\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\cdots+\sigma_{n}^{2}\right)^{\frac{1}{2}}$$

$$\Sigma^{\prime}=\left[\begin{array}{cccccc}
\sigma_{1} & & & & & \\
& \ddots & & & 0 & \\
& & \sigma_{k} & & & \\
& & & 0 & & \\
& 0 & & & \ddots & \\
& & & & 0
\end{array}\right]=\left[\begin{array}{cc}
\Sigma_{k} & 0 \\
0 & 0
\end{array}\right]$$

$$\left\|A-A^{\prime}\right\|_{F}=\left (\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\cdots+\sigma_{n}^{2}\right)^{\frac{1}{2}}=\min _{S \in \mathcal{M}}\|A-S\|_{F}$$

$$\|A-X\|_{F} \leqslant\left\|A-A^{\prime}\right\|_{F}=\left (\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\cdots+\sigma_{n}^{2}\right)^{\frac{1}{2}}$$

$$\|A-X\|_{F} \geqslant\left (\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\cdots+\sigma_{n}^{2}\right)^{\frac{1}{2}}$$

$$\Omega=\left[\begin{array}{cccccc}
\omega_{1} & & & & \\
& \ddots & & & 0 \\
& & \omega_{k} & & \\
& & & 0 & \\
& & & & \ddots & \\
& 0 & & & & \\
& & & & & 0
\end{array}\right]=\left[\begin{array}{cc}
\Omega_{k} & 0 \\
0 & 0
\end{array}\right]$$

$$\|A-X\|_{F}=\left\|Q (B-\Omega) P^{\mathrm{T}}\right\|_{F}=\|B-\Omega\|_{F}$$

$$B=\left[\begin{array}{ll}
B_{11} & B_{12} \\
B_{21} & B_{22}
\end{array}\right]$$

$$\begin{aligned}
\|A-X\|_{F}^{2} & =\|B-\Omega\|_{F}^{2} \\
& =\left\|B_{11}-\Omega_{k}\right\|_{F}^{2}+\left\|B_{12}\right\|_{F}^{2}+\left\|B_{21}\right\|_{F}^{2}+\left\|B_{22}\right\|_{F}^{2}
\end{aligned}$$

$$Y=Q\left[\begin{array}{cc}
B_{11} & B_{12} \\
0 & 0
\end{array}\right] P^{\mathrm{T}}$$

$$\|A-Y\|_{F}^{2}=\left\|B_{21}\right\|_{F}^{2}+\left\|B_{22}\right\|_{F}^{2}<\|A-X\|_{F}^{2}$$

$$\|A-X\|_{F}^{2}=\left\|B_{11}-\Omega_{k}\right\|_{F}^{2}+\left\|B_{22}\right\|_{F}^{2}$$

$$Z=Q\left[\begin{array}{cc}
B_{11} & 0 \\
0 & 0
\end{array}\right] P^{\mathrm{T}}$$

$$\|A-Z\|_{F}^{2}=\left\|B_{22}\right\|_{F}^{2} \leqslant\left\|B_{11}-\Omega_{k}\right\|_{F}^{2}+\left\|B_{22}\right\|_{F}^{2}=\|A-X\|_{F}^{2}$$

$$\|A-X\|_{F}=\left\|B_{22}\right\|_{F}=\|\Lambda\|_{F}$$

$$U_{2}=\left[\begin{array}{cc}
I_{k} & 0 \\
0 & U_{1}
\end{array}\right], \quad V_{2}=\left[\begin{array}{cc}
I_{k} & 0 \\
0 & V_{1}
\end{array}\right]$$

$$U_{2}^{\mathrm{T}} Q^{\mathrm{T}} A P V_{2}=\left[\begin{array}{cc}
\Omega_{k} & 0 \\
0 & \Lambda
\end{array}\right]$$

$$A=\left (Q U_{2}\right)\left[\begin{array}{cc}
\Omega_{k} & 0 \\
0 & \Lambda
\end{array}\right]\left (P V_{2}\right)^{\mathrm{T}}$$

$$\|A-X\|_{F}=\|\Lambda\|_{F} \geqslant\left (\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\cdots+\sigma_{n}^{2}\right)^{\frac{1}{2}}$$

$$\|A-X\|_{F}=\left (\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\cdots+\sigma_{n}^{2}\right)^{\frac{1}{2}}=\left\|A-A^{\prime}\right\|_{F}$$

$$U \Sigma=\left[\begin{array}{cccc}
\sigma_{1} u_{1} & \sigma_{2} u_{2} & \cdots & \sigma_{n} u_{n}
\end{array}\right]$$

$$V^{\mathrm{T}}=\left[\begin{array}{c}
v_{1}^{\mathrm{T}} \\
v_{2}^{\mathrm{T}} \\
\vdots \\
v_{n}^{\mathrm{T}}
\end{array}\right]$$

$$A=\sigma_{1} u_{1} v_{1}^{\mathrm{T}}+\sigma_{2} u_{2} v_{2}^{\mathrm{T}}+\cdots+\sigma_{n} u_{n} v_{n}^{\mathrm{T}}$$

$$u_{i} v_{j}^{\mathrm{T}}=\left[\begin{array}{c}
u_{1 i} \\
u_{2 i} \\
\vdots \\
u_{m i}
\end{array}\right]\left[\begin{array}{cccc}
v_{1 j} & v_{2 j} & \cdots & v_{n j}
\end{array}\right]=\left[\begin{array}{cccc}
u_{1 i} v_{1 j} & u_{1 i} v_{2 j} & \cdots & u_{1 i} v_{n j} \\
u_{2 i} v_{1 j} & u_{2 i} v_{2 j} & \cdots & u_{2 i} v_{n j} \\
\vdots & \vdots & & \vdots \\
u_{m i} v_{1 j} & u_{m i} v_{2 j} & \cdots & u_{m i} v_{n j}
\end{array}\right]$$

$$A=\sum_{k=1}^{n} A_{k}=\sum_{k=1}^{n} \sigma_{k} u_{k} v_{k}^{\mathrm{T}}$$

$$A=\sigma_{1} u_{1} v_{1}^{\mathrm{T}}+\sigma_{2} u_{2} v_{2}^{\mathrm{T}}+\cdots+\sigma_{n} u_{n} v_{n}^{\mathrm{T}}$$

$$A_{n-1}=\sigma_{1} u_{1} v_{1}^{\mathrm{T}}+\sigma_{2} u_{2} v_{2}^{\mathrm{T}}+\cdots+\sigma_{n-1} u_{n-1} v_{n-1}^{\mathrm{T}}$$

$$A_{n-2}=\sigma_{1} u_{1} v_{1}^{\mathrm{T}}+\sigma_{2} u_{2} v_{2}^{\mathrm{T}}+\cdots+\sigma_{n-2} u_{n-2} v_{n-2}^{\mathrm{T}}$$

$$A_{k}=\sigma_{1} u_{1} v_{1}^{\mathrm{T}}+\sigma_{2} u_{2} v_{2}^{\mathrm{T}}+\cdots+\sigma_{k} u_{k} v_{k}^{\mathrm{T}}$$

$$A=U \Sigma V^{\mathrm{T}}$$

$$\Sigma=\operatorname{diag}\left (\sigma_{1}, \sigma_{2}, \cdots, \sigma_{p}\right), \quad p=\min \{m, n\}$$

$$\sigma_{1} \geqslant \sigma_{2} \geqslant \cdots \geqslant \sigma_{p} \geqslant 0$$

$$A^{\mathrm{T}} A=V\left (\Sigma^{\mathrm{T}} \Sigma\right) V^{\mathrm{T}}$$

$$A A^{\mathrm{T}}=U\left (\Sigma \Sigma^{\mathrm{T}}\right) U^{\mathrm{T}}$$

$$\sigma_{j}=\sqrt{\lambda_{j}}, \quad j=1,2, \cdots, n$$

$$\|A\|_{F}=\left (\sum_{i=1}^{m} \sum_{j=1}^{n}\left (a_{i j}\right)^{2}\right)^{\frac{1}{2}}$$

$$A=\sigma_{1} u_{1} v_{1}^{\mathrm{T}}+\sigma_{2} u_{2} v_{2}^{\mathrm{T}}+\cdots+\sigma_{n} u_{n} v_{n}^{\mathrm{T}}$$

# 主成分分析

$$\boldsymbol{\mu}=E (\boldsymbol{x})=\left (\mu_{1}, \mu_{2}, \cdots, \mu_{m}\right)^{\mathrm{T}}$$

$$\Sigma=\operatorname{cov}(\boldsymbol{x}, \boldsymbol{x})=E\left[(\boldsymbol{x}-\boldsymbol{\mu})(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}}\right]$$

$$y_{i}=\alpha_{i}^{\mathrm{T}} \boldsymbol{x}=\alpha_{1 i} x_{1}+\alpha_{2 i} x_{2}+\cdots+\alpha_{m i} x_{m}$$

$$E\left (y_{i}\right)=\alpha_{i}^{\mathrm{T}} \mu, \quad i=1,2, \cdots, m$$

$$\operatorname{var}\left (y_{i}\right)=\alpha_{i}^{\mathrm{T}} \Sigma \alpha_{i}, \quad i=1,2, \cdots, m$$

$$\operatorname{cov}\left (y_{i}, y_{j}\right)=\alpha_{i}^{\mathrm{T}} \Sigma \alpha_{j}, \quad i=1,2, \cdots, m ; \quad j=1,2, \cdots, m$$

$$\alpha_{i}^{\mathrm{T}} \alpha_{j}=\left\{\begin{array}{cc}
1, & i=j \\
0, & i \neq j
\end{array}\right.$$

$$\alpha_{1}^{\mathrm{T}} \boldsymbol{x}=\sum_{i=1}^{m} \alpha_{i 1} x_{i}$$

$$\alpha_{2}^{\mathrm{T}} \boldsymbol{x}=\sum_{i=1}^{m} \alpha_{i 2} x_{i}$$

$$\alpha_{k}^{\mathrm{T}} \boldsymbol{x}=\sum_{i=1}^{m} \alpha_{i k} x_{i}$$

$$y_{k}=\alpha_{k}^{\mathrm{T}} \boldsymbol{x}=\alpha_{1 k} x_{1}+\alpha_{2 k} x_{2}+\cdots+\alpha_{m k} x_{m}, \quad k=1,2, \cdots, m$$

$$\operatorname{var}\left (y_{k}\right)=\alpha_{k}^{\mathrm{T}} \Sigma \alpha_{k}=\lambda_{k}, \quad k=1,2, \cdots, m$$

$$\operatorname{var}\left (\alpha_{1}^{\mathrm{T}} \boldsymbol{x}\right)=\alpha_{1}^{\mathrm{T}} \Sigma \alpha_{1}$$

$$\begin{array}{ll}
\max _{\alpha_{1}} & \alpha_{1}^{\mathrm{T}} \Sigma \alpha_{1} \\
\text { s.t. } & \alpha_{1}^{\mathrm{T}} \alpha_{1}=1
\end{array}$$

$$\alpha_{1}^{\mathrm{T}} \Sigma \alpha_{1}-\lambda\left (\alpha_{1}^{\mathrm{T}} \alpha_{1}-1\right)$$

$$\Sigma \alpha_{1}-\lambda \alpha_{1}=0$$

$$\alpha_{1}^{\mathrm{T}} \Sigma \alpha_{1}=\alpha_{1}^{\mathrm{T}} \lambda \alpha_{1}=\lambda \alpha_{1}^{\mathrm{T}} \alpha_{1}=\lambda$$

$$\operatorname{var}\left (\alpha_{1}^{\mathrm{T}} \boldsymbol{x}\right)=\alpha_{1}^{\mathrm{T}} \Sigma \alpha_{1}=\lambda_{1}$$

$$\operatorname{var}\left (\alpha_{2}^{\mathrm{T}} \boldsymbol{x}\right)=\alpha_{2}^{\mathrm{T}} \Sigma \alpha_{2}$$

$$\begin{array}{ll}
\max _{\alpha_{2}} & \alpha_{2}^{\mathrm{T}} \Sigma \alpha_{2} \\
\text { s.t. } & \alpha_{1}^{\mathrm{T}} \Sigma \alpha_{2}=0, \\
& \alpha_{2}^{\mathrm{T}} \alpha_{2}=1
\end{array}$$

$$\alpha_{1}^{\mathrm{T}} \Sigma \alpha_{2}=\alpha_{2}^{\mathrm{T}} \Sigma \alpha_{1}=\alpha_{2}^{\mathrm{T}} \lambda_{1} \alpha_{1}=\lambda_{1} \alpha_{2}^{\mathrm{T}} \alpha_{1}=\lambda_{1} \alpha_{1}^{\mathrm{T}} \alpha_{2}$$

$$\alpha_{1}^{\mathrm{T}} \alpha_{2}=0, \quad \alpha_{2}^{\mathrm{T}} \alpha_{1}=0$$

$$\alpha_{2}^{\mathrm{T}} \Sigma \alpha_{2}-\lambda\left (\alpha_{2}^{\mathrm{T}} \alpha_{2}-1\right)-\phi \alpha_{2}^{\mathrm{T}} \alpha_{1}$$

$$2 \Sigma \alpha_{2}-2 \lambda \alpha_{2}-\phi \alpha_{1}=0$$

$$2 \alpha_{1}^{\mathrm{T}} \Sigma \alpha_{2}-2 \lambda \alpha_{1}^{\mathrm{T}} \alpha_{2}-\phi \alpha_{1}^{\mathrm{T}} \alpha_{1}=0$$

$$\Sigma \alpha_{2}-\lambda \alpha_{2}=0$$

$$\alpha_{2}^{\mathrm{T}} \Sigma \alpha_{2}=\alpha_{2}^{\mathrm{T}} \lambda \alpha_{2}=\lambda \alpha_{2}^{\mathrm{T}} \alpha_{2}=\lambda$$

$$\operatorname{var}\left (\alpha_{2}^{\mathrm{T}} \boldsymbol{x}\right)=\alpha_{2}^{\mathrm{T}} \Sigma \alpha_{2}=\lambda_{2}$$

$$\operatorname{var}\left (\alpha_{k}^{\mathrm{T}} \boldsymbol{x}\right)=\alpha_{k}^{\mathrm{T}} \Sigma \alpha_{k}=\lambda_{k}, \quad k=1,2, \cdots, m$$

$$A=\left[\begin{array}{cccc}
\alpha_{11} & \alpha_{12} & \cdots & \alpha_{1 m} \\
\alpha_{21} & \alpha_{22} & \cdots & \alpha_{2 m} \\
\vdots & \vdots & & \vdots \\
\alpha_{m 1} & \alpha_{m 2} & \cdots & \alpha_{m m}
\end{array}\right]$$

$$\begin{array}{l}
\operatorname{cov}(\boldsymbol{y})=\operatorname{diag}\left (\lambda_{1}, \lambda_{2}, \cdots, \lambda_{m}\right) \\
\lambda_{1} \geqslant \lambda_{2} \geqslant \cdots \geqslant \lambda_{m}
\end{array}$$

$$\Sigma \alpha_{k}=\lambda_{k} \alpha_{k}, \quad k=1,2, \cdots, m$$

$$\Sigma A=A \Lambda$$

$$A^{\mathrm{T}} \Sigma A=\Lambda$$

$$\Sigma=A \Lambda A^{\mathrm{T}}$$

$$\operatorname{cov}(\boldsymbol{y})=\Lambda=\operatorname{diag}\left (\lambda_{1}, \lambda_{2}, \cdots, \lambda_{m}\right)$$

$$\sum_{i=1}^{m} \lambda_{i}=\sum_{i=1}^{m} \sigma_{i i}$$

$$\begin{aligned}
\sum_{i=1}^{m} \operatorname{var}\left (x_{i}\right) & =\operatorname{tr}\left (\Sigma^{\mathrm{T}}\right)=\operatorname{tr}\left (A \Lambda A^{\mathrm{T}}\right)=\operatorname{tr}\left (A^{\mathrm{T}} \Lambda A\right) \\
& =\operatorname{tr}(\Lambda)=\sum_{i=1}^{m} \lambda_{i}=\sum_{i=1}^{m} \operatorname{var}\left (y_{i}\right)
\end{aligned}$$

$$\rho\left (y_{k}, x_{i}\right)=\frac{\sqrt{\lambda_{k}} \alpha_{i k}}{\sqrt{\sigma_{i i}}}, \quad k, i=1,2, \cdots, m$$

$$\rho\left (y_{k}, x_{i}\right)=\frac{\operatorname{cov}\left (y_{k}, x_{i}\right)}{\sqrt{\operatorname{var}\left (y_{k}\right) \operatorname{var}\left (x_{i}\right)}}=\frac{\operatorname{cov}\left (\alpha_{k}^{\mathrm{T}} \boldsymbol{x}, e_{i}^{\mathrm{T}} \boldsymbol{x}\right)}{\sqrt{\lambda_{k}} \sqrt{\sigma_{i i}}}$$

$$\operatorname{cov}\left (\alpha_{k}^{\mathrm{T}} \boldsymbol{x}, e_{i}^{\mathrm{T}} \boldsymbol{x}\right)=\alpha_{k}^{\mathrm{T}} \Sigma e_{i}=e_{i}^{\mathrm{T}} \Sigma \alpha_{k}=\lambda_{k} e_{i}^{\mathrm{T}} \alpha_{k}=\lambda_{k} \alpha_{i k}$$

$$\sum_{i=1}^{m} \sigma_{i i} \rho^{2}\left (y_{k}, x_{i}\right)=\lambda_{k}$$

$$\sum_{i=1}^{m} \sigma_{i i} \rho^{2}\left (y_{k}, x_{i}\right)=\sum_{i=1}^{m} \lambda_{k} \alpha_{i k}^{2}=\lambda_{k} \alpha_{k}^{\mathrm{T}} \alpha_{k}=\lambda_{k}$$

$$\sum_{k=1}^{m} \rho^{2}\left (y_{k}, x_{i}\right)=1$$

$$\rho^{2}\left (x_{i},\left (y_{1}, y_{2}, \cdots, y_{m}\right)\right)=\sum_{k=1}^{m} \rho^{2}\left (y_{k}, x_{i}\right)$$

$$\rho^{2}\left (x_{i},\left (y_{1}, y_{2}, \cdots, y_{m}\right)\right)=1$$

$$\boldsymbol{y}=B^{\mathrm{T}} \boldsymbol{x}$$

$$\Sigma_{y}=B^{\mathrm{T}} \Sigma B$$

$$\beta_{k}=\sum_{j=1}^{m} c_{j k} \alpha_{j}, \quad k=1,2, \cdots, q$$

$$B=A C$$

$$B^{\mathrm{T}} \Sigma B=C^{\mathrm{T}} A^{\mathrm{T}} \Sigma A C=C^{\mathrm{T}} \Lambda C=\sum_{j=1}^{m} \lambda_{j} c_{j} c_{j}^{\mathrm{T}}$$

$$\begin{aligned}
\operatorname{tr}\left (B^{\mathrm{T}} \Sigma B\right) & =\sum_{j=1}^{m} \lambda_{j} \operatorname{tr}\left (c_{j} c_{j}^{\mathrm{T}}\right) \\
& =\sum_{j=1}^{m} \lambda_{j} \operatorname{tr}\left (c_{j}^{\mathrm{T}} c_{j}\right) \\
& =\sum_{j=1}^{m} \lambda_{j} c_{j}^{\mathrm{T}} c_{j} \\
& =\sum_{j=1}^{m} \sum_{k=1}^{q} \lambda_{j} c_{j k}^{2}
\end{aligned}$$

$$C=A^{\mathrm{T}} B$$

$$C^{\mathrm{T}} C=B^{\mathrm{T}} A A^{\mathrm{T}} B=B^{\mathrm{T}} B=I_{q}$$

$$\operatorname{tr}\left (C^{\mathrm{T}} C\right)=\operatorname{tr}\left (I_{q}\right)$$

$$\sum_{j=1}^{m} \sum_{k=1}^{q} c_{j k}^{2}=q$$

$$d_{j}^{\mathrm{T}} d_{j}=1, \quad j=1,2, \cdots, m$$

$$c_{j}^{\mathrm{T}} c_{j} \leqslant 1, \quad j=1,2, \cdots, m$$

$$\sum_{k=1}^{q} c_{j k}^{2} \leqslant 1, \quad j=1,2, \cdots, m$$

$$\sum_{k=1}^{q} c_{j k}^{2}=\left\{\begin{array}{ll}
1, & j=1, \cdots, q \\
0, & j=q+1, \cdots, m
\end{array}\right.$$

$$c_{j k}=\left\{\begin{array}{ll}
1, & 1 \leqslant j=k \leqslant q \\
0, & \text { 其他 }
\end{array}\right.$$

$$\boldsymbol{y}=B^{\mathrm{T}} \boldsymbol{x}$$

$$\eta_{k}=\frac{\lambda_{k}}{\sum_{i=1}^{m} \lambda_{i}}$$

$$\sum_{i=1}^{k} \eta_{i}=\frac{\sum_{i=1}^{k} \lambda_{i}}{\sum_{i=1}^{m} \lambda_{i}}$$

$$\nu_{i}=\rho^{2}\left (x_{i},\left (y_{1}, y_{2}, \cdots, y_{k}\right)\right)$$

$$\nu_{i}=\rho^{2}\left (x_{i},\left (y_{1}, y_{2}, \cdots, y_{k}\right)\right)=\sum_{j=1}^{k} \rho^{2}\left (x_{i}, y_{j}\right)=\sum_{j=1}^{k} \frac{\lambda_{j} \alpha_{i j}^{2}}{\sigma_{i i}}$$

$$x_{i}^{*}=\frac{x_{i}-E\left (x_{i}\right)}{\sqrt{\operatorname{var}\left (x_{i}\right)}}, \quad i=1,2, \cdots, m$$

$$\Lambda^{*}=\operatorname{diag}\left (\lambda_{1}^{*}, \lambda_{2}^{*}, \cdots, \lambda_{m}^{*}\right)$$

$$\sum_{k=1}^{m} \lambda_{k}^{*}=m$$

$$\rho\left (y_{k}^{*}, x_{i}^{*}\right)=\sqrt{\lambda_{k}^{*}} e_{i k}^{*}, \quad k, i=1,2, \cdots, m$$

$$\sum_{i=1}^{m} \rho^{2}\left (y_{k}^{*}, x_{i}^{*}\right)=\sum_{i=1}^{m} \lambda_{k}^{*} e_{i k}^{* 2}=\lambda_{k}^{*}, \quad k=1,2, \cdots, m$$

$$\sum_{k=1}^{m} \rho^{2}\left (y_{k}^{*}, x_{i}^{*}\right)=\sum_{k=1}^{m} \lambda_{k}^{*} e_{i k}^{* 2}=1, \quad i=1,2, \cdots, m$$

$$X=\left[\begin{array}{llll}
x_{1} & x_{2} & \cdots & \boldsymbol{x}_{n}
\end{array}\right]=\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1 n} \\
x_{21} & x_{22} & \cdots & x_{2 n} \\
\vdots & \vdots & & \vdots \\
x_{m 1} & x_{m 2} & \cdots & x_{m n}
\end{array}\right]$$

$$\bar{x}=\frac{1}{n} \sum_{j=1}^{n} \boldsymbol{x}_{j}$$

$$\begin{array}{l}
S=\left[s_{i j}\right]_{m \times m} \\
s_{i j}=\frac{1}{n-1} \sum_{k=1}^{n}\left (x_{i k}-\bar{x}_{i}\right)\left (x_{j k}-\bar{x}_{j}\right), \quad i, j=1,2, \cdots, m
\end{array}$$

$$R=\left[r_{i j}\right]_{m \times m}, \quad r_{i j}=\frac{s_{i j}}{\sqrt{s_{i i} s_{j j}}}, \quad i, j=1,2, \cdots, m$$

$$\boldsymbol{y}=A^{\mathrm{T}} \boldsymbol{x}$$

$$A=\left[\begin{array}{llll}
a_{1} & a_{2} & \cdots & a_{m}
\end{array}\right]=\left[\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 m} \\
a_{21} & a_{22} & \cdots & a_{2 m} \\
\vdots & \vdots & & \vdots \\
a_{m 1} & a_{m 2} & \cdots & a_{m m}
\end{array}\right]$$

$$a_{i}=\left (a_{1 i}, a_{2 i}, \cdots, a_{m i}\right)^{\mathrm{T}}, \quad i=1,2, \cdots, m$$

$$\boldsymbol{y}_{i}=a_{i}^{\mathrm{T}} \boldsymbol{x}=a_{1 i} \boldsymbol{x}_{1}+a_{2 i} \boldsymbol{x}_{2}+\cdots+a_{m i} \boldsymbol{x}_{m}, \quad i=1,2, \cdots, m$$

$$\bar{y}_{i}=\frac{1}{n} \sum_{j=1}^{n} a_{i}^{\mathrm{T}} \boldsymbol{x}_{j}=a_{i}^{\mathrm{T}} \overline{\boldsymbol{x}}$$

$$\overline{\boldsymbol{x}}=\frac{1}{n} \sum_{j=1}^{n} \boldsymbol{x}_{j}$$

$$\begin{aligned}
\operatorname{var}\left (y_{i}\right) & =\frac{1}{n-1} \sum_{j=1}^{n}\left (a_{i}^{\mathrm{T}} \boldsymbol{x}_{j}-a_{i}^{\mathrm{T}} \overline{\boldsymbol{x}}\right)^{2} \\
& =a_{i}^{\mathrm{T}}\left[\frac{1}{n-1} \sum_{j=1}^{n}\left (\boldsymbol{x}_{j}-\overline{\boldsymbol{x}}\right)\left (\boldsymbol{x}_{j}-\overline{\boldsymbol{x}}\right)^{\mathrm{T}}\right] a_{i}=a_{i}^{\mathrm{T}} S a_{i}
\end{aligned}$$

$$\operatorname{cov}\left (y_{i}, y_{k}\right)=a_{i}^{\mathrm{T}} S a_{k}$$

$$x_{i j}^{*}=\frac{x_{i j}-\bar{x}_{i}}{\sqrt{s_{i i}}}, \quad i=1,2, \cdots, m ; \quad j=1,2, \cdots, n$$

$$\begin{aligned}
\bar{x}_{i} & =\frac{1}{n} \sum_{j=1}^{n} x_{i j}, \quad i=1,2, \cdots, m \\
s_{i i} & =\frac{1}{n-1} \sum_{j=1}^{n}\left (x_{i j}-\bar{x}_{i}\right)^{2}, \quad i=1,2, \cdots, m
\end{aligned}$$

$$R=\frac{1}{n-1} X X^{\mathrm{T}}$$

$$R=\left[r_{i j}\right]_{m \times m}=\frac{1}{n-1} X X^{\mathrm{T}}$$

$$r_{i j}=\frac{1}{n-1} \sum_{l=1}^{n} x_{i l} x_{l j}, \quad i, j=1,2, \cdots, m$$

$$|R-\lambda I|=0$$

$$\lambda_{1} \geqslant \lambda_{2} \geqslant \cdots \geqslant \lambda_{m}$$

$$a_{i}=\left (a_{1 i}, a_{2 i}, \cdots, a_{m i}\right)^{\mathrm{T}}, \quad i=1,2, \cdots, k$$

$$y_{i}=a_{i}^{\mathrm{T}} \boldsymbol{x}, \quad i=1,2, \cdots, k$$

$$\begin{array}{c}
y_{i j}=\left (a_{1 i}, a_{2 i}, \cdots, a_{m i}\right)\left (x_{1 j}, x_{2 j}, \cdots, x_{m j}\right)^{\mathrm{T}}=\sum_{l=1}^{m} a_{l i} x_{l j} \\
i=1,2, \cdots, m, \quad j=1,2, \cdots, n
\end{array}$$

$$A \approx U_{k} \Sigma_{k} V_{k}^{\mathrm{T}}$$

$$X^{\prime}=\frac{1}{\sqrt{n-1}} X^{\mathrm{T}}$$

$$\begin{aligned}
X^{\prime \mathrm{T}} X^{\prime} & =\left (\frac{1}{\sqrt{n-1}} X^{\mathrm{T}}\right)^{\mathrm{T}}\left (\frac{1}{\sqrt{n-1}} X^{\mathrm{T}}\right) \\
& =\frac{1}{n-1} X X^{\mathrm{T}}
\end{aligned}$$

$$S_{X}=X^{\prime \mathrm{T}} X^{\prime}$$

$$X^{\prime}=\frac{1}{\sqrt{n-1}} X^{\mathbf{T}}$$

$$X^{\prime}=U \Sigma V^{\mathrm{T}}$$

$$Y=V^{\mathrm{T}} X$$

$$y_{i}=\alpha_{i}^{\mathrm{T}} \boldsymbol{x}=\sum_{k=1}^{m} \alpha_{k i} x_{k}, \quad i=1,2, \cdots, m$$

$$y_{i}=\alpha_{i}^{\mathrm{T}} \boldsymbol{x}=\sum_{k=1}^{m} \alpha_{k i} x_{k}, \quad i=1,2, \cdots, m$$

$$\operatorname{var}\left (y_{i}\right)=\alpha_{i}^{\mathrm{T}} \Sigma \alpha_{i}=\lambda_{i}$$

$$\operatorname{cov}(\boldsymbol{y})=\Lambda=\operatorname{diag}\left (\lambda_{1}, \lambda_{2}, \cdots, \lambda_{m}\right)$$

$$\sum_{i=1}^{m} \lambda_{i}=\sum_{i=1}^{m} \sigma_{i i}$$

$$\rho\left (y_{k}, x_{i}\right)=\frac{\sqrt{\lambda_{k}} \alpha_{i k}}{\sqrt{\sigma_{i i}}}, \quad k, i=1,2, \cdots, m$$

$$X=\left[\begin{array}{llll}
\boldsymbol{x}_{1} & \boldsymbol{x}_{2} & \cdots & \boldsymbol{x}_{n}
\end{array}\right]=\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1 n} \\
x_{21} & x_{22} & \cdots & x_{2 n} \\
\vdots & \vdots & & \vdots \\
x_{m 1} & x_{m 2} & \cdots & x_{m n}
\end{array}\right]$$

$$\begin{array}{c}
S=\left[s_{i j}\right]_{m \times m}, \quad s_{i j}=\frac{1}{n-1} \sum_{k=1}^{n}\left (x_{i k}-\bar{x}_{i}\right)\left (x_{j k}-\bar{x}_{j}\right) \\
i=1,2, \cdots, m, \quad j=1,2, \cdots, m
\end{array}$$

$$\boldsymbol{y}=A^{\mathrm{T}} \boldsymbol{x}$$

$$A=\left[\begin{array}{llll}
a_{1} & a_{2} & \cdots & a_{m}
\end{array}\right]=\left[\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 m} \\
a_{21} & a_{22} & \cdots & a_{2 m} \\
\vdots & \vdots & & \vdots \\
a_{m 1} & a_{m 2} & \cdots & a_{m m}
\end{array}\right]$$

$$R=\frac{1}{n-1} X X^{\mathrm{T}}$$

$$V=\left (v_{1}, v_{2}, \cdots, v_{k}\right)$$

$$Y=V^{\mathrm{T}} X$$

$$X^{\prime}=\frac{1}{\sqrt{n-1}} X^{\mathrm{T}}$$

$$X^{\prime}=U S V^{\mathrm{T}}$$

$$Y=V^{\mathrm{T}} X$$

# 潜在语义分析

$$X=\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1 n} \\
x_{21} & x_{22} & \cdots & x_{2 n} \\
\vdots & \vdots & & \vdots \\
x_{m 1} & x_{m 2} & \cdots & x_{m n}
\end{array}\right]$$

$$\operatorname{TFIDF}_{i j}=\frac{\mathrm{tf}_{i j}}{\mathrm{tf}_{\bullet j}} \log \frac{\mathrm{df}}{\mathrm{df}_{i}}, \quad i=1,2, \cdots, m ; \quad j=1,2, \cdots, n
$$

$$x_{j}=\left[\begin{array}{c}
x_{1 j} \\
x_{2 j} \\
\vdots \\
x_{m j}
\end{array}\right], \quad j=1,2, \cdots, n$$

$$x_{i} \cdot x_{j}, \quad \frac{x_{i} \cdot x_{j}}{\left\|x_{i}\right\|\left\|x_{j}\right\|}$$

$$X=\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1 n} \\
x_{21} & x_{22} & \cdots & x_{2 n} \\
\vdots & \vdots & & \vdots \\
x_{m 1} & x_{m 2} & \cdots & x_{m n}
\end{array}\right]$$

$$t_{l}=\left[\begin{array}{c}
t_{1 l} \\
t_{2 l} \\
\vdots \\
t_{m l}
\end{array}\right], \quad l=1,2, \cdots, k$$

$$T=\left[\begin{array}{cccc}
t_{11} & t_{12} & \cdots & t_{1 k} \\
t_{21} & t_{22} & \cdots & t_{2 k} \\
\vdots & \vdots & & \vdots \\
t_{m 1} & t_{m 2} & \cdots & t_{m k}
\end{array}\right]$$

$$y_{j}=\left[\begin{array}{c}
y_{1 j} \\
y_{2 j} \\
\vdots \\
y_{k j}
\end{array}\right], \quad j=1,2, \cdots, n$$

$$Y=\left[\begin{array}{cccc}
y_{11} & y_{12} & \cdots & y_{1 n} \\
y_{21} & y_{22} & \cdots & y_{2 n} \\
\vdots & \vdots & & \vdots \\
y_{k 1} & y_{k 2} & \cdots & y_{k n}
\end{array}\right]$$

$$x_{j} \approx y_{1 j} t_{1}+y_{2 j} t_{2}+\cdots+y_{k j} t_{k}, \quad j=1,2, \cdots, n$$

$$X \approx T Y$$

$$X=\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1 n} \\
x_{21} & x_{22} & \cdots & x_{2 n} \\
\vdots & \vdots & & \vdots \\
x_{m 1} & x_{m 2} & \cdots & x_{m n}
\end{array}\right]$$

$$X \approx U_{k} \Sigma_{k} V_{k}^{\mathrm{T}}=\left[\begin{array}{llll}
u_{1} & u_{2} & \cdots & u_{k}
\end{array}\right]\left[\begin{array}{cccc}
\sigma_{1} & 0 & 0 & 0 \\
0 & \sigma_{2} & 0 & 0 \\
0 & 0 & \ddots & 0 \\
0 & 0 & 0 & \sigma_{k}
\end{array}\right]\left[\begin{array}{c}
v_{1}^{\mathrm{T}} \\
v_{2}^{\mathrm{T}} \\
\vdots \\
v_{k}^{\mathrm{T}}
\end{array}\right]$$

$$U_{k}=\left[\begin{array}{llll}
u_{1} & u_{2} & \cdots & u_{k}
\end{array}\right]$$

$$\begin{aligned}
X & =\left[\begin{array}{llll}
x_{1} & x_{2} & \cdots & x_{n}
\end{array}\right] \approx U_{k} \Sigma_{k} V_{k}^{\mathrm{T}} \\
&=  {\left[\begin{array}{llll}
u_{1} & u_{2} & \cdots & u_{k}
\end{array}\right]\left[\begin{array}{llll}
\sigma_{1} & & \\
\sigma_{2} & 0 & \\
0 & \ddots & \\
& & \sigma_{k}
\end{array}\right]\left[\begin{array}{cccc}
v_{11} & v_{21} & \cdots & v_{n 1} \\
v_{12} & v_{22} & \cdots & v_{n 2} \\
\vdots & \vdots & & \vdots \\
v_{1 k} & v_{2 k} & \cdots & v_{n k}
\end{array}\right] } \\
& =\left[\begin{array}{llllll}
u_{1} & u_{2} & \cdots & u_{k}
\end{array}\right]\left[\begin{array}{cccc}
\sigma_{1} v_{11} & \sigma_{1} v_{21} & \cdots & \sigma_{1} v_{n 1} \\
\sigma_{2} v_{12} & \sigma_{2} v_{22} & \cdots & \sigma_{2} v_{n 2} \\
\vdots & \vdots & & \vdots \\
\sigma_{k} v_{1 k} & \sigma_{k} v_{2 k} & \cdots & \sigma_{k} v_{n k}
\end{array}\right]
\end{aligned}$$

$$u_{l}=\left[\begin{array}{c}
u_{1 l} \\
u_{2 l} \\
\vdots \\
u_{m l}
\end{array}\right], \quad l=1,2, \cdots, k$$

$$\begin{aligned}
x_{j} & \approx U_{k}\left (\Sigma_{k} V_{k}^{\mathrm{T}}\right)_{j} \\
& =\left[\begin{array}{llll}
u_{1} & u_{2} & \cdots & u_{k}
\end{array}\right]\left[\begin{array}{c}
\sigma_{1} v_{j 1} \\
\sigma_{2} v_{j 2} \\
\vdots \\
\sigma_{k} v_{j k}
\end{array}\right] \\
& =\sum_{l=1}^{k} \sigma_{l} v_{j l} u_{l}, \quad j=1,2, \cdots, n
\end{aligned}$$

$$\left[\begin{array}{c}
\sigma_{1} v_{11} \\
\sigma_{2} v_{12} \\
\vdots \\
\sigma_{k} v_{1 k}
\end{array}\right],\left[\begin{array}{c}
\sigma_{1} v_{21} \\
\sigma_{2} v_{22} \\
\vdots \\
\sigma_{k} v_{2 k}
\end{array}\right], \cdots,\left[\begin{array}{c}
\sigma_{1} v_{n 1} \\
\sigma_{2} v_{n 2} \\
\vdots \\
\sigma_{k} v_{n k}
\end{array}\right]$$

$$X \approx U_{k} \Sigma_{k} V_{k}^{\mathrm{T}}=U_{k}\left (\Sigma_{k} V_{k}^{\mathrm{T}}\right)$$

$$X \approx W H$$

$$\begin{aligned}
x_{j} & \approx W h_{j} \\
& =\left[\begin{array}{llll}
w_{1} & w_{2} & \cdots & w_{k}
\end{array}\right]\left[\begin{array}{c}
h_{1 j} \\
h_{2 j} \\
\vdots \\
h_{k j}
\end{array}\right] \\
& =\sum_{l=1}^{k} h_{l j} w_{l}, \quad j=1,2, \cdots, n
\end{aligned}$$

$$X \approx W H$$

$$\|A-B\|^{2}=\sum_{i, j}\left (a_{i j}-b_{i j}\right)^{2}$$

$$D (A \| B)=\sum_{i, j}\left (a_{i j} \log \frac{a_{i j}}{b_{i j}}-a_{i j}+b_{i j}\right)$$

$$\begin{array}{cl}
\min _{W, H} & \|X-W H\|^{2} \\
\text { s.t. } & W, H \geqslant 0
\end{array}$$

$$\begin{array}{l}
\min _{W, H} D (X \| W H) \\
\text { s.t. } \quad W, H \geqslant 0
\end{array}$$

$$\begin{array}{c}
H_{l j} \leftarrow H_{l j} \frac{\left (W^{\mathrm{T}} X\right)_{l j}}{\left (W^{\mathrm{T}} W H\right)_{l j}} \\
W_{i l} \leftarrow W_{i l} \frac{\left (X H^{\mathrm{T}}\right)_{i l}}{\left (W H H^{\mathrm{T}}\right)_{i l}}
\end{array}$$

$$\begin{array}{r}
H_{l j} \leftarrow H_{l j} \frac{\sum_{i}\left[W_{i l} X_{i j} /(W H)_{i j}\right]}{\sum_{i} W_{i l}} \\
W_{i l} \leftarrow W_{i l} \frac{\sum_{j}\left[H_{l j} X_{i j} /(W H)_{i j}\right]}{\sum_{j} H_{l j}}
\end{array}$$

$$J (W, H)=\frac{1}{2}\|X-W H\|^{2}=\frac{1}{2} \sum\left[X_{i j}-(W H)_{i j}\right]^{2}$$

$$\begin{aligned}
\frac{\partial J (W, H)}{\partial W_{i l}} & =-\sum_{j}\left[X_{i j}-(W H)_{i j}\right] H_{l j} \\
& =-\left[\left (X H^{\mathrm{T}}\right)_{i l}-\left (W H H^{\mathrm{T}}\right)_{i l}\right]
\end{aligned}$$

$$\frac{\partial J (W, H)}{\partial H_{l j}}=-\left[\left (W^{\mathrm{T}} X\right)_{l j}-\left (W^{\mathrm{T}} W H\right)_{l j}\right]$$

$$\begin{array}{l}
W_{i l}=W_{i l}+\lambda_{i l}\left[\left (X H^{\mathrm{T}}\right)_{i l}-\left (W H H^{\mathrm{T}}\right)_{i l}\right] \\
H_{l j}=H_{l j}+\mu_{l j}\left[\left (W^{\mathrm{T}} X\right)_{l j}-\left (W^{\mathrm{T}} W H\right)_{l j}\right]
\end{array}$$

$$\lambda_{i l}=\frac{W_{i l}}{\left (W H H^{\mathrm{T}}\right)_{i l}}, \quad \mu_{l j}=\frac{H_{l j}}{\left (W^{\mathrm{T}} W H\right)_{l j}}$$

$$\begin{array}{l}
W_{i l}=W_{i l} \frac{\left (X H^{\mathrm{T}}\right)_{i l}}{\left (W H H^{\mathrm{T}}\right)_{i l}}, \quad i=1,2, \cdots, m ; \quad l=1,2, \cdots, k \\
H_{l j}=H_{l j} \frac{\left (W^{\mathrm{T}} X\right)_{l j}}{\left (W^{\mathrm{T}} W H\right)_{l j}}, \quad l=1,2, \cdots, k ; \quad j=1,2, \cdots, n
\end{array}$$

$$X=\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1 n} \\
x_{21} & x_{22} & \cdots & x_{2 n} \\
\vdots & \vdots & & \vdots \\
x_{m 1} & x_{m 2} & \cdots & x_{m n}
\end{array}\right]$$

$$Y=\left[\begin{array}{cccc}
y_{11} & y_{12} & \cdots & y_{1 n} \\
y_{21} & y_{22} & \cdots & y_{2 n} \\
\vdots & \vdots & & \vdots \\
y_{k 1} & y_{k 2} & \cdots & y_{k n}
\end{array}\right]$$

$$T=\left[\begin{array}{cccc}
t_{11} & t_{12} & \cdots & t_{1 k} \\
t_{21} & t_{22} & \cdots & t_{2 k} \\
\vdots & \vdots & & \vdots \\
t_{m 1} & t_{m 2} & \cdots & t_{m k}
\end{array}\right]$$

$$X=\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1 n} \\
x_{21} & x_{22} & \cdots & x_{2 n} \\
\vdots & \vdots & & \vdots \\
x_{m 1} & x_{m 2} & \cdots & x_{m n}
\end{array}\right]$$

$$X \approx T Y$$

$$X \approx U_{k} \Sigma_{k} V_{k}^{\mathrm{T}}=U_{k}\left (\Sigma_{k} V_{k}^{\mathrm{T}}\right)$$

$$X \approx W H$$

$$\begin{array}{cl}
\min _{W, H} & \|X-W H\|^{2} \\
\text { s.t. } & W, H \geqslant 0
\end{array}$$

# 概率潜在语义分析

$$P (T)=\prod_{(w, d)} P (w, d)^{n (w, d)}$$

$$\begin{aligned}
P (w, d) & =P (d) P (w \mid d) \\
& =P (d) \sum_{z} P (w, z \mid d) \\
& =P (d) \sum_{z} P (z \mid d) P (w \mid z)
\end{aligned}$$

$$P (w, z \mid d)=P (z \mid d) P (w \mid z)$$

$$P (T)=\prod_{(w, d)} P (w, d)^{n (w, d)}$$

$$P (w, d)=\sum_{z \in Z} P (z) P (w \mid z) P (d \mid z)$$

$$P (w, d \mid z)=P (w \mid z) P (d \mid z)$$

$$\sum_{i=1}^{M} P\left (w_{i} \mid d\right)=1, \quad 0 \leqslant P\left (w_{i} \mid d\right) \leqslant 1, \quad i=1, \cdots, M$$

$$P (w \mid d)=\sum_{z} P (z \mid d) P (w \mid z)$$

$$\begin{aligned}
X^{\prime} & =U^{\prime} \Sigma^{\prime} V^{\prime \mathrm{T}} \\
X^{\prime} & =[P (w, d)]_{M \times N} \\
U^{\prime} & =[P (w \mid z)]_{M \times K} \\
\Sigma^{\prime} & =[P (z)]_{K \times K} \\
V^{\prime} & =[P (d \mid z)]_{N \times K}
\end{aligned}$$

$$\begin{aligned}
L & =\sum_{i=1}^{M} \sum_{j=1}^{N} n\left (w_{i}, d_{j}\right) \log P\left (w_{i}, d_{j}\right) \\
& =\sum_{i=1}^{M} \sum_{j=1}^{N} n\left (w_{i}, d_{j}\right) \log \left[\sum_{k=1}^{K} P\left (w_{i} \mid z_{k}\right) P\left (z_{k} \mid d_{j}\right)\right]
\end{aligned}$$

$$Q=\sum_{k=1}^{K}\left\{\sum_{j=1}^{N} n\left (d_{j}\right)\left[\log P\left (d_{j}\right)+\sum_{i=1}^{M} \frac{n\left (w_{i}, d_{j}\right)}{n\left (d_{j}\right)} \log P\left (w_{i} \mid z_{k}\right) P\left (z_{k} \mid d_{j}\right)\right]\right\} P\left (z_{k} \mid w_{i}, d_{j}\right)$$

$$Q^{\prime}=\sum_{i=1}^{M} \sum_{j=1}^{N} n\left (w_{i}, d_{j}\right) \sum_{k=1}^{K} P\left (z_{k} \mid w_{i}, d_{j}\right) \log \left[P\left (w_{i} \mid z_{k}\right) P\left (z_{k} \mid d_{j}\right)\right]$$

$$P\left (z_{k} \mid w_{i}, d_{j}\right)=\frac{P\left (w_{i} \mid z_{k}\right) P\left (z_{k} \mid d_{j}\right)}{\sum_{k=1}^{K} P\left (w_{i} \mid z_{k}\right) P\left (z_{k} \mid d_{j}\right)}$$

$$\begin{array}{l}
\sum_{i=1}^{M} P\left (w_{i} \mid z_{k}\right)=1, \quad k=1,2, \cdots, K \\
\sum_{k=1}^{K} P\left (z_{k} \mid d_{j}\right)=1, \quad j=1,2, \cdots, N
\end{array}$$

$$\Lambda=Q^{\prime}+\sum_{k=1}^{K} \tau_{k}\left (1-\sum_{i=1}^{M} P\left (w_{i} \mid z_{k}\right)\right)+\sum_{j=1}^{N} \rho_{j}\left (1-\sum_{k=1}^{K} P\left (z_{k} \mid d_{j}\right)\right)$$

$$\begin{array}{l}
\sum_{j=1}^{N} n\left (w_{i}, d_{j}\right) P\left (z_{k} \mid w_{i}, d_{j}\right)-\tau_{k} P\left (w_{i} \mid z_{k}\right)=0, \quad i=1,2, \cdots, M ; \quad k=1,2, \cdots, K \\
\sum_{i=1}^{M} n\left (w_{i}, d_{j}\right) P\left (z_{k} \mid w_{i}, d_{j}\right)-\rho_{j} P\left (z_{k} \mid d_{j}\right)=0, \quad j=1,2, \cdots, N ; \quad k=1,2, \cdots, K
\end{array}$$

$$P\left (w_{i} \mid z_{k}\right)=\frac{\sum_{j=1}^{N} n\left (w_{i}, d_{j}\right) P\left (z_{k} \mid w_{i}, d_{j}\right)}{\sum_{m=1}^{M} \sum_{j=1}^{N} n\left (w_{m}, d_{j}\right) P\left (z_{k} \mid w_{m}, d_{j}\right)}
$$

$$P\left (z_{k} \mid d_{j}\right)=\frac{\sum_{i=1}^{M} n\left (w_{i}, d_{j}\right) P\left (z_{k} \mid w_{i}, d_{j}\right)}{n\left (d_{j}\right)}$$

$$P\left (z_{k} \mid w_{i}, d_{j}\right)=\frac{P\left (w_{i} \mid z_{k}\right) P\left (z_{k} \mid d_{j}\right)}{\sum_{k=1}^{K} P\left (w_{i} \mid z_{k}\right) P\left (z_{k} \mid d_{j}\right)}$$

$$\begin{aligned}
P\left (w_{i} \mid z_{k}\right) & =\frac{\sum_{j=1}^{N} n\left (w_{i}, d_{j}\right) P\left (z_{k} \mid w_{i}, d_{j}\right)}{\sum_{m=1}^{M} \sum_{j=1}^{N} n\left (w_{m}, d_{j}\right) P\left (z_{k} \mid w_{m}, d_{j}\right)} \\
P\left (z_{k} \mid d_{j}\right) & =\frac{\sum_{i=1}^{M} n\left (w_{i}, d_{j}\right) P\left (z_{k} \mid w_{i}, d_{j}\right)}{n\left (d_{j}\right)}
\end{aligned}$$

$$P (T)=\prod_{(w, d)} P (w, d)^{n (w, d)}$$

$$P (w, d)=P (d) P (w \mid d)=P (d) \sum_{z} P (z \mid d) P (w \mid z)$$

$$P (T)=\prod_{(w, d)} P (w, d)^{n (w, d)}$$

$$P (w, d)=\sum_{z \in Z} P (z) P (w \mid z) P (d \mid z)$$

# 马尔可夫链蒙特卡罗法

$$\hat{f}_{n}=\frac{1}{n} \sum_{i=1}^{n} f\left (x_{i}\right)$$

$$\hat{f}_{n} \rightarrow E_{p (x)}[f (x)], \quad n \rightarrow \infty$$

$$E_{p (x)}[f (x)] \approx \frac{1}{n} \sum_{i=1}^{n} f\left (x_{i}\right)$$

$$\int_{\mathcal{X}} h (x) \mathrm{d} x$$

$$\int_{\mathcal{X}} h (x) \mathrm{d} x=\int_{\mathcal{X}} f (x) p (x) \mathrm{d} x=E_{p (x)}[f (x)]$$

$$\int_{\mathcal{X}} h (x) \mathrm{d} x=E_{p (x)}[f (x)] \approx \frac{1}{n} \sum_{i=1}^{n} f\left (x_{i}\right)$$

$$P\left (X_{t} \mid X_{0}, X_{1}, \cdots, X_{t-1}\right)=P\left (X_{t} \mid X_{t-1}\right), \quad t=1,2, \cdots$$

$$P\left (X_{t+s} \mid X_{t-1+s}\right)=P\left (X_{t} \mid X_{t-1}\right), \quad t=1,2, \cdots ; \quad s=1,2, \cdots$$

$$P\left (X_{t} \mid X_{0} X_{1} \cdots X_{t-2} X_{t-1}\right)=P\left (X_{t} \mid X_{t-n} \cdots X_{t-2} X_{t-1}\right)$$

$$p_{i j}=\left (X_{t}=i \mid X_{t-1}=j\right), \quad i=1,2, \cdots ; \quad j=1,2, \cdots$$

$$p_{i j} \geqslant 0, \quad \sum_{i} p_{i j}=1$$

$$P=\left[\begin{array}{cccc}
p_{11} & p_{12} & p_{13} & \cdots \\
p_{21} & p_{22} & p_{23} & \cdots \\
p_{31} & p_{32} & p_{33} & \cdots \\
\cdots & \cdots & \cdots & \cdots
\end{array}\right]$$

$$\pi (t)=\left[\begin{array}{c}
\pi_{1}(t) \\
\pi_{2}(t) \\
\vdots
\end{array}\right]$$

$$\pi_{i}(t)=P\left (X_{t}=i\right), \quad i=1,2, \cdots$$

$$\pi (0)=\left[\begin{array}{c}
\pi_{1}(0) \\
\pi_{2}(0) \\
\vdots
\end{array}\right]$$

$$\begin{aligned}
& P\left (w_{1} w_{2} \cdots w_{s}\right) \\
= & P\left (w_{1}\right) P\left (w_{2} \mid w_{1}\right) P\left (w_{3} \mid w_{1} w_{2}\right) \cdots P\left (w_{i} \mid w_{1} w_{2} \cdots w_{i-1}\right) \cdots P\left (w_{s} \mid w_{1} w_{2} \cdots w_{s-1}\right) \\
= & P\left (w_{1}\right) P\left (w_{2} \mid w_{1}\right) P\left (w_{3} \mid w_{2}\right) \cdots P\left (w_{i} \mid w_{i-1}\right) \cdots P\left (w_{s} \mid w_{s-1}\right)
\end{aligned}$$

$$\pi (t)=P \pi (t-1)$$

$$\begin{aligned}
\pi_{i}(t) & =P\left (X_{t}=i\right) \\
& =\sum_{m} P\left (X_{t}=i \mid X_{t-1}=m\right) P\left (X_{t-1}=m\right) \\
& =\sum_{m} p_{i m} \pi_{m}(t-1)
\end{aligned}$$

$$\pi (t)=P \pi (t-1)=P (P \pi (t-2))=P^{2} \pi (t-2)$$

$$\pi (t)=P^{t} \pi (0)$$

$$P_{i j}^{t}=P\left (X_{t}=i \mid X_{0}=j\right)$$

$$\pi=\left[\begin{array}{c}
\pi_{1} \\
\pi_{2} \\
\vdots
\end{array}\right]$$

$$\pi=P \pi$$

$$\begin{array}{l}
x_{i}=\sum_{j} p_{i j} x_{j}, \quad i=1,2, \cdots \\
x_{i} \geqslant 0, \quad i=1,2, \cdots \\
\sum_{i} x_{i}=1
\end{array}$$

$$\pi_{i}=\sum_{j} p_{i j} \pi_{j}, \quad i=1,2, \cdots$$

$$P\left (X_{t}=i\right)=\pi_{i}=\sum_{j} p_{i j} \pi_{j}=\sum_{j} p_{i j} P\left (X_{t-1}=j\right), \quad i=1,2, \cdots$$

$$P (x, A)=\int_{A} p (x, y) \mathrm{d} y$$

$$P\left (X_{t}=A \mid X_{t-1}=x\right)=P (x, A)$$

$$\pi (y)=\int p (x, y) \pi (x) \mathrm{d} x, \quad \forall y \in \mathcal{S}$$

$$\pi (A)=\int P (x, A) \pi (x) \mathrm{d} x, \quad \forall A \subset \mathcal{S}$$

$$\pi=P \pi$$

$$P\left (X_{t}=i \mid X_{0}=j\right)>0$$

$$\lim _{t \rightarrow \infty} P\left (X_{t}=i \mid X_{0}=j\right)=\pi_{i}, \quad i=1,2, \cdots ; \quad j=1,2, \cdots$$

$$P\left\{\hat{f}_{t} \rightarrow E_{\pi}[f (X)]\right\}=1$$

$$\hat{f}_{t}=\frac{1}{t} \sum_{s=1}^{t} f\left (x_{s}\right)$$

$$\hat{f}_{t} \rightarrow E_{\pi}[f (X)], \quad t \rightarrow \infty$$

$$\hat{E} f=\frac{1}{n-m} \sum_{i=m+1}^{n} f\left (x_{i}\right)$$

$$P\left (X_{t}=i \mid X_{t-1}=j\right) \pi_{j}=P\left (X_{t-1}=j \mid X_{t}=i\right) \pi_{i}, \quad i, j=1,2, \cdots$$

$$p_{j i} \pi_{j}=p_{i j} \pi_{i}, \quad i, j=1,2, \cdots$$

$$P \pi=\pi$$

$$(P \pi)_{i}=\sum_{j} p_{i j} \pi_{j}=\sum_{j} p_{j i} \pi_{i}=\pi_{i} \sum_{j} p_{j i}=\pi_{i}, \quad i=1,2, \cdots$$

$$\hat{E} f=\frac{1}{n-m} \sum_{i=m+1}^{n} f\left (x_{i}\right)$$

$$\hat{E} f=\frac{1}{n-m} \sum_{i=m+1}^{n} f\left (x_{i}\right)$$

$$p (x \mid y)=\frac{p (x) p (y \mid x)}{\int_{\mathcal{X}} p\left (y \mid x^{\prime}\right) p\left (x^{\prime}\right) \mathrm{d} x^{\prime}}$$

$$\int_{\mathcal{X}} p\left (y \mid x^{\prime}\right) p\left (x^{\prime}\right) \mathrm{d} x^{\prime}$$

$$p (x \mid y)=\int_{\mathcal{Z}} p (x, z \mid y) \mathrm{d} z$$

$$E_{P (x \mid y)}[f (x)]=\int_{\mathcal{X}} f (x) p (x \mid y) \mathrm{d} x$$

$$p\left (x, x^{\prime}\right)=q\left (x, x^{\prime}\right) \alpha\left (x, x^{\prime}\right)$$

$$\alpha\left (x, x^{\prime}\right)=\min \left\{1, \frac{p\left (x^{\prime}\right) q\left (x^{\prime}, x\right)}{p (x) q\left (x, x^{\prime}\right)}\right\}$$

$$p\left (x, x^{\prime}\right)=\left\{\begin{array}{ll}
q\left (x, x^{\prime}\right), & p\left (x^{\prime}\right) q\left (x^{\prime}, x\right) \geqslant p (x) q\left (x, x^{\prime}\right) \\
q\left (x^{\prime}, x\right) \frac{p\left (x^{\prime}\right)}{p (x)}, & p\left (x^{\prime}\right) q\left (x^{\prime}, x\right)<p (x) q\left (x, x^{\prime}\right)
\end{array}\right.$$

$$x_{t}=\left\{\begin{array}{ll}
x^{\prime}, & u \leqslant \alpha\left (x, x^{\prime}\right) \\
x, & u>\alpha\left (x, x^{\prime}\right)
\end{array}\right.$$

$$p (x) p\left (x, x^{\prime}\right)=p\left (x^{\prime}\right) p\left (x^{\prime}, x\right)$$

$$\begin{aligned}
p (x) p\left (x, x^{\prime}\right) & =p (x) q\left (x, x^{\prime}\right) \min \left\{1, \frac{p\left (x^{\prime}\right) q\left (x^{\prime}, x\right)}{p (x) q\left (x, x^{\prime}\right)}\right\} \\
& =\min \left\{p (x) q\left (x, x^{\prime}\right), p\left (x^{\prime}\right) q\left (x^{\prime}, x\right)\right\} \\
& =p\left (x^{\prime}\right) q\left (x^{\prime}, x\right) \min \left\{\frac{p (x) q\left (x, x^{\prime}\right)}{p\left (x^{\prime}\right) q\left (x^{\prime}, x\right)}, 1\right\} \\
& =p\left (x^{\prime}\right) p\left (x^{\prime}, x\right)
\end{aligned}$$

$$\begin{aligned}
\int p (x) p\left (x, x^{\prime}\right) \mathrm{d} x & =\int p\left (x^{\prime}\right) p\left (x^{\prime}, x\right) \mathrm{d} x \\
& =p\left (x^{\prime}\right) \int p\left (x^{\prime}, x\right) \mathrm{d} x \\
& =p\left (x^{\prime}\right)
\end{aligned}$$

$$q\left (x, x^{\prime}\right)=q\left (x^{\prime}, x\right)$$

$$\alpha\left (x, x^{\prime}\right)=\min \left\{1, \frac{p\left (x^{\prime}\right)}{p (x)}\right\}$$

$$q\left (x, x^{\prime}\right) \propto \exp \left (-\frac{\left (x^{\prime}-x\right)^{2}}{2}\right)$$

$$\alpha\left (x, x^{\prime}\right)=\min \left\{1, \frac{w\left (x^{\prime}\right)}{w (x)}\right\}$$

$$p\left (x_{I} \mid x_{-I}\right)=\frac{p (x)}{\int p (x) \mathrm{d} x_{I}} \propto p (x)$$

$$\frac{p\left (x_{I}^{\prime} \mid x_{-I}^{\prime}\right)}{p\left (x_{I} \mid x_{-I}\right)}=\frac{p\left (x^{\prime}\right)}{p (x)}$$

$$p\left (x_{1}, x_{2}\right) \propto \exp \left\{-\frac{1}{2}\left (x_{1}-1\right)^{2}\left (x_{2}-1\right)^{2}\right\}$$

$$\begin{aligned}
p\left (x_{1} \mid x_{2}\right) & \propto p\left (x_{1}, x_{2}\right) \\
& \propto \exp \left\{-\frac{1}{2}\left (x_{1}-1\right)^{2}\left (x_{2}-1\right)^{2}\right\} \\
& \propto N\left (1,\left (x_{2}-1\right)^{-2}\right)
\end{aligned}$$

$$\alpha\left (x, x^{\prime}\right)=\min \left\{1, \frac{p\left (x^{\prime}\right) q\left (x^{\prime}, x\right)}{p (x) q\left (x, x^{\prime}\right)}\right\}$$

$$f_{m n}=\frac{1}{n-m} \sum_{i=m+1}^{n} f\left (x_{i}\right)$$

$$x=\left (x_{1}, x_{2}, \cdots, x_{k}\right)^{\mathrm{T}}$$

$$x^{(i)}=\left (x_{1}^{(i)}, x_{2}^{(i)}, \cdots, x_{k}^{(i)}\right)^{\mathrm{T}}, \quad i=1,2, \cdots, n$$

$$x_{-j}^{(i)}=\left (x_{1}^{(i)}, \cdots, x_{j-1}^{(i)}, x_{j+1}^{(i-1)}, \cdots, x_{k}^{(i-1)}\right)^{\mathrm{T}}$$

$$\alpha\left (x_{j}^{(i-1)},{x^{\prime}}_{j}^{(i)} \mid x_{-j}^{(i)}\right)=\min \left\{1, \frac{p\left ({x^{\prime}}_{j}^{(i)} \mid x_{-j}^{(i)}\right) q\left ({x^{\prime}}_{j}^{(i)}, x_{j}^{(i-1)} \mid x_{-j}^{(i)}\right)}{p\left (x_{j}^{(i-1)} \mid x_{-j}^{(i)}\right) q\left (x_{j}^{(i-1)},{x^{\prime}}_{j}^{(i)} \mid x_{-j}^{(i)}\right)}\right\}$$

$$p\left (x_{j}^{(i-1)},{x^{\prime}}_{j}^{(i)} \mid x_{-j}^{(i)}\right)=\alpha\left (x_{j}^{(i-1)},{x^{\prime}}_{j}^{(i)} \mid x_{-j}^{(i)}\right) q\left (x_{j}^{(i-1)},{x^{\prime}}_{j}^{(i)} \mid x_{-j}^{(i)}\right)$$

$$p\left (x_{1} \mid x_{2}^{(t-1)}, \cdots, x_{k}^{(t-1)}\right)$$

$$p\left (x_{j} \mid x_{1}^{(i)}, \cdots, x_{j-1}^{(i)}, x_{j+1}^{(i-1)}, \cdots, x_{k}^{(i-1)}\right), \quad j=2, \cdots, k-1$$

$$p\left (x_{k} \mid x_{1}^{(i)}, \cdots, x_{k-1}^{(i)}\right)$$

$$q\left (x, x^{\prime}\right)=p\left (x_{j}^{\prime} \mid x_{-j}\right)$$

$$\begin{aligned}
\alpha\left (x, x^{\prime}\right) & =\min \left\{1, \frac{p\left (x^{\prime}\right) q\left (x^{\prime}, x\right)}{p (x) q\left (x, x^{\prime}\right)}\right\} \\
& =\min \left\{1, \frac{p\left (x^{\prime}{ }_{-j}\right) p\left (x^{\prime}{ }_{j} \mid x^{\prime}{ }_{-j}\right) p\left (x_{j} \mid x^{\prime}{ }_{-j}\right)}{p\left (x_{-j}\right) p\left (x_{j} \mid x_{-j}\right) p\left (x^{\prime}{ }_{j} \mid x_{-j}\right)}\right\}=1
\end{aligned}$$

$$p\left (x, x^{\prime}\right)=p\left (x^{\prime}{ }_{j} \mid x_{-j}\right)$$

$$\left\{x^{(m+1)}, x^{(m+2)}, \cdots, x^{(n)}\right\}$$

$$f_{m n}=\frac{1}{n-m} \sum_{i=m+1}^{n} f\left (x^{(i)}\right)$$

$$p (x \mid y)=p (\alpha, \theta, z \mid y) \propto p (z, y \mid \theta) p (\theta \mid \alpha) p (\alpha)$$

$$p\left (\alpha_{i} \mid \alpha_{-i}, \theta, z, y\right) \propto p (\theta \mid \alpha) p (\alpha)$$

$$p\left (\theta_{j} \mid \theta_{-j}, \alpha, z, y\right) \propto p (z, y \mid \theta) p (\theta \mid \alpha)$$

$$p\left (z_{k} \mid z_{-k}, \alpha, \theta, y\right) \propto p (z, y \mid \theta)$$

$$\hat{f}_{n} \rightarrow E_{p (x)}[f (x)], \quad n \rightarrow \infty$$

$$P\left (X_{t} \mid X_{0} X_{1} \cdots X_{t-1}\right)=P\left (X_{t} \mid X_{t-1}\right), \quad t=1,2, \cdots$$

$$\lim _{t \rightarrow \infty} P\left (X_{t}=i \mid X_{0}=j\right)=\pi_{i}, \quad i=1,2, \cdots ; \quad j=1,2, \cdots$$

$$\hat{f}_{t} \rightarrow E_{\pi}[f (X)], \quad t \rightarrow \infty$$

$$\hat{E} f=\frac{1}{n-m} \sum_{i=m+1}^{n} f\left (x_{i}\right)$$

$$q\left (x, x^{\prime}\right)=p\left (x_{j}^{\prime} \mid x_{-j}\right)$$

# 潜在狄利克雷分配

$$\begin{aligned}
P\left (X_{1}=n_{1}, X_{2}=n_{2}, \cdots, X_{k}=n_{k}\right) & =\frac{n !}{n_{1} ! n_{2} ! \cdots n_{k} !} p_{1}^{n_{1}} p_{2}^{n_{2}} \cdots p_{k}^{n_{k}} \\
& =\frac{n !}{\prod_{i=1}^{k} n_{i} !} \prod_{i=1}^{k} p_{i}^{n_{i}}
\end{aligned}$$

$$p (\theta \mid \alpha)=\frac{\Gamma\left (\sum_{i=1}^{k} \alpha_{i}\right)}{\prod_{i=1}^{k} \Gamma\left (\alpha_{i}\right)} \prod_{i=1}^{k} \theta_{i}^{\alpha_{i}-1}$$

$$\Gamma (s)=\int_{0}^{\infty} x^{s-1} \mathrm{e}^{-x} \mathrm{~d} x, \quad s>0$$

$$\Gamma (s+1)=s \Gamma (s)$$

$$\Gamma (s+1)=s !$$

$$\theta_{i} \geqslant 0, \quad \sum_{i=1}^{k} \theta_{i}=1$$

$$\mathrm{B}(\alpha)=\frac{\prod_{i=1}^{k} \Gamma\left (\alpha_{i}\right)}{\Gamma\left (\sum_{i=1}^{k} \alpha_{i}\right)}$$

$$p (\theta \mid \alpha)=\frac{1}{\mathrm{~B}(\alpha)} \prod_{i=1}^{k} \theta_{i}^{\alpha_{i}-1}$$

$$\int \frac{\Gamma\left (\sum_{i=1}^{k} \alpha_{i}\right)}{\prod_{i=1}^{k} \Gamma\left (\alpha_{i}\right)} \prod_{i=1}^{k} \theta_{i}^{\alpha_{i}-1} \mathrm{~d} \theta=\frac{\Gamma\left (\sum_{i=1}^{k} \alpha_{i}\right)}{\prod_{i=1}^{k} \Gamma\left (\alpha_{i}\right)} \int \prod_{i=1}^{k} \theta_{i}^{\alpha_{i}-1} \mathrm{~d} \theta=1$$

$$\mathrm{B}(\alpha)=\int \prod_{i=1}^{k} \theta_{i}^{\alpha_{i}-1} \mathrm{~d} \theta$$

$$P (X=m)=\left (\begin{array}{c}
n \\
m
\end{array}\right) p^{m}(1-p)^{n-m}, \quad m=0,1,2, \cdots, n$$

$$p (x)=\left\{\begin{array}{ll}
\frac{1}{\mathrm{~B}(s, t)} x^{s-1}(1-x)^{t-1}, & 0 \leqslant x \leqslant 1 \\
0, & \text { 其他 }
\end{array}\right.$$

$$\mathrm{B}(s, t)=\int_{0}^{1} x^{s-1}(1-x)^{t-1} \mathrm{~d} x$$

$$\mathrm{B}(s, t)=\frac{(s-1) !(t-1) !}{(s+t-1) !}$$

$$p (D \mid \theta)=\theta_{1}^{n_{1}} \theta_{2}^{n_{2}} \cdots \theta_{k}^{n_{k}}=\prod_{i=1}^{k} \theta_{i}^{n_{i}}$$

$$p (\theta \mid \alpha)=\frac{\Gamma\left (\sum_{i=1}^{k} \alpha_{i}\right)}{\prod_{i=1}^{k} \Gamma\left (\alpha_{i}\right)} \prod_{i=1}^{k} \theta_{i}^{\alpha_{i}-1}=\frac{1}{\mathrm{~B}(\alpha)} \prod_{i=1}^{k} \theta_{i}^{\alpha_{i}-1}=\operatorname{Dir}(\theta \mid \alpha), \quad \alpha_{i}>0$$

$$\begin{aligned}
p (\theta \mid D, \alpha) & =\frac{p (D \mid \theta) p (\theta \mid \alpha)}{p (D \mid \alpha)} \\
& =\frac{\prod_{i=1}^{k} \theta_{i}^{n_{i}} \frac{1}{\mathrm{~B}(\alpha)} \theta_{i}^{\alpha_{i}-1}}{\int \prod_{i=1}^{k} \theta_{i}^{n_{i}} \frac{1}{\mathrm{~B}(\alpha)} \theta_{i}^{\alpha_{i}-1} \mathrm{~d} \theta} \\
& =\frac{1}{\mathrm{~B}(\alpha+n)} \prod_{i=1}^{k} \theta_{i}^{\alpha_{i}+n_{i}-1} \\
& =\operatorname{Dir}(\theta \mid \alpha+n)
\end{aligned}$$

$$P\left (x_{1}, x_{2}, \cdots, x_{N}\right)=P\left (x_{\pi (1)}, x_{\pi (2)}, \cdots, x_{\pi (N)}\right)$$

$$P\left (X_{1}, X_{2}, \cdots, X_{i}, \cdots \mid Y\right)=P\left (X_{1} \mid Y\right) P\left (X_{2} \mid Y\right) \cdots P\left (X_{i} \mid Y\right) \cdots$$

$$p (\mathbf{w}, \mathbf{z}, \theta, \varphi \mid \alpha, \beta)=\prod_{k=1}^{K} p\left (\varphi_{k} \mid \beta\right) \prod_{m=1}^{M} p\left (\theta_{m} \mid \alpha\right) \prod_{n=1}^{N_{m}} p\left (z_{m n} \mid \theta_{m}\right) p\left (w_{m n} \mid z_{m n}, \varphi\right)$$

$$p\left (\mathbf{w}_{m}, \mathbf{z}_{m}, \theta_{m}, \varphi \mid \alpha, \beta\right)=\prod_{k=1}^{K} p\left (\varphi_{k} \mid \beta\right) p\left (\theta_{m} \mid \alpha\right) \prod_{n=1}^{N_{m}} p\left (z_{m n} \mid \theta_{m}\right) p\left (w_{m n} \mid z_{m n}, \varphi\right)$$

$$p\left (\mathbf{w}_{m} \mid \theta_{m}, \varphi\right)=\prod_{n=1}^{N_{m}}\left[\sum_{k=1}^{K} p\left (z_{m n}=k \mid \theta_{m}\right) p\left (w_{m n} \mid \varphi_{k}\right)\right]$$

$$p\left (\mathbf{w}_{m} \mid \alpha, \beta\right)=\prod_{k=1}^{K} \int p\left (\varphi_{k} \mid \beta\right)\left[\int p\left (\theta_{m} \mid \alpha\right) \prod_{n=1}^{N_{m}}\left[\sum_{l=1}^{K} p\left (z_{m n}=l \mid \theta_{m}\right) p\left (w_{m n} \mid \varphi_{l}\right)\right] \mathrm{d} \theta_{m}\right] \mathrm{d} \varphi_{k}$$

$$p (\mathbf{w} \mid \alpha, \beta)=\prod_{k=1}^{K} \int p\left (\varphi_{k} \mid \beta\right)\left[\prod_{m=1}^{M} \int p\left (\theta_{m} \mid \alpha\right) \prod_{n=1}^{N_{m}}\left[\sum_{l=1}^{K} p\left (z_{m n}=l \mid \theta_{m}\right) p\left (w_{m n} \mid \varphi_{l}\right)\right] \mathrm{d} \theta_{m}\right] \mathrm{d} \varphi_{k}$$

$$p (\mathbf{z} \mid \mathbf{w}, \alpha, \beta)=\frac{p (\mathbf{w}, \mathbf{z} \mid \alpha, \beta)}{p (\mathbf{w} \mid \alpha, \beta)} \propto p (\mathbf{w}, \mathbf{z} \mid \alpha, \beta)$$

$$p (\mathbf{w}, \mathbf{z} \mid \alpha, \beta)=p (\mathbf{w} \mid \mathbf{z}, \alpha, \beta) p (\mathbf{z} \mid \alpha, \beta)=p (\mathbf{w} \mid \mathbf{z}, \beta) p (\mathbf{z} \mid \alpha)$$

$$p (\mathbf{w} \mid \mathbf{z}, \varphi)=\prod_{k=1}^{K} \prod_{v=1}^{V} \varphi_{k v}^{n_{k v}}$$

$$\begin{aligned}
p (\mathbf{w} \mid \mathbf{z}, \beta) & =\int p (\mathbf{w} \mid \mathbf{z}, \varphi) p (\varphi \mid \beta) \mathrm{d} \varphi \\
& =\int \prod_{k=1}^{K} \frac{1}{\mathrm{~B}(\beta)} \prod_{v=1}^{V} \varphi_{k v}^{n_{k v}+\beta_{v}-1} \mathrm{~d} \varphi \\
& =\prod_{k=1}^{K} \frac{1}{\mathrm{~B}(\beta)} \int \prod_{v=1}^{V} \varphi_{k v}^{n_{k v}+\beta_{v}-1} \mathrm{~d} \varphi \\
& =\prod_{k=1}^{K} \frac{\mathrm{B}\left (n_{k}+\beta\right)}{\mathrm{B}(\beta)}
\end{aligned}$$

$$p (\mathbf{z} \mid \theta)=\prod_{m=1}^{M} \prod_{k=1}^{K} \theta_{m k}^{n_{m k}}$$

$$\begin{aligned}
p (\mathbf{z} \mid \alpha) & =\int p (\mathbf{z} \mid \theta) p (\theta \mid \alpha) \mathrm{d} \theta \\
& =\int \prod_{m=1}^{M} \frac{1}{\mathrm{~B}(\alpha)} \prod_{k=1}^{K} \theta_{m k}^{n_{m k}+\alpha_{k}-1} \mathrm{~d} \theta \\
& =\prod_{m=1}^{M} \frac{1}{\mathrm{~B}(\alpha)} \int \prod_{k=1}^{K} \theta_{m k}^{n_{m k}+\alpha_{k}-1} \mathrm{~d} \theta \\
& =\prod_{m=1}^{M} \frac{\mathrm{B}\left (n_{m}+\alpha\right)}{\mathrm{B}(\alpha)}
\end{aligned}$$

$$p (\mathbf{z}, \mathbf{w} \mid \alpha, \beta)=\prod_{k=1}^{K} \frac{\mathrm{B}\left (n_{k}+\beta\right)}{\mathrm{B}(\beta)} \cdot \prod_{m=1}^{M} \frac{\mathrm{B}\left (n_{m}+\alpha\right)}{\mathrm{B}(\alpha)}$$

$$p (\mathbf{z} \mid \mathbf{w}, \alpha, \beta) \propto \prod_{k=1}^{K} \frac{\mathrm{B}\left (n_{k}+\beta\right)}{\mathrm{B}(\beta)} \cdot \prod_{m=1}^{M} \frac{\mathrm{B}\left (n_{m}+\alpha\right)}{\mathrm{B}(\alpha)}$$

$$p\left (z_{i} \mid \mathbf{z}_{-i}, \mathbf{w}, \alpha, \beta\right)=\frac{1}{Z_{z_{i}}} p (\mathbf{z} \mid \mathbf{w}, \alpha, \beta)$$

$$p\left (z_{i} \mid \mathbf{z}_{-i}, \mathbf{w}, \alpha, \beta\right) \propto \frac{n_{k v}+\beta_{v}}{\sum_{v=1}^{V}\left (n_{k v}+\beta_{v}\right)} \cdot \frac{n_{m k}+\alpha_{k}}{\sum_{k=1}^{K}\left (n_{m k}+\alpha_{k}\right)}$$

$$p\left (\theta_{m} \mid \mathbf{z}_{m}, \alpha\right)=\frac{1}{Z_{\theta_{m}}} \prod_{n=1}^{N_{m}} p\left (z_{m n} \mid \theta_{m}\right) p\left (\theta_{m} \mid \alpha\right)=\operatorname{Dir}\left (\theta_{m} \mid n_{m}+\alpha\right)$$

$$\theta_{m k}=\frac{n_{m k}+\alpha_{k}}{\sum_{k=1}^{K}\left (n_{m k}+\alpha_{k}\right)}, \quad m=1,2, \cdots, M ; \quad k=1,2, \cdots, K$$

$$p\left (\varphi_{k} \mid \mathbf{w}, \mathbf{z}, \beta\right)=\frac{1}{Z_{\varphi_{k}}} \prod_{i=1}^{I} p\left (w_{i} \mid \varphi_{k}\right) p\left (\varphi_{k} \mid \beta\right)=\operatorname{Dir}\left (\varphi_{k} \mid n_{k}+\beta\right)$$

$$\varphi_{k v}=\frac{n_{k v}+\beta_{v}}{\sum_{v=1}^{V}\left (n_{k v}+\beta_{v}\right)}, \quad k=1,2, \cdots, K ; \quad v=1,2, \cdots, V$$

$$p\left (z_{i} \mid \mathbf{z}_{-i}, \mathbf{w}, \alpha, \beta\right) \propto \frac{n_{k v}+\beta_{v}}{\sum_{v=1}^{V}\left (n_{k v}+\beta_{v}\right)} \cdot \frac{n_{m k}+\alpha_{k}}{\sum_{k=1}^{K}\left (n_{m k}+\alpha_{k}\right)}$$

$$p\left (z_{i} \mid \mathbf{z}_{-i}, \mathbf{w}, \alpha, \beta\right) \propto \frac{n_{k v}+\beta_{v}}{\sum_{v=1}^{V}\left (n_{k v}+\beta_{v}\right)} \cdot \frac{n_{m k}+\alpha_{k}}{\sum_{k=1}^{K}\left (n_{m k}+\alpha_{k}\right)}$$

$$\theta_{m k} =\frac{n_{m k}+\alpha_{k}}{\sum_{k=1}^{K}\left (n_{m k}+\alpha_{k}\right)}$$

$$\varphi_{k v} =\frac{n_{k v}+\beta_{v}}{\sum_{v=1}^{V}\left (n_{k v}+\beta_{v}\right)}$$

$$p (z \mid x) \approx q^{*}(z)$$

$$\begin{aligned}
D (q (z) \| p (z \mid x)) & =E_{q}[\log q (z)]-E_{q}[\log p (z \mid x)] \\
& =E_{q}[\log q (z)]-E_{q}[\log p (x, z)]+\log p (x) \\
& =\log p (x)-\left\{E_{q}[\log p (x, z)]-E_{q}[\log q (z)]\right\}
\end{aligned}$$

$$\log p (x) \geqslant E_{q}[\log p (x, z)]-E_{q}[\log q (z)]$$

$$L (q)=E_{q}[\log p (x, z)]-E_{q}[\log q (z)]$$

$$q (z)=q\left (z_{1}\right) q\left (z_{2}\right) \cdots q\left (z_{n}\right)$$

$$L (q, \theta)=E_{q}[\log p (x, z \mid \theta)]-E_{q}[\log q (z)]$$

$$\log p (x \mid \theta)-L (q, \theta)=D (q (z) \| p (z \mid x, \theta)) \geqslant 0$$

$$\log p\left (x \mid \theta^{(t-1)}\right)=L\left (q^{(t)}, \theta^{(t-1)}\right) \leqslant L\left (q^{(t)}, \theta^{(t)}\right) \leqslant \log p\left (x \mid \theta^{(t)}\right)$$

$$p (\theta, \mathbf{z}, \mathbf{w} \mid \alpha, \varphi)=p (\theta \mid \alpha) \prod_{n=1}^{N} p\left (z_{n} \mid \theta\right) p\left (w_{n} \mid z_{n}, \varphi\right)$$

$$q (\theta, \mathbf{z} \mid \gamma, \eta)=q (\theta \mid \gamma) \prod_{n=1}^{N} q\left (z_{n} \mid \eta_{n}\right)$$

$$L (\gamma, \eta, \alpha, \varphi)=E_{q}[\log p (\theta, \mathbf{z}, \mathbf{w} \mid \alpha, \varphi)]-E_{q}[\log q (\theta, \mathbf{z} \mid \gamma, \eta)]$$

$$L_{\mathbf{w}}(\gamma, \eta, \alpha, \varphi)=\sum_{m=1}^{M}\left\{E_{q_{m}}\left[\log p\left (\theta_{m}, \mathbf{z}_{m}, \mathbf{w}_{m} \mid \alpha, \varphi\right)\right]-E_{q_{m}}\left[\log q\left (\theta_{m}, \mathbf{z}_{m} \mid \gamma_{m}, \eta_{m}\right)\right]\right\}$$

$$\begin{aligned}
L (\gamma, \eta, \alpha, \varphi)= & E_{q}[\log p (\theta \mid \alpha)]+E_{q}[\log p (\mathbf{z} \mid \theta)]+E_{q}[\log p (\mathbf{w} \mid \mathbf{z}, \varphi)]- \\
& E_{q}[\log q (\theta \mid \gamma)]-E_{q}[\log q (\mathbf{z} \mid \eta)]
\end{aligned}$$

$$\begin{aligned}
L (\gamma, \eta, \alpha, \varphi)= & \log \Gamma\left (\sum_{l=1}^{K} \alpha_{l}\right)-\sum_{k=1}^{K} \log \Gamma\left (\alpha_{k}\right)+\sum_{k=1}^{K}\left (\alpha_{k}-1\right)\left[\Psi\left (\gamma_{k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{l}\right)\right]+ \\
& \sum_{n=1}^{N} \sum_{k=1}^{K} \eta_{n k}\left[\Psi\left (\gamma_{k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{l}\right)\right]+ \\
& \sum_{n=1}^{N} \sum_{k=1}^{K} \sum_{v=1}^{V} \eta_{n k} w_{n}^{v} \log \varphi_{k v}- \\
& \log \Gamma\left (\sum_{l=1}^{K} \gamma_{l}\right)+\sum_{k=1}^{K} \log \Gamma\left (\gamma_{k}\right)-\sum_{k=1}^{K}\left (\gamma_{k}-1\right)\left[\Psi\left (\gamma_{k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{l}\right)\right]- \\
& \sum_{n=1}^{N} \sum_{k=1}^{K} \eta_{n k} \log \eta_{n k}
\end{aligned}$$

$$\Psi\left (\alpha_{k}\right)=\frac{\mathrm{d}}{\mathrm{d} \alpha_{k}} \log \Gamma\left (\alpha_{k}\right)$$

$$E_{q}[\log p (\theta \mid \alpha)]=\sum_{k=1}^{K}\left (\alpha_{k}-1\right) E_{q}\left[\log \theta_{k}\right]+\log \Gamma\left (\sum_{l=1}^{K} \alpha_{l}\right)-\sum_{k=1}^{K} \log \Gamma\left (\alpha_{k}\right)$$

$$E_{q (\theta \mid \gamma)}\left[\log \theta_{k}\right]=\Psi\left (\gamma_{k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{l}\right)$$

$$E_{q}[\log p (\theta \mid \alpha)]=\log \Gamma\left (\sum_{l=1}^{K} \alpha_{l}\right)-\sum_{k=1}^{K} \log \Gamma\left (\alpha_{k}\right)+\sum_{k=1}^{K}\left (\alpha_{k}-1\right)\left[\Psi\left (\gamma_{k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{l}\right)\right]$$

$$\begin{aligned}
E_{q}(\log p (\mathbf{z} \mid \theta)) & =\sum_{n=1}^{N} E_{q}\left[\log p\left (z_{n} \mid \theta\right)\right] \\
& =\sum_{n=1}^{N} E_{q\left (\theta, z_{n} \mid \gamma, \eta\right)}\left[\log \left (z_{n} \mid \theta\right)\right] \\
& =\sum_{n=1}^{N} \sum_{k=1}^{K} q\left (z_{n k} \mid \eta\right) E_{q (\theta \mid \gamma)}\left[\log \theta_{k}\right] \\
& =\sum_{n=1}^{N} \sum_{k=1}^{K} \eta_{n k}\left[\Psi\left (\gamma_{k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{l}\right)\right]
\end{aligned}$$

$$\begin{aligned}
E_{q}[\log p (\mathbf{w} \mid \mathbf{z}, \varphi)] & =\sum_{n=1}^{N} E_{q}\left[\log p\left (w_{n} \mid z_{n}, \varphi\right)\right] \\
& =\sum_{n=1}^{N} E_{q\left (z_{n} \mid \eta\right)}\left[\log p\left (w_{n} \mid z_{n}, \varphi\right)\right] \\
& =\sum_{n=1}^{N} \sum_{k=1}^{K} q\left (z_{n k} \mid \eta\right) \log p\left (w_{n} \mid z_{n k}, \varphi\right) \\
& =\sum_{n=1}^{N} \sum_{k=1}^{K} \sum_{v=1}^{V} \eta_{n k} w_{n}^{v} \log \varphi_{k v}
\end{aligned}$$

$$E_{q}[\log q (\theta \mid \gamma)]=\log \Gamma\left (\sum_{l=1}^{K} \gamma_{l}\right)-\sum_{k=1}^{K} \log \Gamma\left (\gamma_{k}\right)+\sum_{k=1}^{K}\left (\gamma_{k}-1\right)\left[\Psi\left (\gamma_{k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{l}\right)\right]$$

$$\begin{aligned}
E_{q}[\log q (\mathbf{z} \mid \eta)] & =\sum_{n=1}^{N} E_{q}\left[\log q\left (z_{n} \mid \eta\right)\right] \\
& =\sum_{n=1}^{N} E_{q\left (z_{n} \mid \eta\right)}\left[\log q\left (z_{n} \mid \eta\right)\right] \\
& =\sum_{n=1}^{N} \sum_{k=1}^{K} q\left (z_{n k} \mid \eta\right) \log q\left (z_{n k} \mid \eta\right) \\
& =\sum_{n=1}^{N} \sum_{k=1}^{K} \eta_{n k} \log \eta_{n k}
\end{aligned}$$

$$L_{\left[\eta_{n k}\right]}=\eta_{n k}\left[\Psi\left (\gamma_{k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{l}\right)\right]+\eta_{n k} \log \varphi_{k v}-\eta_{n k} \log \eta_{n k}+\lambda_{n}\left (\sum_{l=1}^{K} \eta_{n l}-1\right)$$

$$\frac{\partial L}{\partial \eta_{n k}}=\Psi\left (\gamma_{k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{l}\right)+\log \varphi_{k v}-\log \eta_{n k}-1+\lambda_{n}$$

$$\eta_{n k} \propto \varphi_{k v} \exp \left (\Psi\left (\gamma_{k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{l}\right)\right)$$

$$\begin{aligned}
L_{\left[\gamma_{k}\right]}= & \sum_{k=1}^{K}\left (\alpha_{k}-1\right)\left[\Psi\left (\gamma_{k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{l}\right)\right]+\sum_{n=1}^{N} \sum_{k=1}^{K} \eta_{n k}\left[\Psi\left (\gamma_{k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{l}\right)\right]- \\
& \log \Gamma\left (\sum_{l=1}^{K} \gamma_{l}\right)+\log \Gamma\left (\gamma_{k}\right)-\sum_{k=1}^{K}\left (\gamma_{k}-1\right)\left[\Psi\left (\gamma_{k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{l}\right)\right]
\end{aligned}$$

$$L_{\left[\gamma_{k}\right]}=\sum_{k=1}^{K}\left[\Psi\left (\gamma_{k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{l}\right)\right]\left (\alpha_{k}+\sum_{n=1}^{N} \eta_{n k}-\gamma_{k}\right)-\log \Gamma\left (\sum_{l=1}^{K} \gamma_{l}\right)+\log \Gamma\left (\gamma_{k}\right)$$

$$\frac{\partial L}{\partial \gamma_{k}}=\left[\Psi^{\prime}\left (\gamma_{k}\right)-\Psi^{\prime}\left (\sum_{l=1}^{K} \gamma_{l}\right)\right]\left (\alpha_{k}+\sum_{n=1}^{N} \eta_{n k}-\gamma_{k}\right)$$

$$\gamma_{k}=\alpha_{k}+\sum_{n=1}^{N} \eta_{n k}$$

$$\eta_{n k}^{(t+1)}=\varphi_{k v} \exp \left[\Psi\left (\gamma_{k}^{(t)}\right)-\Psi\left (\sum_{l=}^{K} \gamma_{l}^{(t)}\right)\right]$$

$$\gamma^{(t+1)}=\alpha+\sum_{n=1}^{N} \eta_{n}^{(t+1)}$$

$$\sum_{v=1}^{V} \varphi_{k v}=1, \quad k=1,2, \cdots, K$$

$$L_{[\beta]}=\sum_{m=1}^{M} \sum_{n=1}^{N_{m}} \sum_{k=1}^{K} \sum_{v=1}^{V} \eta_{m n k} w_{m n}^{v} \log \varphi_{k v}+\sum_{k=1}^{K} \lambda_{k}\left (\sum_{v=1}^{V} \varphi_{k v}-1\right)$$

$$\varphi_{k v}=\sum_{m=1}^{M} \sum_{n=1}^{N_{m}} \eta_{m n k} w_{m n}^{v}$$

$$L_{[\alpha]}=\sum_{m=1}^{M}\left\{\log \Gamma\left (\sum_{l=1}^{K} \alpha_{l}\right)-\sum_{k=1}^{K} \log \Gamma\left (\alpha_{k}\right)+\sum_{k=1}^{K}\left (\alpha_{k}-1\right)\left[\Psi\left (\gamma_{m k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{m l}\right)\right]\right\}$$

$$\frac{\partial L}{\partial \alpha_{k}}=M\left[\Psi\left (\sum_{l=1}^{K} \alpha_{l}\right)-\Psi\left (\alpha_{k}\right)\right]+\sum_{m=1}^{M}\left[\Psi\left (\gamma_{m k}\right)-\Psi\left (\sum_{l=1}^{K} \gamma_{m l}\right)\right]$$

$$\frac{\partial^{2} L}{\partial \alpha_{k} \partial \alpha_{l}}=M\left[\Psi^{\prime}\left (\sum_{l=1}^{K} \alpha_{l}\right)-\delta (k, l) \Psi^{\prime}\left (\alpha_{k}\right)\right]$$

$$\alpha_{\text {new }}=\alpha_{\text {old }}-H\left (\alpha_{\text {old }}\right)^{-1} g\left (\alpha_{\text {old }}\right)$$

$$p (\theta \mid \alpha)=\frac{\Gamma\left (\sum_{i=1}^{k} \alpha_{i}\right)}{\prod_{i=1}^{k} \Gamma\left (\alpha_{i}\right)} \prod_{i=1}^{k} \theta_{i}^{\alpha_{i}-1}$$

$$L (q, \theta)=E_{q}[\log p (x, z \mid \theta)]-E_{q}[\log q (z)]$$

# PageRank 算法

$$M=\left[m_{i j}\right]_{n \times n}$$

$$m_{i j} \geqslant 0$$

$$\sum_{i=1}^{n} m_{i j}=1$$

$$R_{t+1}=M R_{t}$$

$$R_{0}, M R_{0}, M^{2} R_{0}, \cdots, M^{t} R_{0}, \cdots$$

$$\lim _{t \rightarrow \infty} M^{t} R_{0}=R$$

$$M R=R$$

$$M R=R$$

$$R=\left[\begin{array}{c}
P R\left (v_{1}\right) \\
P R\left (v_{2}\right) \\
\vdots \\
P R\left (v_{n}\right)
\end{array}\right]$$

$$P R\left (v_{i}\right) \geqslant 0, \quad i=1,2, \cdots, n$$

$$\sum_{i=1}^{n} P R\left (v_{i}\right)=1$$

$$P R\left (v_{i}\right)=\sum_{v_{j} \in M\left (v_{i}\right)} \frac{P R\left (v_{j}\right)}{L\left (v_{j}\right)}, \quad i=1,2, \cdots, n$$

$$R=d M R+\frac{1-d}{n} \mathbf{1}$$

$$R=\left[\begin{array}{c}
P R\left (v_{1}\right) \\
P R\left (v_{2}\right) \\
\vdots \\
P R\left (v_{n}\right)
\end{array}\right]$$

$$P R\left (v_{i}\right)=d\left (\sum_{v_{j} \in M\left (v_{i}\right)} \frac{P R\left (v_{j}\right)}{L\left (v_{j}\right)}\right)+\frac{1-d}{n}, \quad i=1,2, \cdots, n$$

$$\begin{array}{c}
P R\left (v_{i}\right)>0, \quad i=1,2, \cdots, n \\
\sum_{i=1}^{n} P R\left (v_{i}\right)=1
\end{array}$$

$$R=d M R+\frac{1-d}{n} \mathbf{1}$$

$$R_{t+1}=d M R_{t}+\frac{1-d}{n} \mathbf{1}$$

$$R_{t+1}=d M R_{t}+\frac{1-d}{n} \mathbf{1}$$

$$x_{0}, \quad x_{1}=A x_{0}, \quad x_{2}=A x_{1}, \quad \cdots, \quad x_{k}=A x_{k-1}$$

$$\left|\lambda_{1}\right| \geqslant\left|\lambda_{2}\right| \geqslant \cdots \geqslant\left|\lambda_{n}\right|$$

$$u_{1}, u_{2}, \cdots, u_{n}$$

$$x_{0}=a_{1} u_{1}+a_{2} u_{2}+\cdots+a_{n} u_{n}$$

$$\begin{aligned}
x_{1} & =A x_{0}=a_{1} A u_{1}+a_{2} A u_{2}+\cdots+a_{n} A u_{n} \\
& \vdots \\
x_{k} & =A^{k} x_{0}=a_{1} A^{k} u_{1}+a_{2} A^{k} u_{2}+\cdots+a_{n} A^{k} u_{n} \\
& =a_{1} \lambda_{1}^{k} u_{1}+a_{2} \lambda_{2}^{k} u_{2}+\cdots+a_{n} \lambda_{n}^{k} u_{n}
\end{aligned}$$

$$x_{k}=a_{1} \lambda_{1}^{k}\left[u_{1}+\frac{a_{2}}{a_{1}}\left (\frac{\lambda_{2}}{\lambda_{1}}\right)^{k} u_{2}+\cdots+\frac{a_{n}}{a_{1}}\left (\frac{\lambda_{n}}{\lambda_{1}}\right)^{k} u_{n}\right]$$

$$x_{k}=a_{1} \lambda_{1}^{k}\left[u_{1}+\varepsilon_{k}\right]$$

$$x_{k} \rightarrow a_{1} \lambda_{1}^{k} u_{1}(k \rightarrow \infty)$$

$$\begin{array}{l}
x_{k} \approx a_{1} \lambda_{1}^{k} u_{1} \\
x_{k+1} \approx a_{1} \lambda_{1}^{k+1} u_{1}
\end{array}$$

$$\lambda_{1} \approx \frac{x_{k+1, j}}{x_{k, j}}$$

$$\begin{aligned}
y_{t+1} & =A x_{t} \\
x_{t+1} & =\frac{y_{t+1}}{\left\|y_{t+1}\right\|}
\end{aligned}$$

$$\|x\|_{\infty}=\max \left\{\left|x_{1}\right|,\left|x_{2}\right|, \cdots,\left|x_{n}\right|\right\}$$

$$R=\left (d M+\frac{1-d}{n} \mathbf{E}\right) R=A R$$

$$A=d M+\frac{1-d}{n} \mathbf{E}$$

$$\begin{aligned}
y_{t+1} & =A x_{t} \\
x_{t+1} & =\frac{y_{t+1}}{\left\|y_{t+1}\right\|}
\end{aligned}$$

$$R=d M R+\frac{1-d}{n} \mathbf{1}$$

$$(I-d M) R=\frac{1-d}{n} \mathbf{1}$$

$$R=(I-d M)^{-1} \frac{1-d}{n} \mathbf{1}$$

$$M=\left[m_{i j}\right]_{n \times n}$$

$$M R=R$$

$$R=\left[\begin{array}{c}
P R\left (v_{1}\right) \\
P R\left (v_{2}\right) \\
\vdots \\
P R\left (v_{n}\right)
\end{array}\right]$$

$$R=d M R+\frac{1-d}{n} \mathbf{1}$$

$$R=\left (d M+\frac{1-d}{n} \mathbf{E}\right) R=A R$$

$$\begin{aligned}
y_{t+1} & =A x_{t} \\
x_{t+1} & =\frac{y_{t+1}}{\left\|y_{t+1}\right\|}
\end{aligned}$$

# 无监督学习方法总结

$$
\min _{U, V} B (D \| U V)
$$

# 附录

## 梯度下降法

$$\min _{x \in \mathbf{R}^{n}} f (x)$$

$$f (x)=f\left (x^{(k)}\right)+g_{k}^{\mathrm{T}}\left (x-x^{(k)}\right)$$

$$x^{(k+1)} \leftarrow x^{(k)}+\lambda_{k} p_{k}$$

$$f\left (x^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geqslant 0} f\left (x^{(k)}+\lambda p_{k}\right)$$

$$f\left (x^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geqslant 0} f\left (x^{(k)}+\lambda p_{k}\right)$$

## 牛顿法与拟牛顿法

$$\min _{x \in \mathbf{R}^{n}} f (x)$$

$$f (x)=f\left (x^{(k)}\right)+g_{k}^{\mathrm{T}}\left (x-x^{(k)}\right)+\frac{1}{2}\left (x-x^{(k)}\right)^{\mathrm{T}} H\left (x^{(k)}\right)\left (x-x^{(k)}\right)$$

$$H (x)=\left[\frac{\partial^{2} f}{\partial x_{i} \partial x_{j}}\right]_{n \times n}$$

$$\nabla f (x)=0$$

$$\nabla f\left (x^{(k+1)}\right)=0$$

$$\nabla f (x)=g_{k}+H_{k}\left (x-x^{(k)}\right)$$

$$g_{k}+H_{k}\left (x^{(k+1)}-x^{(k)}\right)=0$$

$$x^{(k+1)}=x^{(k)}-H_{k}^{-1} g_{k}$$

$$x^{(k+1)}=x^{(k)}+p_{k}$$

$$H_{k} p_{k}=-g_{k}$$

$$H_{k} p_{k}=-g_{k}$$

$$g_{k+1}-g_{k}=H_{k}\left (x^{(k+1)}-x^{(k)}\right)$$

$$y_{k}=H_{k} \delta_{k}$$

$$H_{k}^{-1} y_{k}=\delta_{k}$$

$$x=x^{(k)}+\lambda p_{k}=x^{(k)}-\lambda H_{k}^{-1} g_{k}$$

$$f (x)=f\left (x^{(k)}\right)-\lambda g_{k}^{\mathrm{T}} H_{k}^{-1} g_{k}$$

$$G_{k+1} y_{k}=\delta_{k}$$

$$G_{k+1}=G_{k}+\Delta G_{k}$$

$$G_{k+1}=G_{k}+P_{k}+Q_{k}$$

$$G_{k+1} y_{k}=G_{k} y_{k}+P_{k} y_{k}+Q_{k} y_{k}$$

$$\begin{array}{c}
P_{k} y_{k}=\delta_{k} \\
Q_{k} y_{k}=-G_{k} y_{k}
\end{array}$$

$$\begin{array}{c}
P_{k}=\frac{\delta_{k} \delta_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}} \\
Q_{k}=-\frac{G_{k} y_{k} y_{k}^{\mathrm{T}} G_{k}}{y_{k}^{\mathrm{T}} G_{k} y_{k}}
\end{array}$$

$$G_{k+1}=G_{k}+\frac{\delta_{k} \delta_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}}-\frac{G_{k} y_{k} y_{k}^{\mathrm{T}} G_{k}}{y_{k}^{\mathrm{T}} G_{k} y_{k}}$$

$$f\left (x^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geqslant 0} f\left (x^{(k)}+\lambda p_{k}\right)$$

$$B_{k+1} \delta_{k}=y_{k}$$

$$\begin{array}{c}
B_{k+1}=B_{k}+P_{k}+Q_{k} \\
B_{k+1} \delta_{k}=B_{k} \delta_{k}+P_{k} \delta_{k}+Q_{k} \delta_{k}
\end{array}$$

$$\begin{array}{c}
P_{k} \delta_{k}=y_{k} \\
Q_{k} \delta_{k}=-B_{k} \delta_{k}
\end{array}$$

$$B_{k+1}=B_{k}+\frac{y_{k} y_{k}^{\mathrm{T}}}{y_{k}^{\mathrm{T}} \delta_{k}}-\frac{B_{k} \delta_{k} \delta_{k}^{\mathrm{T}} B_{k}}{\delta_{k}^{\mathrm{T}} B_{k} \delta_{k}}$$

$$f\left (x^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geqslant 0} f\left (x^{(k)}+\lambda p_{k}\right)$$

$$G_{k+1}=\left (I-\frac{\delta_{k} y_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}}\right) G_{k}\left (I-\frac{\delta_{k} y_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}}\right)^{\mathrm{T}}+\frac{\delta_{k} \delta_{k}^{\mathrm{T}}}{\delta_{k}^{\mathrm{T}} y_{k}}$$

$$G_{k+1}=\alpha G^{\mathrm{DFP}}+(1-\alpha) G^{\mathrm{BFGS}}$$

## 拉格朗日对偶性

$$\begin{array}{ll}
\min _{x \in \mathbf{R}^{n}} & f (x) \\
\text { s.t. } & c_{i}(x) \leqslant 0, \quad i=1,2, \cdots, k \\
& h_{j}(x)=0, \quad j=1,2, \cdots, l
\end{array}$$

$$L (x, \alpha, \beta)=f (x)+\sum_{i=1}^{k} \alpha_{i} c_{i}(x)+\sum_{j=1}^{l} \beta_{j} h_{j}(x)$$

$$\theta_{P}(x)=\max _{\alpha, \beta: \alpha_{i} \geqslant 0} L (x, \alpha, \beta)$$

$$\theta_{P}(x)=\max _{\alpha, \beta: \alpha_{i} \geqslant 0}\left[f (x)+\sum_{i=1}^{k} \alpha_{i} c_{i}(x)+\sum_{j=1}^{l} \beta_{j} h_{j}(x)\right]=+\infty$$

$$\theta_{P}(x)=\left\{\begin{array}{ll}
f (x), & x \text { 满足原始问题约束 } \\
+\infty, & \text { 其他 }
\end{array}\right.$$

$$\min _{x} \theta_{P}(x)=\min _{x} \max _{\alpha, \beta: \alpha_{i} \geqslant 0} L (x, \alpha, \beta)$$

$$p^{*}=\min _{x} \theta_{P}(x)$$

$$\theta_{D}(\alpha, \beta)=\min _{x} L (x, \alpha, \beta)$$

$$\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \theta_{D}(\alpha, \beta)=\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \min _{x} L (x, \alpha, \beta)$$

$$\begin{array}{c}
\max _{\alpha, \beta} \theta_{D}(\alpha, \beta)=\max _{\alpha, \beta} \min _{x} L (x, \alpha, \beta) \\
\text { s.t. } \quad \alpha_{i} \geqslant 0, \quad i=1,2, \cdots, k
\end{array}$$

$$d^{*}=\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \theta_{D}(\alpha, \beta)$$

$$d^{*}=\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \min _{x} L (x, \alpha, \beta) \leqslant \min _{x} \max _{\alpha, \beta: \alpha_{i} \geqslant 0} L (x, \alpha, \beta)=p^{*}$$

$$\theta_{D}(\alpha, \beta)=\min _{x} L (x, \alpha, \beta) \leqslant L (x, \alpha, \beta) \leqslant \max _{\alpha, \beta: \alpha_{i} \geqslant 0} L (x, \alpha, \beta)=\theta_{P}(x)$$

$$\theta_{D}(\alpha, \beta) \leqslant \theta_{P}(x)$$

$$\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \theta_{D}(\alpha, \beta) \leqslant \min _{x} \theta_{P}(x)$$

$$d^{*}=\max _{\alpha, \beta: \alpha_{i} \geqslant 0} \min _{x} L (x, \alpha, \beta) \leqslant \min _{x} \max _{\alpha, \beta: \alpha_{i} \geqslant 0} L (x, \alpha, \beta)=p^{*}$$

$$p^{*}=d^{*}=L\left (x^{*}, \alpha^{*}, \beta^{*}\right)$$

$$\begin{array}{c}
\nabla_{x} L\left (x^{*}, \alpha^{*}, \beta^{*}\right)=0 \\
\alpha_{i}^{*} c_{i}\left (x^{*}\right)=0, \quad i=1,2, \cdots, k \\
c_{i}\left (x^{*}\right) \leqslant 0, \quad i=1,2, \cdots, k \\
\alpha_{i}^{*} \geqslant 0, \quad i=1,2, \cdots, k \\
h_{j}\left (x^{*}\right)=0 \quad j=1,2, \cdots, l
\end{array}$$

## 矩阵的基本子空间

$$a_{1} v_{1}+a_{2} v_{2}+\cdots+a_{n} v_{n}$$

$$\operatorname{span}\left (v_{1}, v_{2}, \cdots, v_{n}\right)$$

$$N (A)=\left\{x \in R^{n} \mid A x=0\right\}$$

$$Y^{\perp}=\left\{x \in \mathbf{R}^{n} \mid x^{\mathrm{T}} y=0, \forall y \in Y\right\}$$

$$\begin{aligned}
R (A) & =\left\{z \in \mathbf{R}^{m} \mid \exists x \in \mathbf{R}^{n}, z=A x\right\} \\
& =A \text { 的列空间 }
\end{aligned}$$

$$\begin{aligned}
R\left (A^{\mathrm{T}}\right) & =\left\{y \in \mathbf{R}^{n} \mid \exists x \in \mathbf{R}^{m}, y=A^{\mathrm{T}} x\right\} \\
& =A \text { 的行空间 }
\end{aligned}$$

$$N (A)=R\left (A^{\mathrm{T}}\right)^{\perp}$$

$$N\left (A^{\mathrm{T}}\right)=R (A)^{\perp}$$

## KL 散度的定义和狄利克雷分布的性质

$$D (Q \| P)=\sum_{i} Q (i) \log \frac{Q (i)}{P (i)}$$

$$D (Q \| P)=\int Q (x) \log \frac{Q (x)}{P (x)} \mathrm{d} x$$

$$\begin{aligned}
-D (Q \| P) & =\int Q (x) \log \frac{P (x)}{Q (x)} \mathrm{d} x \\
& \leqslant \log \int Q (x) \frac{P (x)}{Q (x)} \mathrm{d} x \\
& =\log \int P (x) \mathrm{d} x=0
\end{aligned}$$

$$p (x \mid \eta)=h (x) \exp \left\{\eta^{\mathrm{T}} T (x)-A (\eta)\right\}$$

$$\begin{aligned}
\frac{\mathrm{d}}{\mathrm{d} \eta} A (\eta) & =\frac{\mathrm{d}}{\mathrm{d} \eta} \log \int h (x) \exp \left\{\eta^{\mathrm{T}} T (x)\right\} \mathrm{d} x \\
& =\frac{\int T (x) \exp \left\{\eta^{\mathrm{T}} T (x)\right\} h (x) \mathrm{d} x}{\int h (x) \exp \left\{\eta^{\mathrm{T}} T (x)\right\} \mathrm{d} x} \\
& =\int T (x) \exp \left\{\eta^{\mathrm{T}} T (x)-A (\eta)\right\} h (x) \mathrm{d} x \\
& =\int T (x) p (x \mid \eta) \mathrm{d} x \\
& =E[T (X)]
\end{aligned}$$

$$\begin{aligned}
p (\theta \mid \alpha) & =\frac{\Gamma\left (\sum_{l=1}^{K} \alpha_{l}\right)}{\prod_{k=1}^{K} \Gamma\left (\alpha_{k}\right)} \prod_{k=1}^{K} \theta_{k}^{\alpha_{k}-1} \\
& =\exp \left\{\left (\sum_{k=1}^{K}\left (\alpha_{k}-1\right) \log \theta_{k}\right)+\log \Gamma\left (\sum_{l=1}^{K} \alpha_{l}\right)-\sum_{k=1}^{K} \log \Gamma\left (\alpha_{k}\right)\right\}
\end{aligned}$$

$$\begin{aligned}
E_{p (\theta \mid \alpha)}\left[\log \theta_{k}\right] & =\frac{\mathrm{d}}{\mathrm{d} \alpha_{k}} A (\alpha)=\frac{\mathrm{d}}{\mathrm{d} \alpha_{k}}\left[\sum_{k=1}^{K} \log \Gamma\left (\alpha_{k}\right)-\log \Gamma\left (\sum_{l=1}^{K} \alpha_{l}\right)\right] \\
& =\Psi\left (\alpha_{k}\right)-\Psi\left (\sum_{l=1}^{K} \alpha_{l}\right), \quad k=1,2, \cdots, K
\end{aligned}$$

完。