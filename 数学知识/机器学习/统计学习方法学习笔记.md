# 序

本文旨在提供系统的《统计学习方法》**第二版**的学习笔记，本学习笔记的优点在于，提供公式中字母的注释，以便于公式的理解，同时借助 Obsidian 的 [[Obsidian阅读指南#双向链接|双向链接]] 功能，便捷知识之间的串联。

本内容依然在制作中，计划一共分为三步完善本内容：

1. 记录《统计学习方法》中全部的数学公式。
2. 补充知识点的内容。
3. 进行双向链接的建立以及内容的补充。

# 概论

## 分类

一般来说，统计学习或机器学习一般包括监督学习、无监督学习、强化学习。有时还包括半监督学习、主动学习。

### 监督学习

在监督学习中，输入实例 $x$ 的特征向量记作：

$$
x=\left (x^{(1)}, x^{(2)}, \cdots, x^{(i)}, \cdots, x^{(n)}\right)^{\mathrm{T}}
$$

$x^{(i)}$ 表示 $x$ 的第 $i$ 个特征。注意 $x^{(i)}$ 与 $x_{i}$ 不同，本书通常用 $x_{i}$ 表示多个输入变量中的第 $i$ 个变量，即：

$$
x_{i}=\left (x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}}
$$

监督学习从训练数据（training data）集合中学习模型，对测试数据（test data）进行预测。训练数据由输入（或特征向量）与输出对组成，训练集通常表示为：

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

测试数据也由输入与输出对组成。输入与输出对又称为样本（sample）或样本点。

监督学习的模型可以是概率模型或非概率模型，由条件概率分布 $P(Y \mid X)$ 或决策函数（decision function）$Y=f(X)$ 表示, 随具体学习方法而定。对具体的输入进行相应的输出预测时, 写作 $P(y \mid x)$ 或 $y=f(x)$。

监督学习分为学习和预测两个过程，由学习系统和预测系统完成：

![[监督学习.png]]

这其中首先给定一个训练数据集：

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

其中 $\left (x_{i}, y_{i}\right), i=1,2, \cdots, N,$ 称为样本或样本点。$x_{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}$ 是输入的观测值，也称为输入或实例，$y_{i} \in \mathcal{Y}$ 是输出的观测值，也称为输出。

监督学习分为学习和预测两个过程, 由学习系统与预测系统完成。在学习过程中, 学习系统利用给定的训练数据集, 通过学习（或训练）得到一个模型, 表示为条件概率分布 $\hat{P}(Y \mid X)$ 或决策函数 $Y=\hat{f}(X)$。条件概率分布 $\hat{P}(Y \mid X)$ 或决策函数 $Y=\hat{f}(X)$ 描述输入与输出随机变量之间的映射关系。在预测过程中，预测系统对于给定的测试样本集中的输入 $x_{N+1}$，由模型 $y_{N+1}=\arg \max _{y} \hat{P}\left (y \mid x_{N+1}\right)$ 或 $y_{N+1}=\hat{f}\left (x_{N+1}\right)$ 给出相应的输出 $y_{N+1}$。

在监督学习中，假设训练数据与测试数据是依联合概率分布 $P (X, Y)$ 独立同分布产生的。

学习系统（也就是学习算法）试图通过训练数据集中的样本 $\left (x_{i}, y_{i}\right)$ 带来的信息学习模型。具体地说，对输入 $x_{i}$，一个具体的模型 $y=f (x)$ 可以产生一个输出 $f\left (x_{i}\right)$，而训练数据集中对应的输出是 $y_{i}$。如果这个模型有很好的预测能力，训练样本输出 $y_{i}$ 和模型输出 $f\left (x_{i}\right)$ 之间的差就应该足够小。学习系统通过不断地尝试，选取最好的模型，以便对训练数据集有足够好的预测，同时对末知的测试数据集的预测也有尽可能好的推广。

### 无监督学习

假设 $\mathcal{X}$ 是输入空间，$\mathcal{Z}$ 是隐式结构空间。要学习的模型可以表示为函数 $z=g (x)$，条件概率分布 $P (z \mid x)$，或者条件概率分布 $P (x \mid z)$ 的形式, 其中 $x \in \mathcal{X}$ 是输入，$z \in \mathcal{Z}$ 是输出。包含所有可能的模型的集合称为假设空间。无监督学习旨在从假设空间中选出在给定评价标准下的最优模型。

无监督学习通常使用大量的无标注数据学习或训练，每一个样本是一个实例。训练数据表示为 $U=\left\{x_{1}, x_{2}, \cdots, x_{N}\right\}$，其中 $x_{i}, i=1,2, \cdots, N,$ 是样本。

无监督学习可以用于对已有数据的分析，也可以用于对末来数据的预测。分析时使用学习得到的模型, 即函数 $z=\hat{g}(x)$，条件概率分布 $\hat{P}(z \mid x)$，或者条件概率分布 $\hat{P}(x \mid z)$。

预测时，和监督学习有类似的流程。由学习系统与预测系统完成：

![[无监督学习.png]]

在学习过程中，学习系统从训练数据集学习，得到一个最优模型，表示为函数 $z=\hat{g}(x)$，条件概率分布 $\hat{P}(z \mid x)$ 或者条件概率分布 $\hat{P}(x \mid z)$。在预测过程中，预测系统对于给定的输入 $x_{N+1}$，由模型 $z_{N+1}=\hat{g}\left (x_{N+1}\right)$ 或 $z_{N+1}=\arg \max _{z} \hat{P}\left (z \mid x_{N+1}\right)$ 给出相应的输出 $z_{N+1}，进行聚类或降维，或者由模型 $\hat{P}(x \mid z)$ 给出输入的概率 $\hat{P}\left (x_{N+1} \mid z_{N+1}\right)$，进行概率估计。

### 强化学习

强化学习（reinforcement learning）是指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题。假设智能系统与环境的互动基于马尔可夫决策过程（Markov decision process），智能系统能观测到的是与环境互动得到的数据序列。强化学习的本质是学习最优的序贯决策。

智能系统与环境的互动如下：

![[强化学习.png]]

在每一步 $t$，智能系统从环境中观测到一个状态（state）$s_{t}$ 与一个奖励（reward）$r_{t}$，采取一个动作（action）$a_{t}$。环境根据智能系统选择的动作，决定下一步 $t+1$ 的状态 $s_{t+1}$ 与奖励 $r_{t+1}$。要学习的策略表示为给定的状态下采取的动作。智能系统的目标不是短期奖励的最大化，而是长期累积奖励的最大化。强化学习过程中，系统不断地试错（trial and error），以达到学习最优策略的目的。

强化学习的马尔可夫决策过程是状态、奖励、动作序列上的随机过程，由五元组 $\langle S, A, P, r, \gamma\rangle$ 组成。

- $S$ 是有限状态（state）的集合
- $A$ 是有限动作（action）的集合
- $P$ 是状态转移概率（transition probability）函数：

$$
P\left(s^{\prime} \mid s, a\right)=P\left(s_{t+1}=s^{\prime} \mid s_{t}=s, a_{t}=a\right)
$$

- $r$ 是奖励函数（reward function）：$r(s, a)=E\left(r_{t+1} \mid s_{t}=s, a_{t}=a\right.  )$
- $\gamma$ 是衰减系数（discount factor）：$\gamma \in[0,1]$

马尔可夫决策过程具有马尔可夫性，下一个状态只依赖于前一个状态与动作，由状态转移概率函数 $P\left(s^{\prime} \mid s, a\right)$ 表示。下一个奖励依赖于前一个状态与动作，由奖励函数 $r(s, a)$ 表示。

策略 $\pi$ 定义为给定状态下动作的函数 $a=f(s)$ 或者条件概率分布 $P(a \mid s)$。给定一个策略 $\pi$，智能系统与环境互动的行为就已确定（或者是确定性的或者是随机性的）。

价值函数（value function）或状态价值函数（state value function）定义为策略 $\pi$ 从某一个状态 $s$ 开始的长期累积奖励的数学期望：

$$
v_{\pi}(s)=E_{\pi}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\cdots \mid s_{t}=s\right]
$$

动作价值函数（action value function）定义为策略 $\pi$ 的从某一个状态 $s$ 和动作 $a$ 开始的长期累积奖励的数学期望：

$$
q_{\pi}(s, a)=E_{\pi}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\cdots \mid s_{t}=s, a_{t}=a\right]
$$

强化学习的目标就是在所有可能的策略中选出价值函数最大的策略 $\pi^{*}$，而在实际学习中往往从具体的策略出发，不断优化已有策略。这里 $\gamma$ 表示末来的奖励会有衰减。

强化学习方法中有基于策略的（policy-based）、基于价值的（value-based），这两者属于无模型的（model-free）方法, 还有有模型的（model-based）方法。

有模型的方法试图直接学习马尔可夫决策过程的模型，包括转移概率函数 $P\left(s^{\prime} \mid s, a\right)$ 和奖励函数 $r(s, a)$。这样可以通过模型对环境的反馈进行预测，求出价值函数最大的策略 $\pi^{*}$。

无模型的、基于策略的方法不直接学习模型，而是试图求解最优策略 $\pi^{*}$，表示为函数 $a=f^{*}(s)$ 或者是条件概率分布 $P^{*}(a \mid s)$，这样也能达到在环境中做出最优决策的目的。学习通常从一个具体策略开始, 通过搜索更优的策略进行。

无模型的、基于价值的方法也不直接学习模型，而是试图求解最优价值函数，特别是最优动作价值函数 $q^{*}(s, a)$。这样可以间接地学到最优策略，根据该策略在给定的状态下做出相应的动作。学习通常从一个具体价值函数开始，通过搜索更优的价值函数进行。

- 加法规则：$P(x)=\sum_{y} P(x, y)$
- 乘法规则：$P(x, y)=P(x) P(y \mid x)$

其中 $x$ 和 $y$ 是随机变量。

$$P(\theta \mid D)=\frac{P(\theta) P(D \mid \theta)}{P(D)}$$

$$P(x \mid D)=\int P(x \mid \theta, D) P(\theta \mid D) \mathrm{d} \theta$$

$$\mathcal{F}=\{f \mid Y=f(X)\}$$

$$\mathcal{F}=\left\{f \mid Y=f_{\theta}(X), \theta \in \mathbf{R}^{n}\right\}$$

$$\mathcal{F}=\{P \mid P(Y \mid X)\}$$

$$\mathcal{F}=\left\{P \mid P_{\theta}(Y \mid X), \theta \in \mathbf{R}^{n}\right\}$$

$$L(Y, f(X))=\left\{\begin{array}{ll}
1, & Y \neq f(X) \\
0, & Y=f(X)
\end{array}\right.$$

$$L(Y, f(X))=(Y-f(X))^{2}$$

$$L(Y, f(X))=|Y-f(X)|$$

$$L(Y, P(Y \mid X))=-\log P(Y \mid X)$$

$$\begin{aligned}
R_{\exp }(f) & =E_{P}[L(Y, f(X))] \\
& =\int_{\mathcal{X} \times \mathcal{Y}} L(y, f(x)) P(x, y) \mathrm{d} x \mathrm{~d} y
\end{aligned}$$

$$R_{\mathrm{emp}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)$$

$$\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)$$

$$R_{\mathrm{srm}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)$$

$$\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)$$

$$R_{\mathrm{emp}}(\hat{f})=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right)$$

$$e_{\mathrm{test}}=\frac{1}{N^{\prime}} \sum_{i=1}^{N^{\prime}} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right)$$

$$e_{\text {test }}=\frac{1}{N^{\prime}} \sum_{i=1}^{N^{\prime}} I\left(y_{i} \neq \hat{f}\left(x_{i}\right)\right)$$

$$r_{\text {test }}=\frac{1}{N^{\prime}} \sum_{i=1}^{N^{\prime}} I\left(y_{i}=\hat{f}\left(x_{i}\right)\right)$$

$$r_{\text {test }}+e_{\text {test }}=1$$

$$f_{M}(x, w)=w_{0}+w_{1} x+w_{2} x^{2}+\cdots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}$$

$$L(w)=\frac{1}{2} \sum_{i=1}^{N}\left(f\left(x_{i}, w\right)-y_{i}\right)^{2}$$

$$L(w)=\frac{1}{2} \sum_{i=1}^{N}\left(\sum_{j=0}^{M} w_{j} x_{i}^{j}-y_{i}\right)^{2}$$

$$\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)$$

$$L(w)=\frac{1}{N} \sum_{i=1}^{N}\left(f\left(x_{i} ; w\right)-y_{i}\right)^{2}+\frac{\lambda}{2}\|w\|^{2}$$

$$L(w)=\frac{1}{N} \sum_{i=1}^{N}\left(f\left(x_{i} ; w\right)-y_{i}\right)^{2}+\lambda\|w\|_{1}$$

$$\begin{aligned}
R_{\exp }(\hat{f}) & =E_{P}[L(Y, \hat{f}(X))] \\
& =\int_{\mathcal{X} \times \mathcal{Y}} L(y, \hat{f}(x)) P(x, y) \mathrm{d} x \mathrm{~d} y
\end{aligned}$$

$$R(f)=E[L(Y, f(X))]$$

$$\hat{R}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)$$

$$f_{N}=\arg \min _{f \in \mathcal{F}} \hat{R}(f)$$

$$R\left(f_{N}\right)=E\left[L\left(Y, f_{N}(X)\right)\right]$$

$$R(f) \leqslant \hat{R}(f)+\varepsilon(d, N, \delta)$$

$$\varepsilon(d, N, \delta)=\sqrt{\frac{1}{2 N}\left(\log d+\log \frac{1}{\delta}\right)}$$

$$P[\bar{X}-E(\bar{X}) \geqslant t] \leqslant \exp \left(-\frac{2 N^{2} t^{2}}{\sum_{i=1}^{N}\left(b_{i}-a_{i}\right)^{2}}\right) \\
$$

$$P[E(\bar{X})-\bar{X} \geqslant t] \leqslant \exp \left(-\frac{2 N^{2} t^{2}}{\sum_{i=1}^{N}\left(b_{i}-a_{i}\right)^{2}}\right)
$$

$$P(R(f)-\hat{R}(f) \geqslant \varepsilon) \leqslant \exp \left(-2 N \varepsilon^{2}\right)$$

$$\begin{aligned}
P(\exists f \in \mathcal{F}: R(f)-\hat{R}(f) \geqslant \varepsilon) & =P\left(\bigcup_{f \in \mathcal{F}}\{R(f)-\hat{R}(f) \geqslant \varepsilon\}\right) \\
& \leqslant \sum_{f \in \mathcal{F}} P(R(f)-\hat{R}(f) \geqslant \varepsilon) \\
& \leqslant d \exp \left(-2 N \varepsilon^{2}\right)
\end{aligned}$$

$$P(R(f)-\hat{R}(f)<\varepsilon) \geqslant 1-d \exp \left(-2 N \varepsilon^{2}\right)$$

$$\delta=d \exp \left(-2 N \varepsilon^{2}\right)$$

$$P(R(f)<\hat{R}(f)+\varepsilon) \geqslant 1-\delta$$

$$R\left(f_{N}\right) \leqslant \hat{R}\left(f_{N}\right)+\varepsilon(d, N, \delta)$$

$$P(Y \mid X)=\frac{P(X, Y)}{P(X)}$$

$$P=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}$$

$$R=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}$$

$$\frac{2}{F_{1}}=\frac{1}{P}+\frac{1}{R}$$

$$
F_{1}=\frac{2 \mathrm{TP}}{2 \mathrm{TP}+\mathrm{FP}+\mathrm{FN}}
$$

# 感知机

$$
f(x)=\operatorname{sign}(w \cdot x+b)
$$

$$
\operatorname{sign}(x)=\left\{\begin{array}{ll}
+1, & x \geqslant 0 \\
-1, & x<0
\end{array}\right.
$$

$$
w \cdot x+b=0
$$

$$
\frac{1}{\|w\|}\left|w \cdot x_{0}+b\right|
$$

$$
-y_{i}\left(w \cdot x_{i}+b\right)>0
$$

$$
-\frac{1}{\|w\|} y_{i}\left(w \cdot x_{i}+b\right)
$$

$$
-\frac{1}{\|w\|} \sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
$$

$$
L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
$$

$$
\min _{w, b} L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)
$$

$$
\begin{array}{c}
\nabla_{w} L(w, b)=-\sum\limits_{x_{i} \in M} y_{i} x_{i} \\
\nabla_{b} L(w, b)=-\sum\limits_{x_{i} \in M} y_{i}
\end{array}
$$

$$
\begin{array}{l}
w \leftarrow w+\eta y_{i} x_{i}\\
b \leftarrow b+\eta y_{i}
\end{array}
$$

$$
y_{i}\left(\hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i}\right)=y_{i}\left(w_{\mathrm{opt}} \cdot x_{i}+b_{\mathrm{opt}}\right) \geqslant \gamma
$$

令 $R=\max\limits _{1 \leqslant i \leqslant N}\left\|\hat{x}_{i}\right\|$，则感知机算法在训练数据集上的误分类次数 $k$ 满足不等式：

$$
k \leqslant\left(\frac{R}{\gamma}\right)^{2}
$$

$$
y_{i}\left(\hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i}\right)=y_{i}\left(w_{\mathrm{opt}} \cdot x_{i}+b_{\mathrm{opt}}\right)>0
$$

$$
\gamma=\min _{i}\left\{y_{i}\left(w_{\mathrm{opt}} \cdot x_{i}+b_{\mathrm{opt}}\right)\right\}
$$

$$
y_{i}\left(\hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i}\right)=y_{i}\left(w_{\mathrm{opt}} \cdot x_{i}+b_{\mathrm{opt}}\right) \geqslant \gamma
$$

$$
\hat{w}_{k-1}=\left(w_{k-1}^{\mathrm{T}}, b_{k-1}\right)^{\mathrm{T}}
$$

$$
y_{i}\left(\hat{w}_{k-1} \cdot \hat{x}_{i}\right)=y_{i}\left(w_{k-1} \cdot x_{i}+b_{k-1}\right) \leqslant 0
$$

$$
\begin{array}{l}
w_{k} \leftarrow w_{k-1}+\eta y_{i} x_{i} \\
b_{k} \leftarrow b_{k-1}+\eta y_{i}
\end{array}\\
$$

$$
\hat{w}_{k}=\hat{w}_{k-1}+\eta y_{i} \hat{x}_{i}
$$

$$
\hat{w}_{k} \cdot \hat{w}_{\text {opt }} \geqslant k \eta \gamma
$$

$$
\begin{aligned}
\hat{w}_{k} \cdot \hat{w}_{\mathrm{opt}} & =\hat{w}_{k-1} \cdot \hat{w}_{\mathrm{opt}}+\eta y_{i} \hat{w}_{\mathrm{opt}} \cdot \hat{x}_{i} \\
& \geqslant \hat{w}_{k-1} \cdot \hat{w}_{\mathrm{opt}}+\eta \gamma
\end{aligned}
$$

$$
\begin{array}{c}
\hat{w}_{k} \cdot \hat{w}_{\mathrm{opt}} \geqslant \hat{w}_{k-1} \cdot \hat{w}_{\mathrm{opt}}+\eta \gamma \geqslant \hat{w}_{k-2} \cdot \hat{w}_{\mathrm{opt}}+2 \eta \gamma \geqslant \cdots \geqslant k \eta \gamma \\
\left\|\hat{w}_{k}\right\|^{2} \leqslant k \eta^{2} R^{2}
\end{array}
$$

$$
\begin{aligned}
\left\|\hat{w}_{k}\right\|^{2} & =\left\|\hat{w}_{k-1}\right\|^{2}+2 \eta y_{i} \hat{w}_{k-1} \cdot \hat{x}_{i}+\eta^{2}\left\|\hat{x}_{i}\right\|^{2} \\
& \leqslant\left\|\hat{w}_{k-1}\right\|^{2}+\eta^{2}\left\|\hat{x}_{i}\right\|^{2} \\
& \leqslant\left\|\hat{w}_{k-1}\right\|^{2}+\eta^{2} R^{2} \\
& \leqslant\left\|\hat{w}_{k-2}\right\|^{2}+2 \eta^{2} R^{2} \leqslant \cdots \\
& \leqslant k \eta^{2} R^{2}
\end{aligned}
$$

$$
\begin{array}{l}
k \eta \gamma \leqslant \hat{w}_{k} \cdot \hat{w}_{\mathrm{opt}} \leqslant\left\|\hat{w}_{k}\right\|\left\|\hat{w}_{\mathrm{opt}}\right\| \leqslant \sqrt{k} \eta R \\
k^{2} \gamma^{2} \leqslant k R^{2}
\end{array}
$$

$$
w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}
$$

$$
b=\sum_{i=1}^{N} \alpha_{i} y_{i}
$$

输入：线性可分的数据集 $T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}$，其中 $x_{i} \in \mathbf{R}^{n}, y_{i} \in   \{-1,+1\}, i=1,2, \cdots, N$；学习率 $\eta (0<\eta \leqslant 1)$。
输出：$\alpha, b$；感知机模型 $f (x)=\operatorname{sign}\left (\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x+b\right)$，其中 $\alpha=   \left (\alpha_{1}, \alpha_{2}, \cdots, \alpha_{N}\right)^{\mathrm{T}}$。
1.  $\alpha \leftarrow 0$，$ b \leftarrow 0$；
2. 在训练集中选取数据 $\left (x_{i}, y_{i}\right)$；
3. 如果 $y_{i}\left (\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x_{i}+b\right) \leqslant 0$，

$$
\begin{array}{c}
\alpha_{i} \leftarrow \alpha_{i}+\eta \\
b \leftarrow b+\eta y_{i}
\end{array}
$$

4. 转至 <u>2</u> 直到没有误分类数据。

$$
\boldsymbol{G}=\left[x_{i} \cdot x_{j}\right]_{N \times N}
$$

$$
\operatorname{conv}(S)=\left\{x=\sum_{i=1}^{k} \lambda_{i} x_{i} \mid \sum_{i=1}^{k} \lambda_{i}=1, \lambda_{i} \geqslant 0, i=1,2, \cdots, k\right\}
$$

# K 近邻算法

$$
y=\arg \max _{c_{j}} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i}=c_{j}\right), \quad i=1,2, \cdots, N ; j=1,2, \cdots, K
$$

$$
L_{p}\left(x_{i}, x_{j}\right)=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{p}\right)^{\frac{1}{p}}
$$

$$
L_{2}\left(x_{i}, x_{j}\right)=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{2}\right)^{\frac{1}{2}}
$$

$$
L_{1}\left(x_{i}, x_{j}\right)=\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|
$$

$$
L_{\infty}\left(x_{i}, x_{j}\right)=\max _{l}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|
$$

$$
f: \mathbf{R}^{n} \rightarrow\left\{c_{1}, c_{2}, \cdots, c_{K}\right\}
$$

$$
P(Y \neq f(X))=1-P(Y=f(X))
$$

$$
\frac{1}{k} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i} \neq c_{j}\right)=1-\frac{1}{k} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i}=c_{j}\right)
$$

# 朴素贝叶斯方法

设输入空间 $\mathcal{X}\subseteq R^n$ 为维向量的集合，输出空间为类标记集合 $\mathcal{Y}=\{c_1,c_2,\cdots,c_K\}$。输入为特征向量 $x\in \mathcal{X}$，输出为类标记 $y\in \mathcal{Y}$。$X$ 是定义在输入空间 $\mathcal{X}$ 上的随机向量，$Y$ 是定义在输出空间 $\mathcal{Y}$ 上的随机变量。$P(X,Y)$ 是 X 和 Y 的联合概率分布。训练数据集：

$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$

由 $P(X,Y)$ 独立同分布产生。

朴素贝叶斯法通过训练数据集学习联合概率分布 $P(X,Y)$。具体地，学习以下先
验概率分布及条件概率分布。

先验概率分布：

$$P(Y=C_k),k=1,2,\cdots,K$$

条件概率分布：

$$
P\left(X=x \mid Y=c_{k}\right)=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} \mid Y=c_{k}\right), \quad k=1,2, \cdots, K
$$

$$
\begin{aligned}
P\left(X=x \mid Y=c_{k}\right) & =P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} \mid Y=c_{k}\right) \\
& =\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
\end{aligned}
$$

$$
P\left(Y=c_{k} \mid X=x\right)=\frac{P\left(X=x \mid Y=c_{k}\right) P\left(Y=c_{k}\right)}{\sum_{k} P\left(X=x \mid Y=c_{k}\right) P\left(Y=c_{k}\right)}
$$

$$
P\left(Y=c_{k} \mid X=x\right)=\frac{P\left(Y=c_{k}\right) \prod\limits_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}{\sum\limits_{k} P\left(Y=c_{k}\right) \prod\limits_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}, \quad k=1,2, \cdots, K
$$

$$
y=f(x)=\arg \max _{c_{k}} \frac{P\left(Y=c_{k}\right) \prod\limits_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}{\sum\limits_{k} P\left(Y=c_{k}\right) \prod\limits_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}
$$

$$
y=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
$$

0-1 损失函数：

$$
L(Y, f(X))=\left\{\begin{array}{cc}
1, & Y \neq f(X) \\
0, & Y=f(X)
\end{array}\right.
$$

期望风险函数：

$$
R_{\exp }(f)=E[L(Y, f(X))]
$$

$$
R_{\exp }(f)=E[L(Y, f(X))]
$$

$$
R_{\exp }(f)=E_{X} \sum_{k=1}^{K}\left[L\left(c_{k}, f(X)\right)\right] P\left(c_{k} \mid X\right)
$$

$$
\begin{aligned}
f(x) & =\arg \min _{y \in \mathcal{Y}} \sum_{k=1}^{K} L\left(c_{k}, y\right) P\left(c_{k} \mid X=x\right) \\
& =\arg \min _{y \in \mathcal{Y}} \sum_{k=1}^{K} P\left(y \neq c_{k} \mid X=x\right) \\
& =\arg \min _{y \in \mathcal{Y}}\left(1-P\left(y=c_{k} \mid X=x\right)\right) \\
& =\arg \max _{y \in \mathcal{Y}} P\left(y=c_{k} \mid X=x\right)
\end{aligned}
$$

$$
f(x)=\arg \max _{c_{k}} P\left(c_{k} \mid X=x\right)
$$

先验概率的极大似然估计：

$$
P\left(Y=c_{k}\right)=\frac{\sum\limits_{i=1}^{N} I\left(y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K
$$

$$
\begin{array}{l}
P\left(X^{(j)}=a_{j l} \mid Y=c_{k}\right)=\frac{\sum\limits_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)}{\sum\limits_{i=1}^{N} I\left(y_{i}=c_{k}\right)}\\
j=1,2, \cdots, n ; \quad l=1,2, \cdots, S_{j} ; \quad k=1,2, \cdots, K
\end{array}
$$

$$
P\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K
$$

$$
\begin{array}{c}
P\left(X^{(j)}=a_{j l} \mid Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)} \\
j=1,2, \cdots, n ; \quad l=1,2, \cdots, S_{j} ; \quad k=1,2, \cdots, K
\end{array}
$$

$$
P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right), \quad k=1,2, \cdots, K
$$

$$
y=\arg \max _{c_{k}} P\left (Y=c_{k}\right) \prod_{j=1}^{n} P\left (X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
$$

条件概率的贝叶斯估计：

$$
P_{\lambda}\left (X^{(j)}=a_{j l} \mid Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left (x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)+\lambda}{\sum_{i=1}^{N} I\left (y_{i}=c_{k}\right)+S_{j} \lambda}
$$

$$
\begin{array}{l}
P_{\lambda}\left (X^{(j)}=a_{j l} \mid Y=c_{k}\right)>0 \\
\sum_{l=1}^{S_{j}} P\left (X^{(j)}=a_{j l} \mid Y=c_{k}\right)=1
\end{array}
$$

先验概率的贝叶斯估计：

$$
P_{\lambda}\left (Y=c_{k}\right)=\frac{\sum\limits_{i=1}^{N} I\left (y_{i}=c_{k}\right)+\lambda}{N+K \lambda}
$$

# 决策树

随机变量 X 的熵定义为：

$$
P\left (X=x_{i}\right)=p_{i}, \quad i=1,2, \cdots, n
$$

$$
H (X)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$

$$
H (p)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$

$$
0 \leqslant H (p) \leqslant \log n
$$

$$
P (X=1)=p, \quad P (X=0)=1-p, \quad 0 \leqslant p \leqslant 1
$$

$$
H (p)=-p \log _{2} p-(1-p) \log _{2}(1-p)
$$

$$
P\left (X=x_{i}, Y=y_{j}\right)=p_{i j}, \quad i=1,2, \cdots, n ; \quad j=1,2, \cdots, m
$$

$$ 
H (Y \mid X)=\sum\limits_{i=1}^{n} p_{i} H\left (Y \mid X=x_{i}\right)
$$

这里 $p_{i}=P\left (X=x_{i}\right), i=1,2, \cdots, n$。

信息增益：

$$
g (D, A)=H (D)-H (D \mid A)
$$

$$
H (D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}
$$

$$
H (D \mid A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left (D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}
$$

$$
g (D, A)=H (D)-H (D \mid A)
$$

信息增益比：

$$
g_{R}(D, A)=\frac{g (D, A)}{H_{A}(D)}
$$

$$
H_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log _{2} \frac{\left|D_{i}\right|}{|D|}
$$

$$
C_{\alpha}(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)+\alpha|T|
$$

$$
H_{t}(T)=-\sum_{k} \frac{N_{t k}}{N_{t}} \log \frac{N_{t k}}{N_{t}}
$$

$$
C (T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)=-\sum_{t=1}^{|T|} \sum_{k=1}^{K} N_{t k} \log \frac{N_{t k}}{N_{t}}
$$

$$
C_{\alpha}(T)=C (T)+\alpha|T|
$$

$$
C_{\alpha}\left (T_{A}\right) \leqslant C_{\alpha}\left (T_{B}\right)
$$

$$
D=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

$$
f (x)=\sum_{m=1}^{M} c_{m} I\left (x \in R_{m}\right)
$$

可以使用平方误差 $\sum_{x_{i} \in R_{m}}\left(y_{i}-f\left(x_{i}\right)\right)^{2}$

$$
\hat{c}_{m}=\operatorname{ave}\left (y_{i} \mid x_{i} \in R_{m}\right)
$$

$$
R_{1}(j, s)=\left\{x \mid x^{(j)} \leqslant s\right\} \quad R_{2}(j, s)=\left\{x \mid x^{(j)}>s\right\}
$$

$$
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left (y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left (y_{i}-c_{2}\right)^{2}\right]
$$

$$
\hat{c}_{1}=\operatorname{ave}\left (y_{i} \mid x_{i} \in R_{1}(j, s)\right) \quad \hat{c}_{2}=\operatorname{ave}\left (y_{i} \mid x_{i} \in R_{2}(j, s)\right)
$$

$$
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]
$$

$$
\begin{array}{c}
R_{1}(j, s)=\left\{x \mid x^{(j)} \leqslant s\right\}, \quad R_{2}(j, s)=\left\{x \mid x^{(j)}>s\right\} \\
\hat{c}_{m}=\frac{1}{N_{m}} \sum_{x_{i} \in R_{m}(j, s)} y_{i}, \quad x \in R_{m}, \quad m=1,2
\end{array}
$$

$$
f (x)=\sum_{m=1}^{M} \hat{c}_{m} I\left (x \in R_{m}\right)
$$

$$
\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left (1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}
$$

$$
\operatorname{Gini}(p)=2 p (1-p)
$$

$$
\operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left (\frac{\left|C_{k}\right|}{|D|}\right)^{2}
$$

$$
D_{1}=\{(x, y) \in D \mid A (x)=a\}, \quad D_{2}=D-D_{1}
$$

$$
\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left (D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left (D_{2}\right)
$$

$$
C_{\alpha}(T)=C (T)+\alpha|T|
$$

$$
C_{\alpha}(t)=C (t)+\alpha
$$

$$
C_{\alpha}\left (T_{t}\right)=C\left (T_{t}\right)+\alpha\left|T_{t}\right|
$$

$$
C_{\alpha}\left (T_{t}\right)<C_{\alpha}(t)
$$

$$
C_{\alpha}\left (T_{t}\right)=C_{\alpha}(t)
$$

$$
g (t)=\frac{C (t)-C\left (T_{t}\right)}{\left|T_{t}\right|-1}
$$

$$
\begin{aligned}
g(t) & =\frac{C(t)-C\left(T_{t}\right)}{\left|T_{t}\right|-1} \\
\alpha & =\min (\alpha, g(t))
\end{aligned}
$$

$$
\begin{array}{l}
g(D, A)=H(D)-H(D \mid A) \\
H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|} \\
H(D \mid A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)
\end{array}
$$

$$
g_{R}(D, A)=\frac{g(D, A)}{H_{A}(D)}
$$

$$
\operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}
$$

$$
\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)
$$

# 逻辑斯谛回归与最大熵模型

$$
F (x)=P (X \leqslant x)=\frac{1}{1+\mathrm{e}^{-(x-\mu) / \gamma}}
$$

$$
f (x)=F^{\prime}(x)=\frac{\mathrm{e}^{-(x-\mu) / \gamma}}{\gamma\left (1+\mathrm{e}^{-(x-\mu) / \gamma}\right)^{2}}
$$

$$
F(-x+\mu)-\frac{1}{2}=-F(x+\mu)+\frac{1}{2}
$$

$$
P (Y=1 \mid x)=\frac{\exp (w \cdot x+b)}{1+\exp (w \cdot x+b)}
$$

$$
P (Y=0 \mid x)=\frac{1}{1+\exp (w \cdot x+b)}
$$

$$
P (Y=1 \mid x)=\frac{\exp (w \cdot x)}{1+\exp (w \cdot x)}
$$

$$
P (Y=0 \mid x)=\frac{1}{1+\exp (w \cdot x)}
$$

$$
\operatorname{logit}(p)=\log \frac{p}{1-p}
$$

$$
\log \frac{P (Y=1 \mid x)}{1-P (Y=1 \mid x)}=w \cdot x
$$

$$
P (Y=1 \mid x)=\frac{\exp (w \cdot x)}{1+\exp (w \cdot x)}
$$

$$
P (Y=1 \mid x)=\pi (x), \quad P (Y=0 \mid x)=1-\pi (x)
$$

$$
\prod_{i=1}^{N}\left[\pi\left (x_{i}\right)\right]^{y_{i}}\left[1-\pi\left (x_{i}\right)\right]^{1-y_{i}}
$$

$$
\begin{aligned}
L (w) & =\sum_{i=1}^{N}\left[y_{i} \log \pi\left (x_{i}\right)+\left (1-y_{i}\right) \log \left (1-\pi\left (x_{i}\right)\right)\right. \\
& =\sum_{i=1}^{N}\left[y_{i} \log \frac{\pi\left (x_{i}\right)}{1-\pi\left (x_{i}\right)}+\log \left (1-\pi\left (x_{i}\right)\right)\right] \\
& =\sum_{i=1}^{N}\left[y_{i}\left (w \cdot x_{i}\right)-\log \left (1+\exp \left (w \cdot x_{i}\right)\right]\right.
\end{aligned}
$$

$$
P (Y=1 \mid x)=\frac{\exp (\hat{w} \cdot x)}{1+\exp (\hat{w} \cdot x)} 
$$

$$
P (Y=0 \mid x)=\frac{1}{1+\exp (\hat{w} \cdot x)}
$$

$$
P (Y=k \mid x)=\frac{\exp \left (w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left (w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1 
$$

$$
P (Y=K \mid x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp \left (w_{k} \cdot x\right)}
$$

$$
H(P)=-\sum_{x} P(x) \log P(x)
$$

$$
0 \leqslant H(P) \leqslant \log |X|
$$

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

$$
\begin{array}{l}
\tilde{P}(X=x, Y=y)=\frac{\nu (X=x, Y=y)}{N} \\
\tilde{P}(X=x)=\frac{\nu (X=x)}{N}
\end{array}
$$

$$
f (x, y)=\left\{\begin{array}{ll}
1, & x \text { 与 } y \text { 满足某一事实 } \\
0, & \text { 否则 }
\end{array}\right.
$$

$$
E_{\tilde{P}}(f)=\sum_{x, y} \tilde{P}(x, y) f (x, y)
$$

$$
E_{P}(f)=\sum_{x, y} \tilde{P}(x) P (y \mid x) f (x, y)
$$

$$
E_{P}(f)=E_{\tilde{P}}(f)
$$

$$
\sum_{x, y} \tilde{P}(x) P (y \mid x) f (x, y)=\sum_{x, y} \tilde{P}(x, y) f (x, y)
$$

$$
\mathcal{C} \equiv\left\{P \in \mathcal{P} \mid E_{P}\left (f_{i}\right)=E_{\tilde{P}}\left (f_{i}\right), \quad i=1,2, \cdots, n\right\}
$$

$$
H (P)=-\sum_{x, y} \tilde{P}(x) P (y \mid x) \log P (y \mid x)
$$

$$
\begin{array}{ll}
\max _{P \in \mathbf{C}} & H (P)=-\sum_{x, y} \tilde{P}(x) P (y \mid x) \log P (y \mid x) \\
\text { s.t. } & E_{P}\left (f_{i}\right)=E_{\tilde{P}}\left (f_{i}\right), \quad i=1,2, \cdots, n \\
& \sum_{y} P (y \mid x)=1
\end{array}
$$

$$
\begin{array}{ll}
\min _{P \in \mathbf{C}} & -H (P)=\sum_{x, y} \tilde{P}(x) P (y \mid x) \log P (y \mid x) \\
\text { s.t. } & E_{P}\left (f_{i}\right)-E_{\tilde{P}}\left (f_{i}\right)=0, \quad i=1,2, \cdots, n \\
& \sum_{y} P (y \mid x)=1
\end{array}
$$

$$
\begin{aligned}
L (P, w) \equiv & -H (P)+w_{0}\left (1-\sum_{y} P (y \mid x)\right)+\sum_{i=1}^{n} w_{i}\left (E_{\tilde{P}}\left (f_{i}\right)-E_{P}\left (f_{i}\right)\right) \\
= & \sum_{x, y} \tilde{P}(x) P (y \mid x) \log P (y \mid x)+w_{0}\left (1-\sum_{y} P (y \mid x)\right)+ \\
& \sum_{i=1}^{n} w_{i}\left (\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P (y \mid x) f_{i}(x, y)\right)
\end{aligned}
$$

$$
\min _{P \in \mathbf{C}} \max _{w} L (P, w)
$$

$$
\max _{w} \min _{P \in \mathbf{C}} L (P, w)
$$

$$
\Psi (w)=\min _{P \in \mathbf{C}} L (P, w)=L\left (P_{w}, w\right)
$$

$$
P_{w}=\arg \min _{P \in \mathbf{C}} L (P, w)=P_{w}(y \mid x)
$$

$$
\begin{aligned}
\frac{\partial L (P, w)}{\partial P (y \mid x)} & =\sum_{x, y} \tilde{P}(x)(\log P (y \mid x)+1)-\sum_{y} w_{0}-\sum_{x, y}\left (\tilde{P}(x) \sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
& =\sum_{x, y} \tilde{P}(x)\left (\log P (y \mid x)+1-w_{0}-\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
\end{aligned}
$$

$$
P (y \mid x)=\exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)+w_{0}-1\right)=\frac{\exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)}{\exp \left (1-w_{0}\right)}
$$

$$
P_{w}(y \mid x)=\frac{1}{Z_{w}(x)} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
Z_{w}(x)=\sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
\max _{w} \Psi (w)
$$

$$
w^{*}=\arg \max _{w} \Psi (w)
$$

$$
L_{\tilde{P}}\left (P_{w}\right)=\log \prod_{x, y} P (y \mid x)^{\tilde{P}(x, y)}=\sum_{x, y} \tilde{P}(x, y) \log P (y \mid x)
$$

$$
\begin{aligned}
L_{\tilde{P}}\left (P_{w}\right) & =\sum_{x, y} \tilde{P}(x, y) \log P (y \mid x) \\
& =\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x, y} \tilde{P}(x, y) \log Z_{w}(x) \\
& =\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)
\end{aligned}
$$

$$
\begin{aligned}
\Psi (w)= & \sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) \log P_{w}(y \mid x)+ \\
& \sum_{i=1}^{n} w_{i}\left (\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) f_{i}(x, y)\right) \\
= & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)+\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x)\left (\log P_{w}(y \mid x)-\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
= & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) \log Z_{w}(x) \\
= & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)
\end{aligned}
$$

$$
\Psi (w)=L_{\tilde{P}}\left (P_{w}\right)
$$

$$
P_{w}(y \mid x)=\frac{1}{Z_{w}(x)} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
Z_{w}(x)=\sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
P_{w}(y \mid x)=\frac{1}{Z_{w}(x)} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
Z_{w}(x)=\sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
L (w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)
$$

$$
\begin{aligned}
L (w+\delta)-L (w) & =\sum_{x, y} \tilde{P}(x, y) \log P_{w+\delta}(y \mid x)-\sum_{x, y} \tilde{P}(x, y) \log P_{w}(y \mid x) \\
& =\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log \frac{Z_{w+\delta}(x)}{Z_{w}(x)}
\end{aligned}
$$

$$
-\log \alpha \geqslant 1-\alpha, \quad \alpha>0
$$

$$
\begin{aligned}
L (w+\delta)-L (w) & \geqslant \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \frac{Z_{w+\delta}(x)}{Z_{w}(x)} \\
& =\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \exp \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)
\end{aligned}
$$

$$
A (\delta \mid w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \exp \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)
$$

$$
L (w+\delta)-L (w) \geqslant A (\delta \mid w)
$$

$$
f^{\#}(x, y)=\sum_{i} f_{i}(x, y)
$$

$$
\begin{aligned}
A (\delta \mid w)= & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \exp \left (f^{\#}(x, y) \sum_{i=1}^{n} \frac{\delta_{i} f_{i}(x, y)}{f^{\#}(x, y)}\right)
\end{aligned}
$$

$$
\frac{f_{i}(x, y)}{f^{\#}(x, y)} \geqslant 0 \text { 且 } \sum_{i=1}^{n} \frac{f_{i}(x, y)}{f^{\#}(x, y)}=1
$$

$$
\exp \left (\sum_{i=1}^{n} \frac{f_{i}(x, y)}{f^{\#}(x, y)} \delta_{i} f^{\#}(x, y)\right) \leqslant \sum_{i=1}^{n} \frac{f_{i}(x, y)}{f^{\#}(x, y)} \exp \left (\delta_{i} f^{\#}(x, y)\right)
$$

$$
\begin{aligned}
A (\delta \mid w) \geqslant & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \sum_{i=1}^{n}\left (\frac{f_{i}(x, y)}{f^{\#}(x, y)}\right) \exp \left (\delta_{i} f^{\#}(x, y)\right)
\end{aligned}
$$

$$
B (\delta \mid w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \sum_{i=1}^{n}\left (\frac{f_{i}(x, y)}{f^{\#}(x, y)}\right) \exp \left (\delta_{i} f^{\#}(x, y)\right)
$$

$$
L (w+\delta)-L (w) \geqslant B (\delta \mid w)
$$

$$
\frac{\partial B (\delta \mid w)}{\partial \delta_{i}}=\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) f_{i}(x, y) \exp \left (\delta_{i} f^{\#}(x, y)\right)
$$

$$
\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) f_{i}(x, y) \exp \left (\delta_{i} f^{\#}(x, y)\right)=E_{\tilde{P}}\left (f_{i}\right)
$$

$$
\sum_{x, y} \tilde{P}(x) P (y \mid x) f_{i}(x, y) \exp \left (\delta_{i} f^{\#}(x, y)\right)=E_{\tilde{P}}\left (f_{i}\right)
$$

$$
f^{\#}(x, y)=\sum_{i=1}^{n} f_{i}(x, y)
$$

$$
w_{i} \leftarrow w_{i}+\delta_{i}
$$

$$
\delta_{i}=\frac{1}{M} \log \frac{E_{\tilde{P}}\left (f_{i}\right)}{E_{P}\left (f_{i}\right)}
$$

$$
\delta_{i}^{(k+1)}=\delta_{i}^{(k)}-\frac{g\left (\delta_{i}^{(k)}\right)}{g^{\prime}\left (\delta_{i}^{(k)}\right)}
$$

$$
P_{w}(y \mid x)=\frac{\exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)}{\sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)}
$$

$$
\min _{w \in \mathbf{R}^{n}} f (w)=\sum_{x} \tilde{P}(x) \log \sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)-\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)
$$

$$
g (w)=\left (\frac{\partial f (w)}{\partial w_{1}}, \frac{\partial f (w)}{\partial w_{2}}, \cdots, \frac{\partial f (w)}{\partial w_{n}}\right)^{\mathrm{T}}
$$

$$
\frac{\partial f (w)}{\partial w_{i}}=\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) f_{i}(x, y)-E_{\tilde{P}}\left (f_{i}\right), \quad i=1,2, \cdots, n
$$

$$
f\left (w^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geqslant 0} f\left (w^{(k)}+\lambda p_{k}\right)
$$

$$
B_{k+1}=B_{k}+\frac{y_{k} y_{k}^{\mathrm{T}}}{y_{k}^{\mathrm{T}} \delta_{k}}-\frac{B_{k} \delta_{k} \delta_{k}^{\mathrm{T}} B_{k}}{\delta_{k}^{\mathrm{T}} B_{k} \delta_{k}}
$$

$$
y_{k}=g_{k+1}-g_{k}, \quad \delta_{k}=w^{(k+1)}-w^{(k)}
$$

$$
\begin{array}{c}
P(Y=k \mid x)=\frac{\exp \left(w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1 \\
P(Y=K \mid x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}
\end{array}
$$

$$
\begin{array}{l}
P_{w}(y \mid x)=\frac{1}{Z_{w}(x)} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
Z_{w}(x)=\sum_{y} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
\end{array}
$$

$$
\begin{array}{ll}
\min & -H(P)=\sum_{x, y} \tilde{P}(x) P(y \mid x) \log P(y \mid x) \\
\text { s.t. } & P\left(f_{i}\right)-\tilde{P}\left(f_{i}\right)=0, \quad i=1,2, \cdots, n \\
& \sum_{y} P(y \mid x)=1
\end{array}
$$

# 支持向量机

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

$$
w^{*} \cdot x+b^{*}=0
$$

$$
f (x)=\operatorname{sign}\left (w^{*} \cdot x+b^{*}\right)
$$

$$
\hat{\gamma}_{i}=y_{i}\left (w \cdot x_{i}+b\right)
$$

$$
\hat{\gamma}=\min _{i=1, \cdots, N} \hat{\gamma}_{i}
$$

$$
\gamma_{i}=\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}
$$

$$
\gamma_{i}=-\left (\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right)
$$

$$
\gamma_{i}=y_{i}\left (\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right)
$$

$$
\gamma=\min _{i=1, \cdots, N} \gamma_{i}
$$

$$
\begin{aligned}
\gamma_{i} & =\frac{\hat{\gamma}_{i}}{\|w\|} \\
\gamma & =\frac{\hat{\gamma}}{\|w\|}
\end{aligned}
$$

$$
\begin{array}{ll}
\max\limits _{w, b} & \gamma \\
\text { s.t. } & y_{i}\left (\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right) \geqslant \gamma, \quad i=1,2, \cdots, N
\end{array}
$$
$$
\begin{array}{ll}
\max _{w, b} & \frac{\hat{\gamma}}{\|w\|} \\
\text { s.t. } & y_{i}\left (w \cdot x_{i}+b\right) \geqslant \hat{\gamma}, \quad i=1,2, \cdots, N
\end{array}
$$

$$
\begin{array}{ll}
\min _{w, b} & \frac{1}{2}\|w\|^{2} \\
\text { s.t. } & y_{i}\left (w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
\begin{array}{ll}
\min _{w} & f (w) \\
\text { s.t. } & g_{i}(w) \leqslant 0, \quad i=1,2, \cdots, k \\
& h_{i}(w)=0, \quad i=1,2, \cdots, l
\end{array}
$$

$$
\begin{array}{ll}
\min _{w, b} & \frac{1}{2}\|w\|^{2} \\
\text { s.t. } & y_{i}\left (w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
w^{*} \cdot x+b^{*}=0
$$

$$
f (x)=\operatorname{sign}\left (w^{*} \cdot x+b^{*}\right)
$$

$$
c \leqslant\|w\| \leqslant \frac{1}{2}\left\|w_{1}^{*}\right\|+\frac{1}{2}\left\|w_{2}^{*}\right\|=c
$$

$$
w_{1}^{*}=w_{2}^{*}
$$

$$
b_{1}^{*}=-\frac{1}{2}\left(w^{*} \cdot x_{1}^{\prime}+w^{*} \cdot x_{1}^{\prime \prime}\right), b_{2}^{*}=   -\frac{1}{2}\left(w^{*} \cdot x_{2}^{\prime}+w^{*} \cdot x_{2}^{\prime \prime}\right)
$$

$$
b_{1}^{*}-b_{2}^{*}=-\frac{1}{2}\left[w^{*} \cdot\left(x_{1}^{\prime}-x_{2}^{\prime}\right)+w^{*} \cdot\left(x_{1}^{\prime \prime}-x_{2}^{\prime \prime}\right)\right]
$$

$$
\begin{array}{c}
w^{*} \cdot x_{2}^{\prime}+b_{1}^{*} \geqslant 1=w^{*} \cdot x_{1}^{\prime}+b_{1}^{*} \\
w^{*} \cdot x_{1}^{\prime}+b_{2}^{*} \geqslant 1=w^{*} \cdot x_{2}^{\prime}+b_{2}^{*}
\end{array}
$$

$$
w^{*} \cdot\left(x_{1}^{\prime}-x_{2}^{\prime}\right)=0, w^{*} \cdot\left(x_{1}^{\prime \prime}-x_{2}^{\prime \prime}\right)=0
$$

$$
b_{1}^{*}-b_{2}^{*}=0
$$

$$
\begin{array}{l}
H_{1}: w \cdot x+b=1\\
H_{2}: w \cdot x+b=-1  
\end{array}
$$

$$
L(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}
$$

$$
\max _{\alpha} \min _{w, b} L(w, b, \alpha)
$$

$\min _{w, b} L(w, b, \alpha)$

$$
\begin{array}{l}
\nabla_{w} L(w, b, \alpha)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0 \\
\nabla_{b} L(w, b, \alpha)=-\sum_{i=1}^{N} \alpha_{i} y_{i}=0
\end{array}
$$

$$
w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}
$$

上方为 7.19。

# 隐马尔可夫模型

简单来说，马尔可夫链的马尔可夫性的意思是不同事件状态之间的转移相互独立。

$$
\begin{aligned}
Q (\lambda, \bar{\lambda})= & \sum_{I} \log \pi_{i_{1}} P (O, I \mid \bar{\lambda})+\sum_{I}\left (\sum_{t=1}^{T-1} \log a_{i_{t} i_{t+1}}\right) P (O, I \mid \bar{\lambda})+ \\
& \sum_{I}\left (\sum_{t=1}^{T} \log b_{i_{t}}\left (o_{t}\right)\right) P (O, I \mid \bar{\lambda})
\end{aligned}
$$

$$
\sum_{I} \log \pi_{i_{1}} P (O, I \mid \bar{\lambda})=\sum_{i=1}^{N} \log \pi_{i} P\left (O, i_{1}=i \mid \bar{\lambda}\right)
$$

$$
\sum_{i=1}^{N} \log \pi_{i} P\left (O, i_{1}=i \mid \bar{\lambda}\right)+\gamma\left (\sum_{i=1}^{N} \pi_{i}-1\right)
$$

$$
\sum_{I}\left (\sum_{t=1}^{T-1} \log a_{i_{t} i_{t+1}}\right) P (O, I \mid \bar{\lambda})=\sum_{i=1}^{N} \sum_{j=1}^{N} \sum_{t=1}^{T-1} \log a_{i j} P\left (O, i_{t}=i, i_{t+1}=j \mid \bar{\lambda}\right)
$$

$$
a_{i j}=\frac{\sum_{t=1}^{T-1} P\left (O, i_{t}=i, i_{t+1}=j \mid \bar{\lambda}\right)}{\sum_{t=1}^{T-1} P\left (O, i_{t}=i \mid \bar{\lambda}\right)}
$$

$$
\sum_{I}\left (\sum_{t=1}^{T} \log b_{i_{t}}\left (o_{t}\right)\right) P (O, I \mid \bar{\lambda})=\sum_{j=1}^{N} \sum_{t=1}^{T} \log b_{j}\left (o_{t}\right) P\left (O, i_{t}=j \mid \bar{\lambda}\right)
$$

$$
b_{j}(k)=\frac{\sum_{t=1}^{T} P\left (O, i_{t}=j \mid \bar{\lambda}\right) I\left (o_{t}=v_{k}\right)}{\sum_{t=1}^{T} P\left (O, i_{t}=j \mid \bar{\lambda}\right)}
$$

$$

$$