# 序

本文旨在提供系统的《统计学习方法》**第二版**的学习笔记，本学习笔记的优点在于，提供公式中字母的注释，以便于公式的理解，同时借助 Obsidian 的 [[Obsidian阅读指南#双向链接|双向链接]] 功能，便捷知识之间的串联。

本内容依然在制作中，计划一共分为三步完善本内容：

1. 记录《统计学习方法》中全部的数学公式。
2. 补充知识点的内容。
3. 进行双向链接的建立以及内容的补充。

# 概论

## 分类

一般来说，统计学习或机器学习一般包括监督学习、无监督学习、强化学习。有时还包括半监督学习、主动学习。

### 监督学习

在监督学习中，输入实例 $x$ 的特征向量记作：

$$
x=\left (x^{(1)}, x^{(2)}, \cdots, x^{(i)}, \cdots, x^{(n)}\right)^{\mathrm{T}}
$$

$x^{(i)}$ 表示 $x$ 的第 $i$ 个特征。注意 $x^{(i)}$ 与 $x_{i}$ 不同，本书通常用 $x_{i}$ 表示多个输入变量中的第 $i$ 个变量，即：

$$
x_{i}=\left (x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}}
$$

监督学习从训练数据（training data）集合中学习模型，对测试数据（test data）进行预测。训练数据由输入（或特征向量）与输出对组成，训练集通常表示为：

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

测试数据也由输入与输出对组成。输入与输出对又称为样本（sample）或样本点。

监督学习的模型可以是概率模型或非概率模型，由条件概率分布 $P(Y \mid X)$ 或决策函数（decision function）$Y=f(X)$ 表示, 随具体学习方法而定。对具体的输入进行相应的输出预测时, 写作 $P(y \mid x)$ 或 $y=f(x)$。

监督学习分为学习和预测两个过程，由学习系统和预测系统完成：

![[监督学习.png]]

这其中首先给定一个训练数据集：

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

其中 $\left (x_{i}, y_{i}\right), i=1,2, \cdots, N,$ 称为样本或样本点。$x_{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}$ 是输入的观测值，也称为输入或实例，$y_{i} \in \mathcal{Y}$ 是输出的观测值，也称为输出。

监督学习分为学习和预测两个过程, 由学习系统与预测系统完成。在学习过程中, 学习系统利用给定的训练数据集, 通过学习（或训练）得到一个模型, 表示为条件概率分布 $\hat{P}(Y \mid X)$ 或决策函数 $Y=\hat{f}(X)$。条件概率分布 $\hat{P}(Y \mid X)$ 或决策函数 $Y=\hat{f}(X)$ 描述输入与输出随机变量之间的映射关系。在预测过程中，预测系统对于给定的测试样本集中的输入 $x_{N+1}$，由模型 $y_{N+1}=\arg \max _{y} \hat{P}\left (y \mid x_{N+1}\right)$ 或 $y_{N+1}=\hat{f}\left (x_{N+1}\right)$ 给出相应的输出 $y_{N+1}$。

在监督学习中，假设训练数据与测试数据是依联合概率分布 $P (X, Y)$ 独立同分布产生的。

学习系统（也就是学习算法）试图通过训练数据集中的样本 $\left (x_{i}, y_{i}\right)$ 带来的信息学习模型。具体地说，对输入 $x_{i}$，一个具体的模型 $y=f (x)$ 可以产生一个输出 $f\left (x_{i}\right)$，而训练数据集中对应的输出是 $y_{i}$。如果这个模型有很好的预测能力，训练样本输出 $y_{i}$ 和模型输出 $f\left (x_{i}\right)$ 之间的差就应该足够小。学习系统通过不断地尝试，选取最好的模型，以便对训练数据集有足够好的预测，同时对末知的测试数据集的预测也有尽可能好的推广。

### 无监督学习

假设 $\mathcal{X}$ 是输入空间，$\mathcal{Z}$ 是隐式结构空间。要学习的模型可以表示为函数 $z=g (x)$，条件概率分布 $P (z \mid x)$，或者条件概率分布 $P (x \mid z)$ 的形式, 其中 $x \in \mathcal{X}$ 是输入，$z \in \mathcal{Z}$ 是输出。包含所有可能的模型的集合称为假设空间。无监督学习旨在从假设空间中选出在给定评价标准下的最优模型。

无监督学习通常使用大量的无标注数据学习或训练，每一个样本是一个实例。训练数据表示为 $U=\left\{x_{1}, x_{2}, \cdots, x_{N}\right\}$，其中 $x_{i}, i=1,2, \cdots, N,$ 是样本。

无监督学习可以用于对已有数据的分析，也可以用于对末来数据的预测。分析时使用学习得到的模型, 即函数 $z=\hat{g}(x)$，条件概率分布 $\hat{P}(z \mid x)$，或者条件概率分布 $\hat{P}(x \mid z)$。

预测时，和监督学习有类似的流程。由学习系统与预测系统完成：

![[无监督学习.png]]

在学习过程中，学习系统从训练数据集学习，得到一个最优模型，表示为函数 $z=\hat{g}(x)$，条件概率分布 $\hat{P}(z \mid x)$ 或者条件概率分布 $\hat{P}(x \mid z)$。在预测过程中，预测系统对于给定的输入 $x_{N+1}$，由模型 $z_{N+1}=\hat{g}\left (x_{N+1}\right)$ 或 $z_{N+1}=\arg \max _{z} \hat{P}\left (z \mid x_{N+1}\right)$ 给出相应的输出 $z_{N+1}，进行聚类或降维，或者由模型 $\hat{P}(x \mid z)$ 给出输入的概率 $\hat{P}\left (x_{N+1} \mid z_{N+1}\right)$，进行概率估计。

### 强化学习

强化学习（reinforcement learning）是指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题。假设智能系统与环境的互动基于马尔可夫决策过程（Markov decision process），智能系统能观测到的是与环境互动得到的数据序列。强化学习的本质是学习最优的序贯决策。

智能系统与环境的互动如下：

![[强化学习.png]]

在每一步 t , 智能系统从环境中观测到一个状态 (state)  s_{t}  与一个奖励 (reward)  r_{t} , 采取一个动作 (action)  a_{t}  。环境根据智能系统选择的动作, 决定下一步  t+1  的状态  s_{t+1}  与奖励  r_{t+1}  。要学习的策略表示为给定的状态下采取的动作。智能系统的目标不是短期奖励的最大化, 而是长期累积奖励的最大化。强化学习过程中, 系统不断地试错（trial and error），以达到学习最优策略的目的。

# K 近邻算法

# 朴素贝叶斯方法

设输入空间 $\mathcal{X}\subseteq R^n$ 为维向量的集合，输出空间为类标记集合 $\mathcal{Y}=\{c_1,c_2,\cdots,c_K\}$。输入为特征向量 $x\in \mathcal{X}$，输出为类标记 $y\in \mathcal{Y}$。$X$ 是定义在输入空间 $\mathcal{X}$ 上的随机向量，$Y$ 是定义在输出空间 $\mathcal{Y}$ 上的随机变量。$P(X,Y)$ 是 X 和 Y 的联合概率分布。训练数据集：

$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$

由 $P(X,Y)$ 独立同分布产生。

朴素贝叶斯法通过训练数据集学习联合概率分布 $P(X,Y)$。具体地，学习以下先
验概率分布及条件概率分布。

先验概率分布：

$$P(Y=C_k),k=1,2,\cdots,K$$

条件概率分布：

$$
P\left(X=x \mid Y=c_{k}\right)=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} \mid Y=c_{k}\right), \quad k=1,2, \cdots, K
$$

$$
\begin{aligned}
P\left(X=x \mid Y=c_{k}\right) & =P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} \mid Y=c_{k}\right) \\
& =\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
\end{aligned}
$$

$$
P\left(Y=c_{k} \mid X=x\right)=\frac{P\left(Y=c_{k}\right) \prod\limits_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}{\sum\limits_{k} P\left(Y=c_{k}\right) \prod\limits_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}, \quad k=1,2, \cdots, K
$$

$$
y=f(x)=\arg \max _{c_{k}} \frac{P\left(Y=c_{k}\right) \prod\limits_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}{\sum\limits_{k} P\left(Y=c_{k}\right) \prod\limits_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}
$$

$$
y=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
$$

0-1 损失函数：

$$
L(Y, f(X))=\left\{\begin{array}{cc}
1, & Y \neq f(X) \\
0, & Y=f(X)
\end{array}\right.
$$

期望风险函数：

$$
R_{\exp }(f)=E[L(Y, f(X))]
$$

$$
R_{\exp }(f)=E[L(Y, f(X))]
$$

$$
R_{\exp }(f)=E_{X} \sum_{k=1}^{K}\left[L\left(c_{k}, f(X)\right)\right] P\left(c_{k} \mid X\right)
$$

$$
\begin{aligned}
f(x) & =\arg \min _{y \in \mathcal{Y}} \sum_{k=1}^{K} L\left(c_{k}, y\right) P\left(c_{k} \mid X=x\right) \\
& =\arg \min _{y \in \mathcal{Y}} \sum_{k=1}^{K} P\left(y \neq c_{k} \mid X=x\right) \\
& =\arg \min _{y \in \mathcal{Y}}\left(1-P\left(y=c_{k} \mid X=x\right)\right) \\
& =\arg \max _{y \in \mathcal{Y}} P\left(y=c_{k} \mid X=x\right)
\end{aligned}
$$

$$
f(x)=\arg \max _{c_{k}} P\left(c_{k} \mid X=x\right)
$$

先验概率的极大似然估计：

$$
P\left(Y=c_{k}\right)=\frac{\sum\limits_{i=1}^{N} I\left(y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K
$$

$$
\begin{array}{l}
P\left(X^{(j)}=a_{j l} \mid Y=c_{k}\right)=\frac{\sum\limits_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)}{\sum\limits_{i=1}^{N} I\left(y_{i}=c_{k}\right)}\\
j=1,2, \cdots, n ; \quad l=1,2, \cdots, S_{j} ; \quad k=1,2, \cdots, K
\end{array}
$$

$$
P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right), \quad k=1,2, \cdots, K
$$

$$
y=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
$$

条件概率的贝叶斯估计：

$$
P_{\lambda}\left(X^{(j)}=a_{j l} \mid Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)+\lambda}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)+S_{j} \lambda}
$$

$$
\begin{array}{l}
P_{\lambda}\left(X^{(j)}=a_{j l} \mid Y=c_{k}\right)>0 \\
\sum_{l=1}^{S_{j}} P\left(X^{(j)}=a_{j l} \mid Y=c_{k}\right)=1
\end{array}
$$

先验概率的贝叶斯估计：

$$
P_{\lambda}\left(Y=c_{k}\right)=\frac{\sum\limits_{i=1}^{N} I\left(y_{i}=c_{k}\right)+\lambda}{N+K \lambda}
$$

# 决策树

随机变量 X 的熵定义为：

$$
P\left(X=x_{i}\right)=p_{i}, \quad i=1,2, \cdots, n
$$

$$
H(X)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$

$$
H(p)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$

$$
0 \leqslant H(p) \leqslant \log n
$$

$$
P(X=1)=p, \quad P(X=0)=1-p, \quad 0 \leqslant p \leqslant 1
$$

$$
H(p)=-p \log _{2} p-(1-p) \log _{2}(1-p)
$$

$$
P\left (X=x_{i}, Y=y_{j}\right)=p_{i j}, \quad i=1,2, \cdots, n ; \quad j=1,2, \cdots, m
$$

$$
\begin{array}{l} 
H (Y \mid X)=\sum\limits_{i=1}^{n} p_{i} H\left (Y \mid X=x_{i}\right)  \quad
p_{i}=P\left (X=x_{i}\right), i=1,2, \cdots, n
\end{array}
$$

信息增益：

$$
g (D, A)=H (D)-H (D \mid A)
$$

$$
H (D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}
$$

$$
H (D \mid A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left (D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}
$$

$$
g (D, A)=H (D)-H (D \mid A)
$$

信息增益比：

$$
g_{R}(D, A)=\frac{g (D, A)}{H_{A}(D)}
$$

$$
C_{\alpha}(T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)+\alpha|T|
$$

$$
H_{t}(T)=-\sum_{k} \frac{N_{t k}}{N_{t}} \log \frac{N_{t k}}{N_{t}}
$$

$$
C (T)=\sum_{t=1}^{|T|} N_{t} H_{t}(T)=-\sum_{t=1}^{|T|} \sum_{k=1}^{K} N_{t k} \log \frac{N_{t k}}{N_{t}}
$$

$$
C_{\alpha}(T)=C (T)+\alpha|T|
$$

$$
C_{\alpha}\left (T_{A}\right) \leqslant C_{\alpha}\left (T_{B}\right)
$$

$$
D=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

$$
f (x)=\sum_{m=1}^{M} c_{m} I\left (x \in R_{m}\right)
$$

$$
\hat{c}_{m}=\operatorname{ave}\left (y_{i} \mid x_{i} \in R_{m}\right)
$$

$$
R_{1}(j, s)=\left\{x \mid x^{(j)} \leqslant s\right\} \quad R_{2}(j, s)=\left\{x \mid x^{(j)}>s\right\}
$$

$$
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left (y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left (y_{i}-c_{2}\right)^{2}\right]
$$

$$
\hat{c}_{1}=\operatorname{ave}\left (y_{i} \mid x_{i} \in R_{1}(j, s)\right) \quad \hat{c}_{2}=\operatorname{ave}\left (y_{i} \mid x_{i} \in R_{2}(j, s)\right)
$$

$$
\begin{array}{c}
R_{1}(j, s)=\left\{x \mid x^{(j)} \leqslant s\right\}, \quad R_{2}(j, s)=\left\{x \mid x^{(j)}>s\right\} \\
\hat{c}_{m}=\frac{1}{N_{m}} \sum_{x_{i} \in R_{m}(j, s)} y_{i}, \quad x \in R_{m}, \quad m=1,2
\end{array}
$$

$$
f (x)=\sum_{m=1}^{M} \hat{c}_{m} I\left (x \in R_{m}\right)
$$

$$
\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left (1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}
$$

$$
\operatorname{Gini}(p)=2 p (1-p)
$$

$$
\operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left (\frac{\left|C_{k}\right|}{|D|}\right)^{2}
$$

$$
D_{1}=\{(x, y) \in D \mid A (x)=a\}, \quad D_{2}=D-D_{1}
$$

$$
\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left (D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left (D_{2}\right)
$$

$$
C_{\alpha}(T)=C (T)+\alpha|T|
$$

$$
C_{\alpha}(t)=C (t)+\alpha
$$

$$
C_{\alpha}\left (T_{t}\right)=C\left (T_{t}\right)+\alpha\left|T_{t}\right|
$$

$$
C_{\alpha}\left (T_{t}\right)<C_{\alpha}(t)
$$

$$
C_{\alpha}\left (T_{t}\right)=C_{\alpha}(t)
$$

$$
g (t)=\frac{C (t)-C\left (T_{t}\right)}{\left|T_{t}\right|-1}
$$

# 逻辑斯谛回归与最大熵模型

$$
F (x)=P (X \leqslant x)=\frac{1}{1+\mathrm{e}^{-(x-\mu) / \gamma}}
$$

$$
f (x)=F^{\prime}(x)=\frac{\mathrm{e}^{-(x-\mu) / \gamma}}{\gamma\left (1+\mathrm{e}^{-(x-\mu) / \gamma}\right)^{2}}
$$

$$
P (Y=1 \mid x)=\frac{\exp (w \cdot x+b)}{1+\exp (w \cdot x+b)}
$$

$$
P (Y=0 \mid x)=\frac{1}{1+\exp (w \cdot x+b)}
$$

$$
P (Y=1 \mid x)=\frac{\exp (w \cdot x)}{1+\exp (w \cdot x)}
$$

$$
P (Y=0 \mid x)=\frac{1}{1+\exp (w \cdot x)}
$$

$$
\operatorname{logit}(p)=\log \frac{p}{1-p}
$$

$$
\log \frac{P (Y=1 \mid x)}{1-P (Y=1 \mid x)}=w \cdot x
$$

$$
P (Y=1 \mid x)=\frac{\exp (w \cdot x)}{1+\exp (w \cdot x)}
$$

$$
P (Y=1 \mid x)=\pi (x), \quad P (Y=0 \mid x)=1-\pi (x)
$$

$$
\prod_{i=1}^{N}\left[\pi\left (x_{i}\right)\right]^{y_{i}}\left[1-\pi\left (x_{i}\right)\right]^{1-y_{i}}
$$

$$
\begin{aligned}
L (w) & =\sum_{i=1}^{N}\left[y_{i} \log \pi\left (x_{i}\right)+\left (1-y_{i}\right) \log \left (1-\pi\left (x_{i}\right)\right)\right. \\
& =\sum_{i=1}^{N}\left[y_{i} \log \frac{\pi\left (x_{i}\right)}{1-\pi\left (x_{i}\right)}+\log \left (1-\pi\left (x_{i}\right)\right)\right] \\
& =\sum_{i=1}^{N}\left[y_{i}\left (w \cdot x_{i}\right)-\log \left (1+\exp \left (w \cdot x_{i}\right)\right]\right.
\end{aligned}
$$

$$
P (Y=1 \mid x)=\frac{\exp (\hat{w} \cdot x)}{1+\exp (\hat{w} \cdot x)} 
$$

$$
P (Y=0 \mid x)=\frac{1}{1+\exp (\hat{w} \cdot x)}
$$

$$
P (Y=k \mid x)=\frac{\exp \left (w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left (w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1 
$$

$$
P (Y=K \mid x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp \left (w_{k} \cdot x\right)}
$$

$$
H (P)=-\sum_{x, y} \tilde{P}(x) P (y \mid x) \log P (y \mid x)
$$

$$
0 \leqslant H (P) \leqslant \log |X|
$$

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

$$
\begin{array}{l}
\tilde{P}(X=x, Y=y)=\frac{\nu (X=x, Y=y)}{N} \\
\tilde{P}(X=x)=\frac{\nu (X=x)}{N}
\end{array}
$$

$$
f (x, y)=\left\{\begin{array}{ll}
1, & x \text { 与 } y \text { 满足某一事实 } \\
0, & \text { 否则 }
\end{array}\right.
$$

$$
E_{\tilde{P}}(f)=\sum_{x, y} \tilde{P}(x, y) f (x, y)
$$

$$
E_{P}(f)=\sum_{x, y} \tilde{P}(x) P (y \mid x) f (x, y)
$$

$$
E_{P}(f)=E_{\tilde{P}}(f)
$$

$$
\sum_{x, y} \tilde{P}(x) P (y \mid x) f (x, y)=\sum_{x, y} \tilde{P}(x, y) f (x, y)
$$

$$
\mathcal{C} \equiv\left\{P \in \mathcal{P} \mid E_{P}\left (f_{i}\right)=E_{\tilde{P}}\left (f_{i}\right), \quad i=1,2, \cdots, n\right\}
$$

$$
H (P)=-\sum_{x, y} \tilde{P}(x) P (y \mid x) \log P (y \mid x)
$$

$$
\begin{array}{ll}
\max _{P \in \mathbf{C}} & H (P)=-\sum_{x, y} \tilde{P}(x) P (y \mid x) \log P (y \mid x) \\
\text { s.t. } & E_{P}\left (f_{i}\right)=E_{\tilde{P}}\left (f_{i}\right), \quad i=1,2, \cdots, n \\
& \sum_{y} P (y \mid x)=1
\end{array}
$$

$$
\begin{array}{ll}
\min _{P \in \mathbf{C}} & -H (P)=\sum_{x, y} \tilde{P}(x) P (y \mid x) \log P (y \mid x) \\
\text { s.t. } & E_{P}\left (f_{i}\right)-E_{\tilde{P}}\left (f_{i}\right)=0, \quad i=1,2, \cdots, n \\
& \sum_{y} P (y \mid x)=1
\end{array}
$$

$$
\begin{aligned}
L (P, w) \equiv & -H (P)+w_{0}\left (1-\sum_{y} P (y \mid x)\right)+\sum_{i=1}^{n} w_{i}\left (E_{\tilde{P}}\left (f_{i}\right)-E_{P}\left (f_{i}\right)\right) \\
= & \sum_{x, y} \tilde{P}(x) P (y \mid x) \log P (y \mid x)+w_{0}\left (1-\sum_{y} P (y \mid x)\right)+ \\
& \sum_{i=1}^{n} w_{i}\left (\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P (y \mid x) f_{i}(x, y)\right)
\end{aligned}
$$

$$
\min _{P \in \mathbf{C}} \max _{w} L (P, w)
$$

$$
\max _{w} \min _{P \in \mathbf{C}} L (P, w)
$$

$$
\Psi (w)=\min _{P \in \mathbf{C}} L (P, w)=L\left (P_{w}, w\right)
$$

$$
P_{w}=\arg \min _{P \in \mathbf{C}} L (P, w)=P_{w}(y \mid x)
$$

$$
\begin{aligned}
\frac{\partial L (P, w)}{\partial P (y \mid x)} & =\sum_{x, y} \tilde{P}(x)(\log P (y \mid x)+1)-\sum_{y} w_{0}-\sum_{x, y}\left (\tilde{P}(x) \sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
& =\sum_{x, y} \tilde{P}(x)\left (\log P (y \mid x)+1-w_{0}-\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
\end{aligned}
$$

$$
P (y \mid x)=\exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)+w_{0}-1\right)=\frac{\exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)}{\exp \left (1-w_{0}\right)}
$$

$$
P_{w}(y \mid x)=\frac{1}{Z_{w}(x)} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
Z_{w}(x)=\sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
\max _{w} \Psi (w)
$$

$$
w^{*}=\arg \max _{w} \Psi (w)
$$

$$
L_{\tilde{P}}\left (P_{w}\right)=\log \prod_{x, y} P (y \mid x)^{\tilde{P}(x, y)}=\sum_{x, y} \tilde{P}(x, y) \log P (y \mid x)
$$

$$
\begin{aligned}
L_{\tilde{P}}\left (P_{w}\right) & =\sum_{x, y} \tilde{P}(x, y) \log P (y \mid x) \\
& =\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x, y} \tilde{P}(x, y) \log Z_{w}(x) \\
& =\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)
\end{aligned}
$$

$$
\begin{aligned}
\Psi (w)= & \sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) \log P_{w}(y \mid x)+ \\
& \sum_{i=1}^{n} w_{i}\left (\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) f_{i}(x, y)\right) \\
= & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)+\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x)\left (\log P_{w}(y \mid x)-\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right) \\
= & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) \log Z_{w}(x) \\
= & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)
\end{aligned}
$$

$$
\Psi (w)=L_{\tilde{P}}\left (P_{w}\right)
$$

$$
P_{w}(y \mid x)=\frac{1}{Z_{w}(x)} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
Z_{w}(x)=\sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
P_{w}(y \mid x)=\frac{1}{Z_{w}(x)} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
Z_{w}(x)=\sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)
$$

$$
L (w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log Z_{w}(x)
$$

$$
\begin{aligned}
L (w+\delta)-L (w) & =\sum_{x, y} \tilde{P}(x, y) \log P_{w+\delta}(y \mid x)-\sum_{x, y} \tilde{P}(x, y) \log P_{w}(y \mid x) \\
& =\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)-\sum_{x} \tilde{P}(x) \log \frac{Z_{w+\delta}(x)}{Z_{w}(x)}
\end{aligned}
$$

$$
-\log \alpha \geqslant 1-\alpha, \quad \alpha>0
$$

$$
\begin{aligned}
L (w+\delta)-L (w) & \geqslant \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \frac{Z_{w+\delta}(x)}{Z_{w}(x)} \\
& =\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \exp \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)
\end{aligned}
$$

$$
A (\delta \mid w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \exp \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)
$$

$$
L (w+\delta)-L (w) \geqslant A (\delta \mid w)
$$

$$
f^{\#}(x, y)=\sum_{i} f_{i}(x, y)
$$

$$
\begin{aligned}
A (\delta \mid w)= & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \exp \left (f^{\#}(x, y) \sum_{i=1}^{n} \frac{\delta_{i} f_{i}(x, y)}{f^{\#}(x, y)}\right)
\end{aligned}
$$

$$
\frac{f_{i}(x, y)}{f^{\#}(x, y)} \geqslant 0 \text { 且 } \sum_{i=1}^{n} \frac{f_{i}(x, y)}{f^{\#}(x, y)}=1
$$

$$
\exp \left (\sum_{i=1}^{n} \frac{f_{i}(x, y)}{f^{\#}(x, y)} \delta_{i} f^{\#}(x, y)\right) \leqslant \sum_{i=1}^{n} \frac{f_{i}(x, y)}{f^{\#}(x, y)} \exp \left (\delta_{i} f^{\#}(x, y)\right)
$$

$$
\begin{aligned}
A (\delta \mid w) \geqslant & \sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \sum_{i=1}^{n}\left (\frac{f_{i}(x, y)}{f^{\#}(x, y)}\right) \exp \left (\delta_{i} f^{\#}(x, y)\right)
\end{aligned}
$$

$$
B (\delta \mid w)=\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} \delta_{i} f_{i}(x, y)+1-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) \sum_{i=1}^{n}\left (\frac{f_{i}(x, y)}{f^{\#}(x, y)}\right) \exp \left (\delta_{i} f^{\#}(x, y)\right)
$$

$$
L (w+\delta)-L (w) \geqslant B (\delta \mid w)
$$

$$
\frac{\partial B (\delta \mid w)}{\partial \delta_{i}}=\sum_{x, y} \tilde{P}(x, y) f_{i}(x, y)-\sum_{x} \tilde{P}(x) \sum_{y} P_{w}(y \mid x) f_{i}(x, y) \exp \left (\delta_{i} f^{\#}(x, y)\right)
$$

$$
\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) f_{i}(x, y) \exp \left (\delta_{i} f^{\#}(x, y)\right)=E_{\tilde{P}}\left (f_{i}\right)
$$

$$
\sum_{x, y} \tilde{P}(x) P (y \mid x) f_{i}(x, y) \exp \left (\delta_{i} f^{\#}(x, y)\right)=E_{\tilde{P}}\left (f_{i}\right)
$$

$$
f^{\#}(x, y)=\sum_{i=1}^{n} f_{i}(x, y)
$$

$$
w_{i} \leftarrow w_{i}+\delta_{i}
$$

$$
\delta_{i}=\frac{1}{M} \log \frac{E_{\tilde{P}}\left (f_{i}\right)}{E_{P}\left (f_{i}\right)}
$$

$$
\delta_{i}^{(k+1)}=\delta_{i}^{(k)}-\frac{g\left (\delta_{i}^{(k)}\right)}{g^{\prime}\left (\delta_{i}^{(k)}\right)}
$$

$$
P_{w}(y \mid x)=\frac{\exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)}{\sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)}
$$

$$
\min _{w \in \mathbf{R}^{n}} f (w)=\sum_{x} \tilde{P}(x) \log \sum_{y} \exp \left (\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)-\sum_{x, y} \tilde{P}(x, y) \sum_{i=1}^{n} w_{i} f_{i}(x, y)
$$

$$
g (w)=\left (\frac{\partial f (w)}{\partial w_{1}}, \frac{\partial f (w)}{\partial w_{2}}, \cdots, \frac{\partial f (w)}{\partial w_{n}}\right)^{\mathrm{T}}
$$

$$
\frac{\partial f (w)}{\partial w_{i}}=\sum_{x, y} \tilde{P}(x) P_{w}(y \mid x) f_{i}(x, y)-E_{\tilde{P}}\left (f_{i}\right), \quad i=1,2, \cdots, n
$$

$$
f\left (w^{(k)}+\lambda_{k} p_{k}\right)=\min _{\lambda \geqslant 0} f\left (w^{(k)}+\lambda p_{k}\right)
$$

$$
B_{k+1}=B_{k}+\frac{y_{k} y_{k}^{\mathrm{T}}}{y_{k}^{\mathrm{T}} \delta_{k}}-\frac{B_{k} \delta_{k} \delta_{k}^{\mathrm{T}} B_{k}}{\delta_{k}^{\mathrm{T}} B_{k} \delta_{k}}
$$

$$
y_{k}=g_{k+1}-g_{k}, \quad \delta_{k}=w^{(k+1)}-w^{(k)}
$$

# 支持向量机

$$
T=\left\{\left (x_{1}, y_{1}\right),\left (x_{2}, y_{2}\right), \cdots,\left (x_{N}, y_{N}\right)\right\}
$$

$$
w^{*} \cdot x+b^{*}=0
$$

$$
f (x)=\operatorname{sign}\left (w^{*} \cdot x+b^{*}\right)
$$

$$
\hat{\gamma}_{i}=y_{i}\left (w \cdot x_{i}+b\right)
$$

$$
\hat{\gamma}=\min _{i=1, \cdots, N} \hat{\gamma}_{i}
$$

$$
\gamma_{i}=\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}
$$

$$
\gamma_{i}=-\left (\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right)
$$

$$
\gamma_{i}=y_{i}\left (\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right)
$$

$$
\gamma=\min _{i=1, \cdots, N} \gamma_{i}
$$

$$
\begin{aligned}
\gamma_{i} & =\frac{\hat{\gamma}_{i}}{\|w\|} \\
\gamma & =\frac{\hat{\gamma}}{\|w\|}
\end{aligned}
$$

$$
\begin{array}{ll}
\max\limits _{w, b} & \gamma \\
\text { s.t. } & y_{i}\left (\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right) \geqslant \gamma, \quad i=1,2, \cdots, N
\end{array}
$$
$$
\begin{array}{ll}
\max _{w, b} & \frac{\hat{\gamma}}{\|w\|} \\
\text { s.t. } & y_{i}\left (w \cdot x_{i}+b\right) \geqslant \hat{\gamma}, \quad i=1,2, \cdots, N
\end{array}
$$

$$
\begin{array}{ll}
\min _{w, b} & \frac{1}{2}\|w\|^{2} \\
\text { s.t. } & y_{i}\left (w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
\begin{array}{ll}
\min _{w} & f (w) \\
\text { s.t. } & g_{i}(w) \leqslant 0, \quad i=1,2, \cdots, k \\
& h_{i}(w)=0, \quad i=1,2, \cdots, l
\end{array}
$$

$$
\begin{array}{ll}
\min _{w, b} & \frac{1}{2}\|w\|^{2} \\
\text { s.t. } & y_{i}\left (w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N
\end{array}
$$

$$
w^{*} \cdot x+b^{*}=0
$$

$$
f (x)=\operatorname{sign}\left (w^{*} \cdot x+b^{*}\right)
$$

$$
c \leqslant\|w\| \leqslant \frac{1}{2}\left\|w_{1}^{*}\right\|+\frac{1}{2}\left\|w_{2}^{*}\right\|=c
$$

# 隐马尔可夫模型

在这里需要先再次介绍 [[马尔可夫链]] 的概念，简单来说，马尔可夫链的马尔可夫性的意思是不同事件状态之间的转移相互独立。

$$
\begin{aligned}
Q (\lambda, \bar{\lambda})= & \sum_{I} \log \pi_{i_{1}} P (O, I \mid \bar{\lambda})+\sum_{I}\left (\sum_{t=1}^{T-1} \log a_{i_{t} i_{t+1}}\right) P (O, I \mid \bar{\lambda})+ \\
& \sum_{I}\left (\sum_{t=1}^{T} \log b_{i_{t}}\left (o_{t}\right)\right) P (O, I \mid \bar{\lambda})
\end{aligned}
$$

$$
\sum_{I} \log \pi_{i_{1}} P (O, I \mid \bar{\lambda})=\sum_{i=1}^{N} \log \pi_{i} P\left (O, i_{1}=i \mid \bar{\lambda}\right)
$$

$$
\sum_{i=1}^{N} \log \pi_{i} P\left (O, i_{1}=i \mid \bar{\lambda}\right)+\gamma\left (\sum_{i=1}^{N} \pi_{i}-1\right)
$$

$$
\sum_{I}\left (\sum_{t=1}^{T-1} \log a_{i_{t} i_{t+1}}\right) P (O, I \mid \bar{\lambda})=\sum_{i=1}^{N} \sum_{j=1}^{N} \sum_{t=1}^{T-1} \log a_{i j} P\left (O, i_{t}=i, i_{t+1}=j \mid \bar{\lambda}\right)
$$

$$
a_{i j}=\frac{\sum_{t=1}^{T-1} P\left (O, i_{t}=i, i_{t+1}=j \mid \bar{\lambda}\right)}{\sum_{t=1}^{T-1} P\left (O, i_{t}=i \mid \bar{\lambda}\right)}
$$

$$
\sum_{I}\left (\sum_{t=1}^{T} \log b_{i_{t}}\left (o_{t}\right)\right) P (O, I \mid \bar{\lambda})=\sum_{j=1}^{N} \sum_{t=1}^{T} \log b_{j}\left (o_{t}\right) P\left (O, i_{t}=j \mid \bar{\lambda}\right)
$$

$$
b_{j}(k)=\frac{\sum_{t=1}^{T} P\left (O, i_{t}=j \mid \bar{\lambda}\right) I\left (o_{t}=v_{k}\right)}{\sum_{t=1}^{T} P\left (O, i_{t}=j \mid \bar{\lambda}\right)}
$$

$$

$$