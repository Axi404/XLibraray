最小二乘法（Least Squares Method）是一种常用的数学优化技术，用于估计数据中的未知参数。最小二乘法其核心在于使得数学模型的 [[统计学习方法学习笔记#^MSE|平方损失函数]] 的 [[统计学习方法学习笔记#经验风险|经验风险]] 最小。

## 原理

最小二乘法的基本思想是寻找一组参数，使得数学模型的预测值与实际观测值之间的误差（即残差）的平方和最小。通过最小化误差平方和，我们可以找到最优的参数估计，使得模型能够更好地拟合数据。

## 数学表达

假设我们有 m 个观测数据点 $(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)$，我们的模型可以表示为一个函数 $f(x; \theta)$，其中 $\theta$ 表示参数向量。我们的目标是找到最优的参数向量 $\theta^*$，使得下面的目标函数最小化：

$$ J(\theta) = \sum_{i=1}^{m} (y_i - f(x_i; \theta))^2 $$

也就是使数学模型的 [[统计学习方法学习笔记#^MSE|平方损失函数]] 的 [[统计学习方法学习笔记#经验风险|经验风险]] 最小化。

## 求解最优参数

最小二乘法的优化目标是找到使得目标函数 $J(\theta)$ 最小的参数向量 $\theta^*$。因为目标函数是关于参数 $\theta$ 的凸函数（通常是二次函数），它具有唯一的最小值。

当使用最小二乘法求解最优参数时，常用的优化算法之一是梯度下降法（Gradient Descent）。梯度下降法是一种迭代的优化算法，通过不断更新参数的值，使目标函数逐渐趋近最优解。它利用目标函数的梯度（导数）信息来指导参数的更新方向，以找到最小化目标函数的参数值。

### 梯度下降法

在这里介绍一种运算符 $:=$，含义为赋值，不同于正常的等号表示的相等含义，其将左侧的值赋值为右侧的值。

1. **初始化参数**：首先，我们需要对参数向量 $\theta$ 进行初始化，可以随机地选取一组初始值，或者使用某种启发式方法进行初始化。
2. **计算梯度**：对于目标函数 $J(\theta)$，我们计算其关于参数向量 $\theta$ 的梯度 $\nabla J(\theta)$，梯度是一个向量，其中每个分量表示目标函数在相应参数上的偏导数。
3. **更新参数**：通过以下更新规则来更新参数向量 $\theta$：$\theta := \theta - \alpha \cdot \nabla J(\theta)$。其中 $\alpha$ 是学习率（learning rate），它是一个正数，控制每次更新的步长。较小的学习率可以使得收敛更稳定，但可能需要更多的迭代次数。较大的学习率可能导致更新过大而错过最优解，或者震荡不收敛。
4. **迭代更新**：重复步骤 2 和步骤 3，直到满足停止条件（如达到最大迭代次数、梯度接近零等），或者目标函数的值达到满意的收敛程度。

#### 示例

以下给出一个示例：

存在 $m$ 个观测数据点 $(0,1),(1,2),(2,3),(3,4),(4,5)\cdots$，不难看出其为函数 $y=x+1$ 上的数据点，使用模型 $f_\theta(x)=\theta^{(0)}+\theta^{(1)}x$。其中 $\theta^{(0)}$ 与 $\theta^{(1)}$ 均会被给予一个初始值。

则计算 $J(\theta)=\sum{(y-(\theta^{(0)}+\theta^{(1)}x))^2}$，值得注意的是，在这个式子中的 y 与 x 均为常数，同时因为 $\\theta^{(0)}$ 与 $\theta^{(1)}$ 并不相关，因此不难理解，其展开之后合并同类项，对于 $\theta^{(0)}$ 来说，其最高次项为二次，则 $J(\theta)$ 中与 $\theta^{(0)}$ 相关的式子为二次函数，且开口向上。

对其他参数同理，因此不难得到 $\theta^{(0)},\theta^{(1)}\cdots,\theta^{(n)}$ 均存在唯一最小值。

为此进行一个直观理解，对于二维参数向量来说，目标函数为三维函数，且形状为开口向上的凸函数，形如盆地。试想一个人位该盆地（其当前位置为参数向量的初始值），想要到达盆地最低点（目标函数最小），一种比较简单的方法是看当前所在位置的坡度，然后往低处走，这也正是梯度下降法的核心思想。

在数学运算中，这一坡度被解释为偏导得到的梯度，也就是斜率，其描绘了函数向最低点的走势。

如此便不难理解使用梯度下降法求得目标函数最小的直观理解了，其核心思想就在于通过梯度得到最小值点相较于现在的起始点的方位，然后通过改变自变量，前往最小值点。

如此便有了数学表示：

$$
\begin{aligned}
\theta_0&:=\theta_0-\eta\sum\limits_{i=1}^{n}(f_{\theta}(x^{(i)})-y^{(i)})\\
\theta_1&:=\theta_1-\eta\sum\limits_{i=1}^{n}(f_{\theta}(x^{(i)})-y^{(i)})x^{(i)}
\end{aligned}
$$

而假如说觉得 $f_{\theta}(x)=\theta_0+\theta_1x+\theta_2x^2$ 能够更好的拟合数据，也可以得到 $\theta_2:=\theta_2-\eta\sum\limits_{i=1}^{n}(f_{\theta}(x^{(i)})-y^{(i)})x^{(i)^2}$。

在这里需要解释的是在梯度前面添加的 $-\eta$：

- 负号：求梯度之后获得的梯度描述的是增加量，而希望向最低点逼近，则需要前往增加的相反方向，也就是负号。
- $\eta$：实际上求梯度之后获得的值并不一定可以直接使用，一个例子是对于函数 $y=10000x^2$，选择起始点 $x=1$，则求导之后得到梯度值为 $20000$，则有 $x:=x-10000$，$x=-19999$，且继续迭代，$x$ 会继续不断远离最小值点，这里不难得出，梯度只能指示最小值点相对于当前点的方向，且确实越接近最小值点梯度越小，但是这个值并不相对，而是一个绝对值，因此在实际学习的时候，需要添加一个系数 $\eta$ 并且加以调整，称 $\eta$ 为学习率，或者步长。

梯度下降法的一种变体是随机梯度下降和批量梯度下降。

### 随机梯度下降（Stochastic Gradient Descent）

在梯度下降法中，每次迭代都要计算所有样本数据的梯度并更新参数。而随机梯度下降则是每次迭代仅使用一个随机样本的梯度来更新参数，因此每次迭代的计算代价较低。由于使用随机样本，其更新方向可能更加不稳定，但它能够更快地收敛并逃离局部最优。

### 批量梯度下降（Batch Gradient Descent）

批量梯度下降是梯度下降的另一种变体，它在每次迭代中使用所有样本数据的梯度来更新参数。与随机梯度下降相比，批量梯度下降的更新方向更加稳定，但每次迭代的计算代价较大。

### 小批量梯度下降（Mini-Batch Gradient Descent）

小批量梯度下降是随机梯度下降和批量梯度下降的折中方案。它在每次迭代中使用一小批次（mini-batch）的随机样本数据来计算梯度并更新参数。这样既保持了较低的计算代价，也相对稳定地更新参数。

梯度下降法及其变体是许多优化问题的核心方法，选择合适的梯度下降法及学习率是非常重要的，因为不当的选择可能导致算法无法收敛或收敛速度过慢。

## 最优模型构建

一旦我们找到了最优的参数向量 $\theta^*$，我们就可以使用它来构建最优的模型。该模型会在给定自变量值时预测最优的因变量值，从而更好地拟合观测数据。

但值得一提的是，完全使用最小二乘法很可能产生 [[统计学习方法学习笔记#过拟合与模型选择|过拟合]] 的风险，因此可能需要添加罚项而改为 [[统计学习方法学习笔记#结构风险|结构风险]] 的计算。